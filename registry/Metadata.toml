
[BetaML.RandomForestRegressor]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Function\", \"Float64\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "BetaML.Bmlj.RandomForestRegressor"
":hyperparameters" = "`(:n_trees, :max_depth, :min_gain, :min_records, :max_features, :splitting_criterion, :β, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "random forest regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct RandomForestRegressor <: MLJModelInterface.Deterministic\n```\n\nA simple Random Forest model for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `n_trees::Int64`: Number of (decision) trees in the forest [def: `30`]\n  * `max_depth::Int64`: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: `0`, i.e. no limits]\n  * `min_gain::Float64`: The minimum information gain to allow for a node's partition [def: `0`]\n  * `min_records::Int64`: The minimum number of records a node must holds to consider for a partition of it [def: `2`]\n  * `max_features::Int64`: The maximum number of (random) features to consider at each partitioning [def: `0`, i.e. square root of the data dimension]\n  * `splitting_criterion::Function`: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: `variance`]. Either `variance` or a custom function. It can also be an anonymous function.\n  * `β::Float64`: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour \"better\" trees, but too high values will cause overfitting [def: `0`, i.e. uniform weigths]\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_boston;\n\njulia> modelType   = @load RandomForestRegressor pkg = \"BetaML\" verbosity=0\nBetaML.Trees.RandomForestRegressor\n\njulia> model       = modelType()\nRandomForestRegressor(\n  n_trees = 30, \n  max_depth = 0, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = 0, \n  splitting_criterion = BetaML.Utils.variance, \n  β = 0.0, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n[ Info: Training machine(RandomForestRegressor(n_trees = 30, …), …).\n\njulia> ŷ           = predict(mach, X);\n\njulia> hcat(y,ŷ)\n506×2 Matrix{Float64}:\n 24.0  25.8433\n 21.6  22.4317\n 34.7  35.5742\n 33.4  33.9233\n  ⋮    \n 23.9  24.42\n 22.0  22.4433\n 11.9  15.5833\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "RandomForestRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[BetaML.GaussianMixtureImputer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Vector{Float64}\", \"Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Continuous}}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "BetaML.Bmlj.GaussianMixtureImputer"
":hyperparameters" = "`(:n_classes, :initial_probmixtures, :mixtures, :tol, :minimum_variance, :minimum_covariance, :initialisation_strategy, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "gaussian mixture imputer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct GaussianMixtureImputer <: MLJModelInterface.Unsupervised\n```\n\nImpute missing values using a probabilistic approach (Gaussian Mixture Models) fitted using the Expectation-Maximisation algorithm, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `n_classes::Int64`: Number of mixtures (latent classes) to consider [def: 3]\n  * `initial_probmixtures::Vector{Float64}`: Initial probabilities of the categorical distribution (n_classes x 1) [default: `[]`]\n  * `mixtures::Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}`: An array (of length `n_classes``) of the mixtures to employ (see the [`?GMM`](@ref GMM) module in BetaML). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the`initialisation*strategy`parameter is  set to \"gived\"` This parameter can also be given symply in term of a _type*. In this case it is automatically extended to a vector of `n_classes``mixtures of the specified type. Note that mixing of different mixture types is not currently supported and that currently implemented mixtures are`SphericalGaussian`,`DiagonalGaussian`and`FullGaussian`. [def:`DiagonalGaussian`]\n  * `tol::Float64`: Tolerance to stop the algorithm [default: 10^(-6)]\n  * `minimum_variance::Float64`: Minimum variance for the mixtures [default: 0.05]\n  * `minimum_covariance::Float64`: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance.\n  * `initialisation_strategy::String`: The computation method of the vector of the initial mixtures. One of the following:\n\n      * \"grid\": using a grid approach\n      * \"given\": using the mixture provided in the fully qualified `mixtures` parameter\n      * \"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\n\n    Note that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\n\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example :\n\n```julia\njulia> using MLJ\n\njulia> X = [1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4] |> table ;\n\njulia> modelType   = @load GaussianMixtureImputer  pkg = \"BetaML\" verbosity=0\nBetaML.Imputation.GaussianMixtureImputer\n\njulia> model     = modelType(initialisation_strategy=\"grid\")\nGaussianMixtureImputer(\n  n_classes = 3, \n  initial_probmixtures = Float64[], \n  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], \n  tol = 1.0e-6, \n  minimum_variance = 0.05, \n  minimum_covariance = 0.0, \n  initialisation_strategy = \"grid\", \n  rng = Random._GLOBAL_RNG())\n\njulia> mach      = machine(model, X);\n\njulia> fit!(mach);\n[ Info: Training machine(GaussianMixtureImputer(n_classes = 3, …), …).\nIter. 1:        Var. of the post  2.0225921341714286      Log-likelihood -42.96100103213314\n\njulia> X_full       = transform(mach) |> MLJ.matrix\n9×2 Matrix{Float64}:\n 1.0      10.5\n 1.5      14.7366\n 1.8       8.0\n 1.7      15.0\n 3.2      40.0\n 2.51842  15.1747\n 3.3      38.0\n 2.47412  -2.3\n 5.2      -2.4\n```\n"""
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "GaussianMixtureImputer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":constructor" = "`nothing`"

[BetaML.RandomForestClassifier]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Function\", \"Float64\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "BetaML.Bmlj.RandomForestClassifier"
":hyperparameters" = "`(:n_trees, :max_depth, :min_gain, :min_records, :max_features, :splitting_criterion, :β, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "random forest classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct RandomForestClassifier <: MLJModelInterface.Probabilistic\n```\n\nA simple Random Forest model for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `n_trees::Int64`\n  * `max_depth::Int64`: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: `0`, i.e. no limits]\n  * `min_gain::Float64`: The minimum information gain to allow for a node's partition [def: `0`]\n  * `min_records::Int64`: The minimum number of records a node must holds to consider for a partition of it [def: `2`]\n  * `max_features::Int64`: The maximum number of (random) features to consider at each partitioning [def: `0`, i.e. square root of the data dimensions]\n  * `splitting_criterion::Function`: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: `gini`]. Either `gini`, `entropy` or a custom function. It can also be an anonymous function.\n  * `β::Float64`: Parameter that regulate the weights of the scoring of each tree, to be (optionally) used in prediction based on the error of the individual trees computed on the records on which trees have not been trained. Higher values favour \"better\" trees, but too high values will cause overfitting [def: `0`, i.e. uniform weigths]\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example :\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load RandomForestClassifier pkg = \"BetaML\" verbosity=0\nBetaML.Trees.RandomForestClassifier\n\njulia> model       = modelType()\nRandomForestClassifier(\n  n_trees = 30, \n  max_depth = 0, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = 0, \n  splitting_criterion = BetaML.Utils.gini, \n  β = 0.0, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n[ Info: Training machine(RandomForestClassifier(n_trees = 30, …), …).\n\njulia> cat_est    = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0667, virginica=>0.933)\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "RandomForestClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[BetaML.RandomForestImputer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Nothing, Int64}\", \"Float64\", \"Int64\", \"Union{Nothing, Int64}\", \"Vector{Int64}\", \"Union{Nothing, Function}\", \"Int64\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Known}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "BetaML.Bmlj.RandomForestImputer"
":hyperparameters" = "`(:n_trees, :max_depth, :min_gain, :min_records, :max_features, :forced_categorical_cols, :splitting_criterion, :recursive_passages, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "random forest imputer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct RandomForestImputer <: MLJModelInterface.Unsupervised\n```\n\nImpute missing values using Random Forests, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `n_trees::Int64`: Number of (decision) trees in the forest [def: `30`]\n  * `max_depth::Union{Nothing, Int64}`: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: `nothing`, i.e. no limits]\n  * `min_gain::Float64`: The minimum information gain to allow for a node's partition [def: `0`]\n  * `min_records::Int64`: The minimum number of records a node must holds to consider for a partition of it [def: `2`]\n  * `max_features::Union{Nothing, Int64}`: The maximum number of (random) features to consider at each partitioning [def: `nothing`, i.e. square root of the data dimension]\n  * `forced_categorical_cols::Vector{Int64}`: Specify the positions of the integer columns to treat as categorical instead of cardinal. [Default: empty vector (all numerical cols are treated as cardinal by default and the others as categorical)]\n  * `splitting_criterion::Union{Nothing, Function}`: Either `gini`, `entropy` or `variance`. This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: `nothing`, i.e. `gini` for categorical labels (classification task) and `variance` for numerical labels(regression task)]. It can be an anonymous function.\n  * `recursive_passages::Int64`: Define the times to go trough the various columns to impute their data. Useful when there are data to impute on multiple columns. The order of the first passage is given by the decreasing number of missing values per column, the other passages are random [default: `1`].\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X = [1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4] |> table ;\n\njulia> modelType   = @load RandomForestImputer  pkg = \"BetaML\" verbosity=0\nBetaML.Imputation.RandomForestImputer\n\njulia> model     = modelType(n_trees=40)\nRandomForestImputer(\n  n_trees = 40, \n  max_depth = nothing, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = nothing, \n  forced_categorical_cols = Int64[], \n  splitting_criterion = nothing, \n  recursive_passages = 1, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach      = machine(model, X);\n\njulia> fit!(mach);\n[ Info: Training machine(RandomForestImputer(n_trees = 40, …), …).\n\njulia> X_full       = transform(mach) |> MLJ.matrix\n9×2 Matrix{Float64}:\n 1.0      10.5\n 1.5      10.3909\n 1.8       8.0\n 1.7      15.0\n 3.2      40.0\n 2.88375   8.66125\n 3.3      38.0\n 3.98125  -2.3\n 5.2      -2.4\n```\n"""
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "RandomForestImputer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Known}}`"
":constructor" = "`nothing`"

[BetaML.PerceptronClassifier]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Union{Nothing, Matrix{Float64}}\", \"Union{Nothing, Vector{Float64}}\", \"Int64\", \"Bool\", \"Bool\", \"Bool\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}, AbstractMatrix{<:ScientificTypesBase.Infinite}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "BetaML.Bmlj.PerceptronClassifier"
":hyperparameters" = "`(:initial_coefficients, :initial_constant, :epochs, :shuffle, :force_origin, :return_mean_hyperplane, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "perceptron classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct PerceptronClassifier <: MLJModelInterface.Probabilistic\n```\n\nThe classical perceptron algorithm using one-vs-all for multiclass, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `initial_coefficients::Union{Nothing, Matrix{Float64}}`: N-classes by D-dimensions matrix of initial linear coefficients [def: `nothing`, i.e. zeros]\n  * `initial_constant::Union{Nothing, Vector{Float64}}`: N-classes vector of initial contant terms [def: `nothing`, i.e. zeros]\n  * `epochs::Int64`: Maximum number of epochs, i.e. passages trough the whole training sample [def: `1000`]\n  * `shuffle::Bool`: Whether to randomly shuffle the data at each iteration (epoch) [def: `true`]\n  * `force_origin::Bool`: Whether to force the parameter associated with the constant term to remain zero [def: `false`]\n  * `return_mean_hyperplane::Bool`: Whether to return the average hyperplane coefficients instead of the final ones  [def: `false`]\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load PerceptronClassifier pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.Perceptron.PerceptronClassifier\n\njulia> model       = modelType()\nPerceptronClassifier(\n  initial_coefficients = nothing, \n  initial_constant = nothing, \n  epochs = 1000, \n  shuffle = true, \n  force_origin = false, \n  return_mean_hyperplane = false, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n[ Info: Training machine(PerceptronClassifier(initial_coefficients = nothing, …), …).\n*** Avg. error after epoch 2 : 0.0 (all elements of the set has been correctly classified)\njulia> est_classes = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>2.53e-34, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>1.27e-18, virginica=>1.86e-310)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>2.77e-57, versicolor=>1.1099999999999999e-82, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>3.09e-22, versicolor=>4.03e-25, virginica=>1.0)\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "PerceptronClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}, AbstractMatrix{<:ScientificTypesBase.Infinite}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[BetaML.AutoEncoder]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Union{Float64, Int64}\", \"Union{Nothing, Float64, Int64}\", \"Union{Nothing, Vector{BetaML.Nn.AbstractLayer}}\", \"Union{Nothing, Vector{BetaML.Nn.AbstractLayer}}\", \"Union{Nothing, Function}\", \"Union{Nothing, Function}\", \"Int64\", \"Int64\", \"BetaML.Nn.OptimisationAlgorithm\", \"Bool\", \"BetaML.Api.AutoTuneMethod\", \"String\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":output_scitype" = "`AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "BetaML.Bmlj.AutoEncoder"
":hyperparameters" = "`(:encoded_size, :layers_size, :e_layers, :d_layers, :loss, :dloss, :epochs, :batch_size, :opt_alg, :shuffle, :tunemethod, :descr, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "auto encoder"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct AutoEncoder <: MLJModelInterface.Unsupervised\n```\n\nA ready-to use AutoEncoder, from the Beta Machine Learning Toolkit (BetaML) for ecoding and decoding of data using neural networks\n\n# Parameters:\n\n  * `encoded_size`: The number of neurons (i.e. dimensions) of the encoded data. If the value is a float it is consiered a percentual (to be rounded) of the dimensionality of the data [def: `0.33`]\n  * `layers_size`: Inner layer dimension (i.e. number of neurons). If the value is a float it is considered a percentual (to be rounded) of the dimensionality of the data [def: `nothing` that applies a specific heuristic]. Consider that the underlying neural network is trying to predict multiple values at the same times. Normally this requires many more neurons than a scalar prediction. If `e_layers` or `d_layers` are specified, this parameter is ignored for the respective part.\n  * `e_layers`: The layers (vector of `AbstractLayer`s) responsable of the encoding of the data [def: `nothing`, i.e. two dense layers with the inner one of `layers_size`]. See `subtypes(BetaML.AbstractLayer)` for supported layers\n  * `d_layers`: The layers (vector of `AbstractLayer`s) responsable of the decoding of the data [def: `nothing`, i.e. two dense layers with the inner one of `layers_size`]. See `subtypes(BetaML.AbstractLayer)` for supported layers\n  * `loss`: Loss (cost) function [def: `BetaML.squared_cost`]. Should always assume y and ŷ as (n x d) matrices.\n\n    !!! warning\n        If you change the parameter `loss`, you need to either provide its derivative on the parameter `dloss` or use autodiff with `dloss=nothing`.\n\n  * `dloss`: Derivative of the loss function [def: `BetaML.dsquared_cost` if `loss==squared_cost`, `nothing` otherwise, i.e. use the derivative of the squared cost or autodiff]\n  * `epochs`: Number of epochs, i.e. passages trough the whole training sample [def: `200`]\n  * `batch_size`: Size of each individual batch [def: `8`]\n  * `opt_alg`: The optimisation algorithm to update the gradient at each batch [def: `BetaML.ADAM()`] See `subtypes(BetaML.OptimisationAlgorithm)` for supported optimizers\n  * `shuffle`: Whether to randomly shuffle the data at each iteration (epoch) [def: `true`]\n  * `tunemethod`: The method - and its parameters - to employ for hyperparameters autotuning. See [`SuccessiveHalvingSearch`](@ref) for the default method. To implement automatic hyperparameter tuning during the (first) `fit!` call simply set `autotune=true` and eventually change the default `tunemethod` options (including the parameter ranges, the resources to employ and the loss function to adopt).\n\n  * `descr`: An optional title and/or description for this model\n  * `rng`: Random Number Generator (see [`FIXEDSEED`](@ref)) [deafult: `Random.GLOBAL_RNG`]\n\n# Notes:\n\n  * data must be numerical\n  * use `transform` to obtain the encoded data, and `inverse_trasnform` to decode to the original data\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load AutoEncoder pkg = \"BetaML\" verbosity=0;\n\njulia> model       = modelType(encoded_size=2,layers_size=10);\n\njulia> mach        = machine(model, X)\nuntrained Machine; caches model-specific representations of data\n  model: AutoEncoder(e_layers = nothing, …)\n  args: \n    1:\tSource @334 ⏎ Table{AbstractVector{Continuous}}\n\njulia> fit!(mach,verbosity=2)\n[ Info: Training machine(AutoEncoder(e_layers = nothing, …), …).\n***\n*** Training  for 200 epochs with algorithm BetaML.Nn.ADAM.\nTraining.. \t avg loss on epoch 1 (1): \t 35.48243542158747\nTraining.. \t avg loss on epoch 20 (20): \t 0.07528042222678126\nTraining.. \t avg loss on epoch 40 (40): \t 0.06293071729378613\nTraining.. \t avg loss on epoch 60 (60): \t 0.057035588828991145\nTraining.. \t avg loss on epoch 80 (80): \t 0.056313167754822875\nTraining.. \t avg loss on epoch 100 (100): \t 0.055521461091809436\nTraining the Neural Network...  52%|██████████████████████████████████████                                   |  ETA: 0:00:01Training.. \t avg loss on epoch 120 (120): \t 0.06015206472927942\nTraining.. \t avg loss on epoch 140 (140): \t 0.05536835903285201\nTraining.. \t avg loss on epoch 160 (160): \t 0.05877560142428245\nTraining.. \t avg loss on epoch 180 (180): \t 0.05476302769966953\nTraining.. \t avg loss on epoch 200 (200): \t 0.049240864053557445\nTraining the Neural Network... 100%|█████████████████████████████████████████████████████████████████████████| Time: 0:00:01\nTraining of 200 epoch completed. Final epoch error: 0.049240864053557445.\ntrained Machine; caches model-specific representations of data\n  model: AutoEncoder(e_layers = nothing, …)\n  args: \n    1:\tSource @334 ⏎ Table{AbstractVector{Continuous}}\n\n\njulia> X_latent    = transform(mach, X)\n150×2 Matrix{Float64}:\n 7.01701   -2.77285\n 6.50615   -2.9279\n 6.5233    -2.60754\n ⋮        \n 6.70196  -10.6059\n 6.46369  -11.1117\n 6.20212  -10.1323\n\njulia> X_recovered = inverse_transform(mach,X_latent)\n150×4 Matrix{Float64}:\n 5.04973  3.55838  1.43251  0.242215\n 4.73689  3.19985  1.44085  0.295257\n 4.65128  3.25308  1.30187  0.244354\n ⋮                          \n 6.50077  2.93602  5.3303   1.87647\n 6.38639  2.83864  5.54395  2.04117\n 6.01595  2.67659  5.03669  1.83234\n\njulia> BetaML.relative_mean_error(MLJ.matrix(X),X_recovered)\n0.03387721261716176\n\n\n```\n"""
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "AutoEncoder"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":inverse_transform", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}`"
":transform_scitype" = "`AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}`"
":constructor" = "`nothing`"

[BetaML.DecisionTreeRegressor]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Function\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "BetaML.Bmlj.DecisionTreeRegressor"
":hyperparameters" = "`(:max_depth, :min_gain, :min_records, :max_features, :splitting_criterion, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "decision tree regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct DecisionTreeRegressor <: MLJModelInterface.Deterministic\n```\n\nA simple Decision Tree model for regression with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `max_depth::Int64`: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: `0`, i.e. no limits]\n  * `min_gain::Float64`: The minimum information gain to allow for a node's partition [def: `0`]\n  * `min_records::Int64`: The minimum number of records a node must holds to consider for a partition of it [def: `2`]\n  * `max_features::Int64`: The maximum number of (random) features to consider at each partitioning [def: `0`, i.e. look at all features]\n  * `splitting_criterion::Function`: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: `variance`]. Either `variance` or a custom function. It can also be an anonymous function.\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_boston;\n\njulia> modelType   = @load DecisionTreeRegressor pkg = \"BetaML\" verbosity=0\nBetaML.Trees.DecisionTreeRegressor\n\njulia> model       = modelType()\nDecisionTreeRegressor(\n  max_depth = 0, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = 0, \n  splitting_criterion = BetaML.Utils.variance, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n[ Info: Training machine(DecisionTreeRegressor(max_depth = 0, …), …).\n\njulia> ŷ           = predict(mach, X);\n\njulia> hcat(y,ŷ)\n506×2 Matrix{Float64}:\n 24.0  26.35\n 21.6  21.6\n 34.7  34.8\n  ⋮    \n 23.9  23.75\n 22.0  22.2\n 11.9  13.2\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "DecisionTreeRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[BetaML.PegasosClassifier]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Union{Nothing, Matrix{Float64}}\", \"Union{Nothing, Vector{Float64}}\", \"Function\", \"Float64\", \"Int64\", \"Bool\", \"Bool\", \"Bool\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}, AbstractMatrix{<:ScientificTypesBase.Infinite}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "BetaML.Bmlj.PegasosClassifier"
":hyperparameters" = "`(:initial_coefficients, :initial_constant, :learning_rate, :learning_rate_multiplicative, :epochs, :shuffle, :force_origin, :return_mean_hyperplane, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "pegasos classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct PegasosClassifier <: MLJModelInterface.Probabilistic\n```\n\nThe gradient-based linear \"pegasos\" classifier using one-vs-all for multiclass, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `initial_coefficients::Union{Nothing, Matrix{Float64}}`: N-classes by D-dimensions matrix of initial linear coefficients [def: `nothing`, i.e. zeros]\n  * `initial_constant::Union{Nothing, Vector{Float64}}`: N-classes vector of initial contant terms [def: `nothing`, i.e. zeros]\n  * `learning_rate::Function`: Learning rate [def: (epoch -> 1/sqrt(epoch))]\n  * `learning_rate_multiplicative::Float64`: Multiplicative term of the learning rate [def: `0.5`]\n  * `epochs::Int64`: Maximum number of epochs, i.e. passages trough the whole training sample [def: `1000`]\n  * `shuffle::Bool`: Whether to randomly shuffle the data at each iteration (epoch) [def: `true`]\n  * `force_origin::Bool`: Whether to force the parameter associated with the constant term to remain zero [def: `false`]\n  * `return_mean_hyperplane::Bool`: Whether to return the average hyperplane coefficients instead of the final ones  [def: `false`]\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load PegasosClassifier pkg = \"BetaML\" verbosity=0\nBetaML.Perceptron.PegasosClassifier\n\njulia> model       = modelType()\nPegasosClassifier(\n  initial_coefficients = nothing, \n  initial_constant = nothing, \n  learning_rate = BetaML.Perceptron.var\"#71#73\"(), \n  learning_rate_multiplicative = 0.5, \n  epochs = 1000, \n  shuffle = true, \n  force_origin = false, \n  return_mean_hyperplane = false, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n\njulia> est_classes = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.817, versicolor=>0.153, virginica=>0.0301)\n UnivariateFinite{Multiclass{3}}(setosa=>0.791, versicolor=>0.177, virginica=>0.0318)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.254, versicolor=>0.5, virginica=>0.246)\n UnivariateFinite{Multiclass{3}}(setosa=>0.283, versicolor=>0.51, virginica=>0.207)\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "PegasosClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}, AbstractMatrix{<:ScientificTypesBase.Infinite}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[BetaML.KMeansClusterer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Function\", \"String\", \"Union{Nothing, Matrix{Float64}}\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "BetaML.Bmlj.KMeansClusterer"
":hyperparameters" = "`(:n_classes, :dist, :initialisation_strategy, :initial_representatives, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "k means clusterer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct KMeansClusterer <: MLJModelInterface.Unsupervised\n```\n\nThe classical KMeansClusterer clustering algorithm, from the Beta Machine Learning Toolkit (BetaML).\n\n# Parameters:\n\n  * `n_classes::Int64`: Number of classes to discriminate the data [def: 3]\n  * `dist::Function`: Function to employ as distance. Default to the Euclidean distance. Can be one of the predefined distances (`l1_distance`, `l2_distance`, `l2squared_distance`),  `cosine_distance`), any user defined function accepting two vectors and returning a scalar or an anonymous function with the same characteristics. Attention that, contrary to `KMedoidsClusterer`, the `KMeansClusterer` algorithm is not guaranteed to converge with other distances than the Euclidean one.\n  * `initialisation_strategy::String`: The computation method of the vector of the initial representatives. One of the following:\n\n      * \"random\": randomly in the X space\n      * \"grid\": using a grid approach\n      * \"shuffle\": selecting randomly within the available points [default]\n      * \"given\": using a provided set of initial representatives provided in the `initial_representatives` parameter\n\n  * `initial_representatives::Union{Nothing, Matrix{Float64}}`: Provided (K x D) matrix of initial representatives (useful only with `initialisation_strategy=\"given\"`) [default: `nothing`]\n  * `rng::Random.AbstractRNG`: Random Number Generator [deafult: `Random.GLOBAL_RNG`]\n\n# Notes:\n\n  * data must be numerical\n  * online fitting (re-fitting with new data) is supported\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load KMeansClusterer pkg = \"BetaML\" verbosity=0\nBetaML.Clustering.KMeansClusterer\n\njulia> model       = modelType()\nKMeansClusterer(\n  n_classes = 3, \n  dist = BetaML.Clustering.var\"#34#36\"(), \n  initialisation_strategy = \"shuffle\", \n  initial_representatives = nothing, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X);\n\njulia> fit!(mach);\n[ Info: Training machine(KMeansClusterer(n_classes = 3, …), …).\n\njulia> classes_est = predict(mach, X);\n\njulia> hcat(y,classes_est)\n150×2 CategoricalArrays.CategoricalArray{Union{Int64, String},2,UInt32}:\n \"setosa\"     2\n \"setosa\"     2\n \"setosa\"     2\n ⋮            \n \"virginica\"  3\n \"virginica\"  3\n \"virginica\"  1\n```\n"""
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "KMeansClusterer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":constructor" = "`nothing`"

[BetaML.NeuralNetworkRegressor]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Union{Nothing, Vector{BetaML.Nn.AbstractLayer}}\", \"Union{Nothing, Function}\", \"Union{Nothing, Function}\", \"Int64\", \"Int64\", \"BetaML.Nn.OptimisationAlgorithm\", \"Bool\", \"String\", \"Function\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "BetaML.Bmlj.NeuralNetworkRegressor"
":hyperparameters" = "`(:layers, :loss, :dloss, :epochs, :batch_size, :opt_alg, :shuffle, :descr, :cb, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "neural network regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct NeuralNetworkRegressor <: MLJModelInterface.Deterministic\n```\n\nA simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for regression of a single dimensional target.\n\n# Parameters:\n\n  * `layers`: Array of layer objects [def: `nothing`, i.e. basic network]. See `subtypes(BetaML.AbstractLayer)` for supported layers\n  * `loss`: Loss (cost) function [def: `BetaML.squared_cost`]. Should always assume y and ŷ as matrices, even if the regression task is 1-D\n\n    !!! warning\n        If you change the parameter `loss`, you need to either provide its derivative on the parameter `dloss` or use autodiff with `dloss=nothing`.\n\n  * `dloss`: Derivative of the loss function [def: `BetaML.dsquared_cost`, i.e. use the derivative of the squared cost]. Use `nothing` for autodiff.\n  * `epochs`: Number of epochs, i.e. passages trough the whole training sample [def: `200`]\n  * `batch_size`: Size of each individual batch [def: `16`]\n  * `opt_alg`: The optimisation algorithm to update the gradient at each batch [def: `BetaML.ADAM()`]. See `subtypes(BetaML.OptimisationAlgorithm)` for supported optimizers\n  * `shuffle`: Whether to randomly shuffle the data at each iteration (epoch) [def: `true`]\n  * `descr`: An optional title and/or description for this model\n  * `cb`: A call back function to provide information during training [def: `fitting_info`]\n  * `rng`: Random Number Generator (see [`FIXEDSEED`](@ref)) [deafult: `Random.GLOBAL_RNG`]\n\n# Notes:\n\n  * data must be numerical\n  * the label should be be a *n-records* vector.\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_boston;\n\njulia> modelType   = @load NeuralNetworkRegressor pkg = \"BetaML\" verbosity=0\nBetaML.Nn.NeuralNetworkRegressor\n\njulia> layers                      = [BetaML.DenseLayer(12,20,f=BetaML.relu),BetaML.DenseLayer(20,20,f=BetaML.relu),BetaML.DenseLayer(20,1,f=BetaML.relu)];\n\njulia> model       = modelType(layers=layers,opt_alg=BetaML.ADAM());\nNeuralNetworkRegressor(\n  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.23249759178069676 -0.4125090172711131 … 0.41401934928739 -0.33017881111237535; -0.27912169279319965 0.270551221249931 … 0.19258414323473344 0.1703002982374256; … ; 0.31186742456482447 0.14776438287394805 … 0.3624993442655036 0.1438885872964824; 0.24363744610286758 -0.3221033024934767 … 0.14886090419299408 0.038411663101909355], [-0.42360286004241765, -0.34355377040029594, 0.11510963232946697, 0.29078650404397893, -0.04940236502546075, 0.05142849152316714, -0.177685375947775, 0.3857630523957018, -0.25454667127064756, -0.1726731848206195, 0.29832456225553444, -0.21138505291162835, -0.15763643112604903, -0.08477044513587562, -0.38436681165349196, 0.20538016429104916, -0.25008157754468335, 0.268681800562054, 0.10600581996650865, 0.4262194464325672], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.08534180387478185 0.19659398307677617 … -0.3413633217504578 -0.0484925247381256; 0.0024419192794883915 -0.14614102508129 … -0.21912059923003044 0.2680725396694708; … ; 0.25151545823147886 -0.27532269951606037 … 0.20739970895058063 0.2891938885916349; -0.1699020711688904 -0.1350423717084296 … 0.16947589410758873 0.3629006047373296], [0.2158116357688406, -0.3255582642532289, -0.057314442103850394, 0.29029696770539953, 0.24994080694366455, 0.3624239027782297, -0.30674318230919984, -0.3854738338935017, 0.10809721838554087, 0.16073511121016176, -0.005923262068960489, 0.3157147976348795, -0.10938918304264739, -0.24521229198853187, -0.307167732178712, 0.0808907777008302, -0.014577497150872254, -0.0011287181458157214, 0.07522282588658086, 0.043366500526073104], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.021367697115938555 -0.28326652172347155 … 0.05346175368370165 -0.26037328415871647], [-0.2313659199724562], BetaML.Utils.relu, BetaML.Utils.drelu)], \n  loss = BetaML.Utils.squared_cost, \n  dloss = BetaML.Utils.dsquared_cost, \n  epochs = 100, \n  batch_size = 32, \n  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var\"#90#93\"(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), \n  shuffle = true, \n  descr = \"\", \n  cb = BetaML.Nn.fitting_info, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n\njulia> ŷ    = predict(mach, X);\n\njulia> hcat(y,ŷ)\n506×2 Matrix{Float64}:\n 24.0  30.7726\n 21.6  28.0811\n 34.7  31.3194\n  ⋮    \n 23.9  30.9032\n 22.0  29.49\n 11.9  27.2438\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "NeuralNetworkRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}`"
":target_scitype" = "`AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[BetaML.MultitargetGaussianMixtureRegressor]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Vector{Float64}\", \"Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Int64\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Infinite}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Infinite}}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "BetaML.Bmlj.MultitargetGaussianMixtureRegressor"
":hyperparameters" = "`(:n_classes, :initial_probmixtures, :mixtures, :tol, :minimum_variance, :minimum_covariance, :initialisation_strategy, :maximum_iterations, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "multitarget gaussian mixture regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct MultitargetGaussianMixtureRegressor <: MLJModelInterface.Deterministic\n```\n\nA non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast but generally not very precise, except for data with a structure matching the chosen underlying mixture.\n\nThis is the multi-target version of the model. If you want to predict a single label (y), use the MLJ model [`GaussianMixtureRegressor`](@ref).\n\n# Hyperparameters:\n\n  * `n_classes::Int64`: Number of mixtures (latent classes) to consider [def: 3]\n  * `initial_probmixtures::Vector{Float64}`: Initial probabilities of the categorical distribution (n_classes x 1) [default: `[]`]\n  * `mixtures::Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}`: An array (of length `n_classes``) of the mixtures to employ (see the [`?GMM`](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the`initialisation*strategy`parameter is  set to \"gived\"` This parameter can also be given symply in term of a _type*. In this case it is automatically extended to a vector of `n_classes``mixtures of the specified type. Note that mixing of different mixture types is not currently supported. [def:`[DiagonalGaussian() for i in 1:n_classes]`]\n  * `tol::Float64`: Tolerance to stop the algorithm [default: 10^(-6)]\n  * `minimum_variance::Float64`: Minimum variance for the mixtures [default: 0.05]\n  * `minimum_covariance::Float64`: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\n  * `initialisation_strategy::String`: The computation method of the vector of the initial mixtures. One of the following:\n\n      * \"grid\": using a grid approach\n      * \"given\": using the mixture provided in the fully qualified `mixtures` parameter\n      * \"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\n\n    Note that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\n\n  * `maximum_iterations::Int64`: Maximum number of iterations [def: `typemax(Int64)`, i.e. ∞]\n  * `rng::Random.AbstractRNG`: Random Number Generator [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_boston;\n\njulia> ydouble     = hcat(y, y .*2  .+5);\n\njulia> modelType   = @load MultitargetGaussianMixtureRegressor pkg = \"BetaML\" verbosity=0\nBetaML.GMM.MultitargetGaussianMixtureRegressor\n\njulia> model       = modelType()\nMultitargetGaussianMixtureRegressor(\n  n_classes = 3, \n  initial_probmixtures = Float64[], \n  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], \n  tol = 1.0e-6, \n  minimum_variance = 0.05, \n  minimum_covariance = 0.0, \n  initialisation_strategy = \"kmeans\", \n  maximum_iterations = 9223372036854775807, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, ydouble);\n\njulia> fit!(mach);\n[ Info: Training machine(MultitargetGaussianMixtureRegressor(n_classes = 3, …), …).\nIter. 1:        Var. of the post  20.46947926187522       Log-likelihood -23662.72770575145\n\njulia> ŷdouble    = predict(mach, X)\n506×2 Matrix{Float64}:\n 23.3358  51.6717\n 23.3358  51.6717\n  ⋮       \n 16.6843  38.3686\n 16.6843  38.3686\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "MultitargetGaussianMixtureRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractMatrix{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractMatrix{<:ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Infinite}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Infinite}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[BetaML.GaussianMixtureRegressor]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Vector{Float64}\", \"Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Int64\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Infinite}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Infinite}}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "BetaML.Bmlj.GaussianMixtureRegressor"
":hyperparameters" = "`(:n_classes, :initial_probmixtures, :mixtures, :tol, :minimum_variance, :minimum_covariance, :initialisation_strategy, :maximum_iterations, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "gaussian mixture regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct GaussianMixtureRegressor <: MLJModelInterface.Deterministic\n```\n\nA non-linear regressor derived from fitting the data on a probabilistic model (Gaussian Mixture Model). Relatively fast but generally not very precise, except for data with a structure matching the chosen underlying mixture.\n\nThis is the single-target version of the model. If you want to predict several labels (y) at once, use the MLJ model [`MultitargetGaussianMixtureRegressor`](@ref).\n\n# Hyperparameters:\n\n  * `n_classes::Int64`: Number of mixtures (latent classes) to consider [def: 3]\n  * `initial_probmixtures::Vector{Float64}`: Initial probabilities of the categorical distribution (n_classes x 1) [default: `[]`]\n  * `mixtures::Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}`: An array (of length `n_classes``) of the mixtures to employ (see the [`?GMM`](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the`initialisation*strategy`parameter is  set to \"gived\"` This parameter can also be given symply in term of a _type*. In this case it is automatically extended to a vector of `n_classes``mixtures of the specified type. Note that mixing of different mixture types is not currently supported. [def:`[DiagonalGaussian() for i in 1:n_classes]`]\n  * `tol::Float64`: Tolerance to stop the algorithm [default: 10^(-6)]\n  * `minimum_variance::Float64`: Minimum variance for the mixtures [default: 0.05]\n  * `minimum_covariance::Float64`: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\n  * `initialisation_strategy::String`: The computation method of the vector of the initial mixtures. One of the following:\n\n      * \"grid\": using a grid approach\n      * \"given\": using the mixture provided in the fully qualified `mixtures` parameter\n      * \"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\n\n    Note that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\n\n  * `maximum_iterations::Int64`: Maximum number of iterations [def: `typemax(Int64)`, i.e. ∞]\n  * `rng::Random.AbstractRNG`: Random Number Generator [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y      = @load_boston;\n\njulia> modelType = @load GaussianMixtureRegressor pkg = \"BetaML\" verbosity=0\nBetaML.GMM.GaussianMixtureRegressor\n\njulia> model     = modelType()\nGaussianMixtureRegressor(\n  n_classes = 3, \n  initial_probmixtures = Float64[], \n  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], \n  tol = 1.0e-6, \n  minimum_variance = 0.05, \n  minimum_covariance = 0.0, \n  initialisation_strategy = \"kmeans\", \n  maximum_iterations = 9223372036854775807, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach      = machine(model, X, y);\n\njulia> fit!(mach);\n[ Info: Training machine(GaussianMixtureRegressor(n_classes = 3, …), …).\nIter. 1:        Var. of the post  21.74887448784976       Log-likelihood -21687.09917379566\n\njulia> ŷ         = predict(mach, X)\n506-element Vector{Float64}:\n 24.703442835305577\n 24.70344283512716\n  ⋮\n 17.172486989759676\n 17.172486989759644\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "GaussianMixtureRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Infinite}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Infinite}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[BetaML.MultitargetNeuralNetworkRegressor]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Union{Nothing, Vector{BetaML.Nn.AbstractLayer}}\", \"Union{Nothing, Function}\", \"Union{Nothing, Function}\", \"Int64\", \"Int64\", \"BetaML.Nn.OptimisationAlgorithm\", \"Bool\", \"String\", \"Function\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "BetaML.Bmlj.MultitargetNeuralNetworkRegressor"
":hyperparameters" = "`(:layers, :loss, :dloss, :epochs, :batch_size, :opt_alg, :shuffle, :descr, :cb, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "multitarget neural network regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct MultitargetNeuralNetworkRegressor <: MLJModelInterface.Deterministic\n```\n\nA simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for regression of multiple dimensional targets.\n\n# Parameters:\n\n  * `layers`: Array of layer objects [def: `nothing`, i.e. basic network]. See `subtypes(BetaML.AbstractLayer)` for supported layers\n  * `loss`: Loss (cost) function [def: `BetaML.squared_cost`].  Should always assume y and ŷ as matrices.\n\n    !!! warning\n        If you change the parameter `loss`, you need to either provide its derivative on the parameter `dloss` or use autodiff with `dloss=nothing`.\n\n  * `dloss`: Derivative of the loss function [def: `BetaML.dsquared_cost`, i.e. use the derivative of the squared cost]. Use `nothing` for autodiff.\n  * `epochs`: Number of epochs, i.e. passages trough the whole training sample [def: `300`]\n  * `batch_size`: Size of each individual batch [def: `16`]\n  * `opt_alg`: The optimisation algorithm to update the gradient at each batch [def: `BetaML.ADAM()`]. See `subtypes(BetaML.OptimisationAlgorithm)` for supported optimizers\n  * `shuffle`: Whether to randomly shuffle the data at each iteration (epoch) [def: `true`]\n  * `descr`: An optional title and/or description for this model\n  * `cb`: A call back function to provide information during training [def: `BetaML.fitting_info`]\n  * `rng`: Random Number Generator (see [`FIXEDSEED`](@ref)) [deafult: `Random.GLOBAL_RNG`]\n\n# Notes:\n\n  * data must be numerical\n  * the label should be a *n-records* by *n-dimensions* matrix\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_boston;\n\njulia> ydouble     = hcat(y, y .*2  .+5);\n\njulia> modelType   = @load MultitargetNeuralNetworkRegressor pkg = \"BetaML\" verbosity=0\nBetaML.Nn.MultitargetNeuralNetworkRegressor\n\njulia> layers                      = [BetaML.DenseLayer(12,50,f=BetaML.relu),BetaML.DenseLayer(50,50,f=BetaML.relu),BetaML.DenseLayer(50,50,f=BetaML.relu),BetaML.DenseLayer(50,2,f=BetaML.relu)];\n\njulia> model       = modelType(layers=layers,opt_alg=BetaML.ADAM(),epochs=500)\nMultitargetNeuralNetworkRegressor(\n  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.2591582523441157 -0.027962845131416225 … 0.16044535560124418 -0.12838827994676857; -0.30381834909561184 0.2405495243851402 … -0.2588144861880588 0.09538577909777807; … ; -0.017320292924711156 -0.14042266424603767 … 0.06366999105841187 -0.13419651752478906; 0.07393079961409338 0.24521350531110264 … 0.04256867886217541 -0.0895506802948175], [0.14249427336553644, 0.24719379413682485, -0.25595911822556566, 0.10034088778965933, -0.017086404878505712, 0.21932184025609347, -0.031413516834861266, -0.12569076082247596, -0.18080140982481183, 0.14551901873323253  …  -0.13321995621967364, 0.2436582233332092, 0.0552222336976439, 0.07000814133633904, 0.2280064379660025, -0.28885681475734193, -0.07414214246290696, -0.06783184733650621, -0.055318068046308455, -0.2573488383282579], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.0395424111703751 -0.22531232360829911 … -0.04341228943744482 0.024336206858365517; -0.16481887432946268 0.17798073384748508 … -0.18594039305095766 0.051159225856547474; … ; -0.011639475293705043 -0.02347011206244673 … 0.20508869536159186 -0.1158382446274592; -0.19078069527757857 -0.007487540070740484 … -0.21341165344291158 -0.24158671316310726], [-0.04283623889330032, 0.14924461547060602, -0.17039563392959683, 0.00907774027816255, 0.21738885963113852, -0.06308040225941691, -0.14683286822101105, 0.21726892197970937, 0.19784321784707126, -0.0344988665714947  …  -0.23643089430602846, -0.013560425201427584, 0.05323948910726356, -0.04644175812567475, -0.2350400292671211, 0.09628312383424742, 0.07016420995205697, -0.23266392927140334, -0.18823664451487, 0.2304486691429084], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.11504184627266828 0.08601794194664503 … 0.03843129724045469 -0.18417305624127284; 0.10181551438831654 0.13459759904443674 … 0.11094951365942118 -0.1549466590355218; … ; 0.15279817525427697 0.0846661196058916 … -0.07993619892911122 0.07145402617285884; -0.1614160186346092 -0.13032002335149 … -0.12310552194729624 -0.15915773071049827], [-0.03435885900946367, -0.1198543931290306, 0.008454985905194445, -0.17980887188986966, -0.03557204910359624, 0.19125847393334877, -0.10949700778538696, -0.09343206702591, -0.12229583511781811, -0.09123969069220564  …  0.22119233518322862, 0.2053873143308657, 0.12756489387198222, 0.11567243705173319, -0.20982445664020496, 0.1595157838386987, -0.02087331046544119, -0.20556423263489765, -0.1622837764237961, -0.019220998739847395], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.25796717031347993 0.17579536633402948 … -0.09992960168785256 -0.09426177454620635; -0.026436330246675632 0.18070899284865127 … -0.19310119102392206 -0.06904005900252091], [0.16133004882307822, -0.3061228721091248], BetaML.Utils.relu, BetaML.Utils.drelu)], \n  loss = BetaML.Utils.squared_cost, \n  dloss = BetaML.Utils.dsquared_cost, \n  epochs = 500, \n  batch_size = 32, \n  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var\"#90#93\"(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), \n  shuffle = true, \n  descr = \"\", \n  cb = BetaML.Nn.fitting_info, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, ydouble);\n\njulia> fit!(mach);\n\njulia> ŷdouble    = predict(mach, X);\n\njulia> hcat(ydouble,ŷdouble)\n506×4 Matrix{Float64}:\n 24.0  53.0  28.4624  62.8607\n 21.6  48.2  22.665   49.7401\n 34.7  74.4  31.5602  67.9433\n 33.4  71.8  33.0869  72.4337\n  ⋮                   \n 23.9  52.8  23.3573  50.654\n 22.0  49.0  22.1141  48.5926\n 11.9  28.8  19.9639  45.5823\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "MultitargetNeuralNetworkRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}`"
":target_scitype" = "`AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[BetaML.DecisionTreeClassifier]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Int64\", \"Int64\", \"Function\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "BetaML.Bmlj.DecisionTreeClassifier"
":hyperparameters" = "`(:max_depth, :min_gain, :min_records, :max_features, :splitting_criterion, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "decision tree classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct DecisionTreeClassifier <: MLJModelInterface.Probabilistic\n```\n\nA simple Decision Tree model for classification with support for Missing data, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `max_depth::Int64`: The maximum depth the tree is allowed to reach. When this is reached the node is forced to become a leaf [def: `0`, i.e. no limits]\n  * `min_gain::Float64`: The minimum information gain to allow for a node's partition [def: `0`]\n  * `min_records::Int64`: The minimum number of records a node must holds to consider for a partition of it [def: `2`]\n  * `max_features::Int64`: The maximum number of (random) features to consider at each partitioning [def: `0`, i.e. look at all features]\n  * `splitting_criterion::Function`: This is the name of the function to be used to compute the information gain of a specific partition. This is done by measuring the difference betwwen the \"impurity\" of the labels of the parent node with those of the two child nodes, weighted by the respective number of items. [def: `gini`]. Either `gini`, `entropy` or a custom function. It can also be an anonymous function.\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load DecisionTreeClassifier pkg = \"BetaML\" verbosity=0\nBetaML.Trees.DecisionTreeClassifier\n\njulia> model       = modelType()\nDecisionTreeClassifier(\n  max_depth = 0, \n  min_gain = 0.0, \n  min_records = 2, \n  max_features = 0, \n  splitting_criterion = BetaML.Utils.gini, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n[ Info: Training machine(DecisionTreeClassifier(max_depth = 0, …), …).\n\njulia> cat_est    = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n UnivariateFinite{Multiclass{3}}(setosa=>1.0, versicolor=>0.0, virginica=>0.0)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)\n UnivariateFinite{Multiclass{3}}(setosa=>0.0, versicolor=>0.0, virginica=>1.0)\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "DecisionTreeClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[BetaML.GeneralImputer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Union{String, Vector{Int64}}\", \"Any\", \"Union{Bool, Vector{Bool}}\", \"Union{Function, Vector{Function}}\", \"Union{Function, Vector{Function}}\", \"Int64\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Known}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "BetaML.Bmlj.GeneralImputer"
":hyperparameters" = "`(:cols_to_impute, :estimator, :missing_supported, :fit_function, :predict_function, :recursive_passages, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "general imputer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct GeneralImputer <: MLJModelInterface.Unsupervised\n```\n\nImpute missing values using arbitrary learning models, from the Beta Machine Learning Toolkit (BetaML).\n\nImpute missing values using a vector (one per column) of arbitrary learning models (classifiers/regressors, not necessarily from BetaML) that implement the interface `m = Model([options])`, `train!(m,X,Y)` and `predict(m,X)`.\n\n# Hyperparameters:\n\n  * `cols_to_impute::Union{String, Vector{Int64}}`: Columns in the matrix for which to create an imputation model, i.e. to impute. It can be a vector of columns IDs (positions), or the keywords \"auto\" (default) or \"all\". With \"auto\" the model automatically detects the columns with missing data and impute only them. You may manually specify the columns or use \"all\" if you want to create a imputation model for that columns during training even if all training data are non-missing to apply then the training model to further data with possibly missing values.\n  * `estimator::Any`: An entimator model (regressor or classifier), with eventually its options (hyper-parameters), to be used to impute the various columns of the matrix. It can also be a `cols_to_impute`-length vector of different estimators to consider a different estimator for each column (dimension) to impute, for example when some columns are categorical (and will hence require a classifier) and some others are numerical (hence requiring a regressor). [default: `nothing`, i.e. use BetaML random forests, handling classification and regression jobs automatically].\n  * `missing_supported::Union{Bool, Vector{Bool}}`: Wheter the estimator(s) used to predict the missing data support itself missing data in the training features (X). If not, when the model for a certain dimension is fitted, dimensions with missing data in the same rows of those where imputation is needed are dropped and then only non-missing rows in the other remaining dimensions are considered. It can be a vector of boolean values to specify this property for each individual estimator or a single booleann value to apply to all the estimators [default: `false`]\n  * `fit_function::Union{Function, Vector{Function}}`: The function used by the estimator(s) to fit the model. It should take as fist argument the model itself, as second argument a matrix representing the features, and as third argument a vector representing the labels. This parameter is mandatory for non-BetaML estimators and can be a single value or a vector (one per estimator) in case of different estimator packages used. [default: `BetaML.fit!`]\n  * `predict_function::Union{Function, Vector{Function}}`: The function used by the estimator(s) to predict the labels. It should take as fist argument the model itself and as second argument a matrix representing the features. This parameter is mandatory for non-BetaML estimators and can be a single value or a vector (one per estimator) in case of different estimator packages used. [default: `BetaML.predict`]\n  * `recursive_passages::Int64`: Define the number of times to go trough the various columns to impute their data. Useful when there are data to impute on multiple columns. The order of the first passage is given by the decreasing number of missing values per column, the other passages are random [default: `1`].\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]. Note that this influence only the specific GeneralImputer code, the individual estimators may have their own rng (or similar) parameter.\n\n# Examples :\n\n  * *Using BetaML models*:\n\n```julia\njulia> using MLJ;\njulia> import BetaML # The library from which to get the individual estimators to be used for each column imputation\njulia> X = [\"a\"         8.2;\n            \"a\"     missing;\n            \"a\"         7.8;\n            \"b\"          21;\n            \"b\"          18;\n            \"c\"        -0.9;\n            missing      20;\n            \"c\"        -1.8;\n            missing    -2.3;\n            \"c\"        -2.4] |> table ;\njulia> modelType = @load GeneralImputer  pkg = \"BetaML\" verbosity=0\nBetaML.Imputation.GeneralImputer\njulia> model     = modelType(estimator=BetaML.DecisionTreeEstimator(),recursive_passages=2);\njulia> mach      = machine(model, X);\njulia> fit!(mach);\n[ Info: Training machine(GeneralImputer(cols_to_impute = auto, …), …).\njulia> X_full       = transform(mach) |> MLJ.matrix\n10×2 Matrix{Any}:\n \"a\"   8.2\n \"a\"   8.0\n \"a\"   7.8\n \"b\"  21\n \"b\"  18\n \"c\"  -0.9\n \"b\"  20\n \"c\"  -1.8\n \"c\"  -2.3\n \"c\"  -2.4\n```\n\n  * *Using third party packages* (in this example `DecisionTree`):\n\n```julia\njulia> using MLJ;\njulia> import DecisionTree # An example of external estimators to be used for each column imputation\njulia> X = [\"a\"         8.2;\n            \"a\"     missing;\n            \"a\"         7.8;\n            \"b\"          21;\n            \"b\"          18;\n            \"c\"        -0.9;\n            missing      20;\n            \"c\"        -1.8;\n            missing    -2.3;\n            \"c\"        -2.4] |> table ;\njulia> modelType   = @load GeneralImputer  pkg = \"BetaML\" verbosity=0\nBetaML.Imputation.GeneralImputer\njulia> model     = modelType(estimator=[DecisionTree.DecisionTreeClassifier(),DecisionTree.DecisionTreeRegressor()], fit_function=DecisionTree.fit!,predict_function=DecisionTree.predict,recursive_passages=2);\njulia> mach      = machine(model, X);\njulia> fit!(mach);\n[ Info: Training machine(GeneralImputer(cols_to_impute = auto, …), …).\njulia> X_full       = transform(mach) |> MLJ.matrix\n10×2 Matrix{Any}:\n \"a\"   8.2\n \"a\"   7.51111\n \"a\"   7.8\n \"b\"  21\n \"b\"  18\n \"c\"  -0.9\n \"b\"  20\n \"c\"  -1.8\n \"c\"  -2.3\n \"c\"  -2.4\n```\n"""
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "GeneralImputer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Known}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Known}}}`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Known}}`"
":constructor" = "`nothing`"

[BetaML.NeuralNetworkClassifier]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Union{Nothing, Vector{BetaML.Nn.AbstractLayer}}\", \"Union{Nothing, Function}\", \"Union{Nothing, Function}\", \"Int64\", \"Int64\", \"BetaML.Nn.OptimisationAlgorithm\", \"Bool\", \"String\", \"Function\", \"Union{Nothing, Vector}\", \"String\", \"Any\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "BetaML.Bmlj.NeuralNetworkClassifier"
":hyperparameters" = "`(:layers, :loss, :dloss, :epochs, :batch_size, :opt_alg, :shuffle, :descr, :cb, :categories, :handle_unknown, :other_categories_name, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "neural network classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct NeuralNetworkClassifier <: MLJModelInterface.Probabilistic\n```\n\nA simple but flexible Feedforward Neural Network, from the Beta Machine Learning Toolkit (BetaML) for classification  problems.\n\n# Parameters:\n\n  * `layers`: Array of layer objects [def: `nothing`, i.e. basic network]. See `subtypes(BetaML.AbstractLayer)` for supported layers. The last \"softmax\" layer is automatically added.\n  * `loss`: Loss (cost) function [def: `BetaML.crossentropy`]. Should always assume y and ŷ as matrices.\n\n    !!! warning\n        If you change the parameter `loss`, you need to either provide its derivative on the parameter `dloss` or use autodiff with `dloss=nothing`.\n\n  * `dloss`: Derivative of the loss function [def: `BetaML.dcrossentropy`, i.e. the derivative of the cross-entropy]. Use `nothing` for autodiff.\n  * `epochs`: Number of epochs, i.e. passages trough the whole training sample [def: `200`]\n  * `batch_size`: Size of each individual batch [def: `16`]\n  * `opt_alg`: The optimisation algorithm to update the gradient at each batch [def: `BetaML.ADAM()`]. See `subtypes(BetaML.OptimisationAlgorithm)` for supported optimizers\n  * `shuffle`: Whether to randomly shuffle the data at each iteration (epoch) [def: `true`]\n  * `descr`: An optional title and/or description for this model\n  * `cb`: A call back function to provide information during training [def: `BetaML.fitting_info`]\n  * `categories`: The categories to represent as columns. [def: `nothing`, i.e. unique training values].\n  * `handle_unknown`: How to handle categories not seens in training or not present in the provided `categories` array? \"error\" (default) rises an error, \"infrequent\" adds a specific column for these categories.\n  * `other_categories_name`: Which value during prediction to assign to this \"other\" category (i.e. categories not seen on training or not present in the provided `categories` array? [def: `nothing`, i.e. typemax(Int64) for integer vectors and \"other\" for other types]. This setting is active only if `handle_unknown=\"infrequent\"` and in that case it MUST be specified if Y is neither integer or strings\n  * `rng`: Random Number Generator [deafult: `Random.GLOBAL_RNG`]\n\n# Notes:\n\n  * data must be numerical\n  * the label should be a *n-records* by *n-dimensions* matrix (e.g. a one-hot-encoded data for classification), where the output columns should be interpreted as the probabilities for each categories.\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load NeuralNetworkClassifier pkg = \"BetaML\" verbosity=0\nBetaML.Nn.NeuralNetworkClassifier\n\njulia> layers      = [BetaML.DenseLayer(4,8,f=BetaML.relu),BetaML.DenseLayer(8,8,f=BetaML.relu),BetaML.DenseLayer(8,3,f=BetaML.relu),BetaML.VectorFunctionLayer(3,f=BetaML.softmax)];\n\njulia> model       = modelType(layers=layers,opt_alg=BetaML.ADAM())\nNeuralNetworkClassifier(\n  layers = BetaML.Nn.AbstractLayer[BetaML.Nn.DenseLayer([-0.376173352338049 0.7029289511758696 -0.5589563304592478 -0.21043274001651874; 0.044758889527899415 0.6687689636685921 0.4584331114653877 0.6820506583840453; … ; -0.26546358457167507 -0.28469736227283804 -0.164225549922154 -0.516785639164486; -0.5146043550684141 -0.0699113265130964 0.14959906603941908 -0.053706860039406834], [0.7003943613125758, -0.23990840466587576, -0.23823126271387746, 0.4018101580410387, 0.2274483050356888, -0.564975060667734, 0.1732063297031089, 0.11880299829896945], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([-0.029467850439546583 0.4074661266592745 … 0.36775675246760053 -0.595524555448422; 0.42455597698371306 -0.2458082732997091 … -0.3324220683462514 0.44439454998610595; … ; -0.2890883863364267 -0.10109249362508033 … -0.0602680568207582 0.18177278845097555; -0.03432587226449335 -0.4301192922760063 … 0.5646018168286626 0.47269177680892693], [0.13777442835428688, 0.5473306726675433, 0.3781939472904011, 0.24021813428130567, -0.0714779477402877, -0.020386373530818958, 0.5465466618404464, -0.40339790713616525], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.DenseLayer([0.6565120540082393 0.7139211611842745 … 0.07809812467915389 -0.49346311403373844; -0.4544472987041656 0.6502667641568863 … 0.43634608676548214 0.7213049952968921; 0.41212264783075303 -0.21993289366360613 … 0.25365007887755064 -0.5664469566269569], [-0.6911986792747682, -0.2149343209329364, -0.6347727539063817], BetaML.Utils.relu, BetaML.Utils.drelu), BetaML.Nn.VectorFunctionLayer{0}(fill(NaN), 3, 3, BetaML.Utils.softmax, BetaML.Utils.dsoftmax, nothing)], \n  loss = BetaML.Utils.crossentropy, \n  dloss = BetaML.Utils.dcrossentropy, \n  epochs = 100, \n  batch_size = 32, \n  opt_alg = BetaML.Nn.ADAM(BetaML.Nn.var\"#90#93\"(), 1.0, 0.9, 0.999, 1.0e-8, BetaML.Nn.Learnable[], BetaML.Nn.Learnable[]), \n  shuffle = true, \n  descr = \"\", \n  cb = BetaML.Nn.fitting_info, \n  categories = nothing, \n  handle_unknown = \"error\", \n  other_categories_name = nothing, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n\njulia> classes_est = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.575, versicolor=>0.213, virginica=>0.213)\n UnivariateFinite{Multiclass{3}}(setosa=>0.573, versicolor=>0.213, virginica=>0.213)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.236, versicolor=>0.236, virginica=>0.529)\n UnivariateFinite{Multiclass{3}}(setosa=>0.254, versicolor=>0.254, virginica=>0.492)\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "NeuralNetworkClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}, AbstractMatrix{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[BetaML.SimpleImputer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Function\", \"Union{Nothing, Int64}\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Continuous}}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "BetaML.Bmlj.SimpleImputer"
":hyperparameters" = "`(:statistic, :norm)`"
":is_pure_julia" = "`true`"
":human_name" = "simple imputer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct SimpleImputer <: MLJModelInterface.Unsupervised\n```\n\nImpute missing values using feature (column) mean, with optional record normalisation (using l-`norm` norms), from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `statistic::Function`: The descriptive statistic of the column (feature) to use as imputed value [def: `mean`]\n  * `norm::Union{Nothing, Int64}`: Normalise the feature mean by l-`norm` norm of the records [default: `nothing`]. Use it (e.g. `norm=1` to use the l-1 norm) if the records are highly heterogeneus (e.g. quantity exports of different countries).\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X = [1 10.5;1.5 missing; 1.8 8; 1.7 15; 3.2 40; missing missing; 3.3 38; missing -2.3; 5.2 -2.4] |> table ;\n\njulia> modelType   = @load SimpleImputer  pkg = \"BetaML\" verbosity=0\nBetaML.Imputation.SimpleImputer\n\njulia> model     = modelType(norm=1)\nSimpleImputer(\n  statistic = Statistics.mean, \n  norm = 1)\n\njulia> mach      = machine(model, X);\n\njulia> fit!(mach);\n[ Info: Training machine(SimpleImputer(statistic = mean, …), …).\n\njulia> X_full       = transform(mach) |> MLJ.matrix\n9×2 Matrix{Float64}:\n 1.0        10.5\n 1.5         0.295466\n 1.8         8.0\n 1.7        15.0\n 3.2        40.0\n 0.280952    1.69524\n 3.3        38.0\n 0.0750839  -2.3\n 5.2        -2.4\n```\n"""
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "SimpleImputer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":constructor" = "`nothing`"

[BetaML.GaussianMixtureClusterer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"AbstractVector{Float64}\", \"Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Int64\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Continuous}}}}`"
":output_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "BetaML.Bmlj.GaussianMixtureClusterer"
":hyperparameters" = "`(:n_classes, :initial_probmixtures, :mixtures, :tol, :minimum_variance, :minimum_covariance, :initialisation_strategy, :maximum_iterations, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "gaussian mixture clusterer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct GaussianMixtureClusterer <: MLJModelInterface.Unsupervised\n```\n\nA Expectation-Maximisation clustering algorithm with customisable mixtures, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `n_classes::Int64`: Number of mixtures (latent classes) to consider [def: 3]\n  * `initial_probmixtures::AbstractVector{Float64}`: Initial probabilities of the categorical distribution (n_classes x 1) [default: `[]`]\n  * `mixtures::Union{Type, Vector{<:BetaML.GMM.AbstractMixture}}`: An array (of length `n_classes`) of the mixtures to employ (see the [`?GMM`](@ref GMM) module). Each mixture object can be provided with or without its parameters (e.g. mean and variance for the gaussian ones). Fully qualified mixtures are useful only if the `initialisation_strategy` parameter is set to \"gived\". This parameter can also be given symply in term of a *type*. In this case it is automatically extended to a vector of `n_classes` mixtures of the specified type. Note that mixing of different mixture types is not currently supported. [def: `[DiagonalGaussian() for i in 1:n_classes]`]\n  * `tol::Float64`: Tolerance to stop the algorithm [default: 10^(-6)]\n  * `minimum_variance::Float64`: Minimum variance for the mixtures [default: 0.05]\n  * `minimum_covariance::Float64`: Minimum covariance for the mixtures with full covariance matrix [default: 0]. This should be set different than minimum_variance (see notes).\n  * `initialisation_strategy::String`: The computation method of the vector of the initial mixtures. One of the following:\n\n      * \"grid\": using a grid approach\n      * \"given\": using the mixture provided in the fully qualified `mixtures` parameter\n      * \"kmeans\": use first kmeans (itself initialised with a \"grid\" strategy) to set the initial mixture centers [default]\n\n    Note that currently \"random\" and \"shuffle\" initialisations are not supported in gmm-based algorithms.\n  * `maximum_iterations::Int64`: Maximum number of iterations [def: `typemax(Int64)`, i.e. ∞]\n  * `rng::Random.AbstractRNG`: Random Number Generator [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\n\njulia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load GaussianMixtureClusterer pkg = \"BetaML\" verbosity=0\nBetaML.GMM.GaussianMixtureClusterer\n\njulia> model       = modelType()\nGaussianMixtureClusterer(\n  n_classes = 3, \n  initial_probmixtures = Float64[], \n  mixtures = BetaML.GMM.DiagonalGaussian{Float64}[BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing), BetaML.GMM.DiagonalGaussian{Float64}(nothing, nothing)], \n  tol = 1.0e-6, \n  minimum_variance = 0.05, \n  minimum_covariance = 0.0, \n  initialisation_strategy = \"kmeans\", \n  maximum_iterations = 9223372036854775807, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X);\n\njulia> fit!(mach);\n[ Info: Training machine(GaussianMixtureClusterer(n_classes = 3, …), …).\nIter. 1:        Var. of the post  10.800150114964184      Log-likelihood -650.0186451891216\n\njulia> classes_est = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, Int64, UInt32, Float64}:\n UnivariateFinite{Multiclass{3}}(1=>1.0, 2=>4.17e-15, 3=>2.1900000000000003e-31)\n UnivariateFinite{Multiclass{3}}(1=>1.0, 2=>1.25e-13, 3=>5.87e-31)\n UnivariateFinite{Multiclass{3}}(1=>1.0, 2=>4.5e-15, 3=>1.55e-32)\n UnivariateFinite{Multiclass{3}}(1=>1.0, 2=>6.93e-14, 3=>3.37e-31)\n ⋮\n UnivariateFinite{Multiclass{3}}(1=>5.39e-25, 2=>0.0167, 3=>0.983)\n UnivariateFinite{Multiclass{3}}(1=>7.5e-29, 2=>0.000106, 3=>1.0)\n UnivariateFinite{Multiclass{3}}(1=>1.6e-20, 2=>0.594, 3=>0.406)\n```\n"""
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "GaussianMixtureClusterer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}}, AbstractMatrix{<:Union{Missing, ScientificTypesBase.Continuous}}}`"
":transform_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":constructor" = "`nothing`"

[BetaML.KernelPerceptronClassifier]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Function\", \"Int64\", \"Union{Nothing, Vector{Vector{Int64}}}\", \"Bool\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}, AbstractMatrix{<:ScientificTypesBase.Infinite}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "BetaML.Bmlj.KernelPerceptronClassifier"
":hyperparameters" = "`(:kernel, :epochs, :initial_errors, :shuffle, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "kernel perceptron classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct KernelPerceptronClassifier <: MLJModelInterface.Probabilistic\n```\n\nThe kernel perceptron algorithm using one-vs-one for multiclass, from the Beta Machine Learning Toolkit (BetaML).\n\n# Hyperparameters:\n\n  * `kernel::Function`: Kernel function to employ. See `?radial_kernel` or `?polynomial_kernel` (once loaded the BetaML package) for details or check `?BetaML.Utils` to verify if other kernels are defined (you can alsways define your own kernel) [def: [`radial_kernel`](@ref)]\n  * `epochs::Int64`: Maximum number of epochs, i.e. passages trough the whole training sample [def: `100`]\n  * `initial_errors::Union{Nothing, Vector{Vector{Int64}}}`: Initial distribution of the number of errors errors [def: `nothing`, i.e. zeros]. If provided, this should be a nModels-lenght vector of nRecords integer values vectors , where nModels is computed as `(n_classes  * (n_classes - 1)) / 2`\n  * `shuffle::Bool`: Whether to randomly shuffle the data at each iteration (epoch) [def: `true`]\n  * `rng::Random.AbstractRNG`: A Random Number Generator to be used in stochastic parts of the code [deafult: `Random.GLOBAL_RNG`]\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load KernelPerceptronClassifier pkg = \"BetaML\"\n[ Info: For silent loading, specify `verbosity=0`. \nimport BetaML ✔\nBetaML.Perceptron.KernelPerceptronClassifier\n\njulia> model       = modelType()\nKernelPerceptronClassifier(\n  kernel = BetaML.Utils.radial_kernel, \n  epochs = 100, \n  initial_errors = nothing, \n  shuffle = true, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X, y);\n\njulia> fit!(mach);\n\njulia> est_classes = predict(mach, X)\n150-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String, UInt8, Float64}:\n UnivariateFinite{Multiclass{3}}(setosa=>0.665, versicolor=>0.245, virginica=>0.09)\n UnivariateFinite{Multiclass{3}}(setosa=>0.665, versicolor=>0.245, virginica=>0.09)\n ⋮\n UnivariateFinite{Multiclass{3}}(setosa=>0.09, versicolor=>0.245, virginica=>0.665)\n UnivariateFinite{Multiclass{3}}(setosa=>0.09, versicolor=>0.665, virginica=>0.245)\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "KernelPerceptronClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Infinite}}, AbstractMatrix{<:ScientificTypesBase.Infinite}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[BetaML.KMedoidsClusterer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Function\", \"String\", \"Union{Nothing, Matrix{Float64}}\", \"Random.AbstractRNG\")`"
":package_uuid" = "024491cd-cc6b-443e-8034-08ea7eb7db2b"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "BetaML.Bmlj.KMedoidsClusterer"
":hyperparameters" = "`(:n_classes, :dist, :initialisation_strategy, :initial_representatives, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "k medoids clusterer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```julia\nmutable struct KMedoidsClusterer <: MLJModelInterface.Unsupervised\n```\n\n# Parameters:\n\n  * `n_classes::Int64`: Number of classes to discriminate the data [def: 3]\n  * `dist::Function`: Function to employ as distance. Default to the Euclidean distance. Can be one of the predefined distances (`l1_distance`, `l2_distance`, `l2squared_distance`),  `cosine_distance`), any user defined function accepting two vectors and returning a scalar or an anonymous function with the same characteristics.\n  * `initialisation_strategy::String`: The computation method of the vector of the initial representatives. One of the following:\n\n      * \"random\": randomly in the X space\n      * \"grid\": using a grid approach\n      * \"shuffle\": selecting randomly within the available points [default]\n      * \"given\": using a provided set of initial representatives provided in the `initial_representatives` parameter\n\n  * `initial_representatives::Union{Nothing, Matrix{Float64}}`: Provided (K x D) matrix of initial representatives (useful only with `initialisation_strategy=\"given\"`) [default: `nothing`]\n  * `rng::Random.AbstractRNG`: Random Number Generator [deafult: `Random.GLOBAL_RNG`]\n\nThe K-medoids clustering algorithm with customisable distance function, from the Beta Machine Learning Toolkit (BetaML).\n\nSimilar to K-Means, but the \"representatives\" (the cetroids) are guaranteed to be one of the training points. The algorithm work with any arbitrary distance measure.\n\n# Notes:\n\n  * data must be numerical\n  * online fitting (re-fitting with new data) is supported\n\n# Example:\n\n```julia\njulia> using MLJ\n\njulia> X, y        = @load_iris;\n\njulia> modelType   = @load KMedoidsClusterer pkg = \"BetaML\" verbosity=0\nBetaML.Clustering.KMedoidsClusterer\n\njulia> model       = modelType()\nKMedoidsClusterer(\n  n_classes = 3, \n  dist = BetaML.Clustering.var\"#39#41\"(), \n  initialisation_strategy = \"shuffle\", \n  initial_representatives = nothing, \n  rng = Random._GLOBAL_RNG())\n\njulia> mach        = machine(model, X);\n\njulia> fit!(mach);\n[ Info: Training machine(KMedoidsClusterer(n_classes = 3, …), …).\n\njulia> classes_est = predict(mach, X);\n\njulia> hcat(y,classes_est)\n150×2 CategoricalArrays.CategoricalArray{Union{Int64, String},2,UInt32}:\n \"setosa\"     3\n \"setosa\"     3\n \"setosa\"     3\n ⋮            \n \"virginica\"  1\n \"virginica\"  1\n \"virginica\"  2\n```\n"""
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/sylvaticus/BetaML.jl"
":package_name" = "BetaML"
":name" = "KMedoidsClusterer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":fitted_params", ":predict", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractArray{<:ScientificTypesBase.Multiclass}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":constructor" = "`nothing`"

[CatBoost.CatBoostRegressor]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Nothing, String}\", \"Union{Nothing, PythonCall.Core.Py}\", \"Union{Nothing, String}\", \"Union{Nothing, String}\", \"Int64\", \"String\", \"String\", \"Union{Nothing, Int64}\", \"Union{Nothing, String}\", \"Int64\", \"Union{Nothing, Int64}\", \"Int64\", \"Union{Nothing, Int64}\", \"Bool\", \"Union{Nothing, Bool}\", \"Bool\", \"Bool\", \"Union{Nothing, Float64}\", \"Union{Nothing, Int64}\", \"Float64\", \"Union{Nothing, PythonCall.Core.Py, String}\", \"Float64\", \"Float64\", \"Union{Nothing, Int64}\", \"Float64\", \"Int64\", \"Union{Nothing, Bool}\", \"Bool\", \"Union{Nothing, String}\", \"Union{Nothing, PythonCall.Core.Py}\", \"Union{Nothing, PythonCall.Core.Py}\", \"Union{Nothing, PythonCall.Core.Py}\", \"Union{Nothing, Int64}\", \"Union{Nothing, String}\", \"Union{Nothing, String}\", \"Union{Nothing, String}\", \"Union{Nothing, Int64}\", \"String\", \"String\", \"String\", \"Union{Nothing, String}\", \"Union{Nothing, Int64}\", \"String\", \"Int64\", \"Int64\", \"String\", \"Union{Nothing, PythonCall.Core.Py}\", \"Float64\", \"Union{Nothing, Float64}\", \"String\", \"Bool\", \"Float64\", \"Bool\", \"Union{Nothing, Bool}\", \"Union{Nothing, PythonCall.Core.Py}\")`"
":package_uuid" = "e2e10f9a-a85d-4fa9-b6b2-639a32100a12"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}, AbstractVector{<:ScientificTypesBase.Multiclass}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "CatBoost.MLJCatBoostInterface.CatBoostRegressor"
":hyperparameters" = "`(:iterations, :learning_rate, :depth, :l2_leaf_reg, :model_size_reg, :rsm, :loss_function, :border_count, :feature_border_type, :per_float_feature_quantization, :input_borders, :output_borders, :fold_permutation_block, :nan_mode, :counter_calc_method, :leaf_estimation_iterations, :leaf_estimation_method, :thread_count, :random_seed, :metric_period, :ctr_leaf_count_limit, :store_all_simple_ctr, :max_ctr_complexity, :has_time, :allow_const_label, :target_border, :one_hot_max_size, :random_strength, :custom_metric, :bagging_temperature, :fold_len_multiplier, :used_ram_limit, :gpu_ram_part, :pinned_memory_size, :allow_writing_files, :approx_on_full_history, :boosting_type, :simple_ctr, :combinations_ctr, :per_feature_ctr, :ctr_target_border_count, :task_type, :devices, :bootstrap_type, :subsample, :sampling_frequency, :sampling_unit, :gpu_cat_features_storage, :data_partition, :early_stopping_rounds, :grow_policy, :min_data_in_leaf, :max_leaves, :leaf_estimation_backtracking, :feature_weights, :penalties_coefficient, :model_shrink_rate, :model_shrink_mode, :langevin, :diffusion_temperature, :posterior_sampling, :boost_from_average, :text_processing)`"
":is_pure_julia" = "`false`"
":human_name" = "CatBoost regressor"
":is_supervised" = "`true`"
":iteration_parameter" = ":iterations"
":docstring" = """```\nCatBoostRegressor\n```\n\nA model type for constructing a CatBoost regressor, based on [CatBoost.jl](https://github.com/JuliaAI/CatBoost.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nCatBoostRegressor = @load CatBoostRegressor pkg=CatBoost\n```\n\nDo `model = CatBoostRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `CatBoostRegressor(iterations=...)`.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, `Finite`, `Textual`; check column scitypes with `schema(X)`. `Textual` columns will be passed to catboost as `text_features`, `Multiclass` columns will be passed to catboost as `cat_features`, and `OrderedFactor` columns will be converted to integers.\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\nMore details on the catboost hyperparameters, here are the Python docs:  https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier#parameters\n\n# Operations\n\n  * `predict(mach, Xnew)`: probabilistic predictions of the target given new features `Xnew` having the same scitype as `X` above.\n\n# Accessor functions\n\n  * `feature_importances(mach)`: return vector of feature importances, in the form of   `feature::Symbol => importance::Real` pairs\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `model`: The Python CatBoostRegressor model\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `feature_importances`: Vector{Pair{Symbol, Float64}} of feature importances\n\n# Examples\n\n```\nusing CatBoost.MLJCatBoostInterface\nusing MLJ\n\nX = (\n    duration = [1.5, 4.1, 5.0, 6.7], \n    n_phone_calls = [4, 5, 6, 7], \n    department = coerce([\"acc\", \"ops\", \"acc\", \"ops\"], Multiclass), \n)\ny = [2.0, 4.0, 6.0, 7.0]\n\nmodel = CatBoostRegressor(iterations=5)\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n```\n\nSee also [catboost](https://github.com/catboost/catboost) and the unwrapped model type [`CatBoost.CatBoostRegressor`](@ref).\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/CatBoost.jl"
":package_name" = "CatBoost"
":name" = "CatBoostRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":reformat", ":selectrows", ":update"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}, AbstractVector{<:ScientificTypesBase.Multiclass}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[CatBoost.CatBoostClassifier]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Union{Nothing, String}\", \"Union{Nothing, Int64}\", \"Union{Nothing, String}\", \"Union{Nothing, PythonCall.Core.Py}\", \"Union{Nothing, String}\", \"Union{Nothing, String}\", \"Int64\", \"String\", \"String\", \"Union{Nothing, Int64}\", \"Union{Nothing, String}\", \"Int64\", \"Union{Nothing, Int64}\", \"Int64\", \"Union{Nothing, Int64}\", \"Bool\", \"Union{Nothing, Bool}\", \"Bool\", \"Bool\", \"Union{Nothing, Float64}\", \"Union{Nothing, PythonCall.Core.Py}\", \"Union{Nothing, Bool}\", \"Union{Nothing, Int64}\", \"Float64\", \"Float64\", \"Float64\", \"Union{Nothing, Int64}\", \"Float64\", \"Int64\", \"Union{Nothing, Bool}\", \"Bool\", \"Union{Nothing, String}\", \"Union{Nothing, PythonCall.Core.Py}\", \"Union{Nothing, PythonCall.Core.Py}\", \"Union{Nothing, PythonCall.Core.Py}\", \"Union{Nothing, String}\", \"Union{Nothing, String}\", \"Union{Nothing, String}\", \"Union{Nothing, Int64}\", \"String\", \"String\", \"String\", \"Union{Nothing, String}\", \"Union{Nothing, Int64}\", \"String\", \"Int64\", \"Int64\", \"String\", \"Union{Nothing, PythonCall.Core.Py}\", \"Float64\", \"Union{Nothing, Float64}\", \"String\", \"Bool\", \"Float64\", \"Bool\", \"Union{Nothing, Bool}\", \"Union{Nothing, PythonCall.Core.Py}\")`"
":package_uuid" = "e2e10f9a-a85d-4fa9-b6b2-639a32100a12"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}, AbstractVector{<:ScientificTypesBase.Multiclass}}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "CatBoost.MLJCatBoostInterface.CatBoostClassifier"
":hyperparameters" = "`(:iterations, :learning_rate, :depth, :l2_leaf_reg, :model_size_reg, :rsm, :loss_function, :border_count, :feature_border_type, :per_float_feature_quantization, :input_borders, :output_borders, :fold_permutation_block, :nan_mode, :counter_calc_method, :leaf_estimation_iterations, :leaf_estimation_method, :thread_count, :random_seed, :metric_period, :ctr_leaf_count_limit, :store_all_simple_ctr, :max_ctr_complexity, :has_time, :allow_const_label, :target_border, :class_weights, :auto_class_weights, :one_hot_max_size, :random_strength, :bagging_temperature, :fold_len_multiplier, :used_ram_limit, :gpu_ram_part, :pinned_memory_size, :allow_writing_files, :approx_on_full_history, :boosting_type, :simple_ctr, :combinations_ctr, :per_feature_ctr, :task_type, :devices, :bootstrap_type, :subsample, :sampling_frequency, :sampling_unit, :gpu_cat_features_storage, :data_partition, :early_stopping_rounds, :grow_policy, :min_data_in_leaf, :max_leaves, :leaf_estimation_backtracking, :feature_weights, :penalties_coefficient, :model_shrink_rate, :model_shrink_mode, :langevin, :diffusion_temperature, :posterior_sampling, :boost_from_average, :text_processing)`"
":is_pure_julia" = "`false`"
":human_name" = "CatBoost classifier"
":is_supervised" = "`true`"
":iteration_parameter" = ":iterations"
":docstring" = """```\nCatBoostClassifier\n```\n\nA model type for constructing a CatBoost classifier, based on [CatBoost.jl](https://github.com/JuliaAI/CatBoost.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nCatBoostClassifier = @load CatBoostClassifier pkg=CatBoost\n```\n\nDo `model = CatBoostClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `CatBoostClassifier(iterations=...)`.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, `Finite`, `Textual`; check column scitypes with `schema(X)`. `Textual` columns will be passed to catboost as `text_features`, `Multiclass` columns will be passed to catboost as `cat_features`, and `OrderedFactor` columns will be converted to integers.\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `Finite`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\nMore details on the catboost hyperparameters, here are the Python docs:  https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier#parameters\n\n# Operations\n\n  * `predict(mach, Xnew)`: probabilistic predictions of the target given new features `Xnew` having the same scitype as `X` above.\n  * `predict_mode(mach, Xnew)`: returns the mode of each of the prediction above.\n\n# Accessor functions\n\n  * `feature_importances(mach)`: return vector of feature importances, in the form of   `feature::Symbol => importance::Real` pairs\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `model`: The Python CatBoostClassifier model\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `feature_importances`: Vector{Pair{Symbol, Float64}} of feature importances\n\n# Examples\n\n```\nusing CatBoost.MLJCatBoostInterface\nusing MLJ\n\nX = (\n    duration = [1.5, 4.1, 5.0, 6.7], \n    n_phone_calls = [4, 5, 6, 7], \n    department = coerce([\"acc\", \"ops\", \"acc\", \"ops\"], Multiclass), \n)\ny = coerce([0, 0, 1, 1], Multiclass)\n\nmodel = CatBoostClassifier(iterations=5)\nmach = machine(model, X, y)\nfit!(mach)\nprobs = predict(mach, X)\npreds = predict_mode(mach, X)\n```\n\nSee also [catboost](https://github.com/catboost/catboost) and the unwrapped model type [`CatBoost.CatBoostClassifier`](@ref).\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/CatBoost.jl"
":package_name" = "CatBoost"
":name" = "CatBoostClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":predict_mode", ":reformat", ":selectrows", ":update"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.OrderedFactor}, AbstractVector{<:ScientificTypesBase.Multiclass}}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[NearestNeighborModels.KNNClassifier]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Distances.Metric\", \"Int64\", \"Bool\", \"NearestNeighborModels.KNNKernel\")`"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "NearestNeighborModels.KNNClassifier"
":hyperparameters" = "`(:K, :algorithm, :metric, :leafsize, :reorder, :weights)`"
":is_pure_julia" = "`true`"
":human_name" = "K-nearest neighbor classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nKNNClassifier\n```\n\nA model type for constructing a K-nearest neighbor classifier, based on [NearestNeighborModels.jl](https://github.com/JuliaAI/NearestNeighborModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\n```\n\nDo `model = KNNClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `KNNClassifier(K=...)`.\n\nKNNClassifier implements [K-Nearest Neighbors classifier](https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm)  which is non-parametric algorithm that predicts a discrete class distribution associated  with a new point by taking a vote over the classes of the k-nearest points. Each neighbor  vote is assigned a weight based on proximity of the neighbor point to the test point  according to a specified distance metric.\n\nFor more information about the weighting kernels, see the paper by Geler et.al  [Comparison of different weighting schemes for the kNN classifier on time-series data](https://perun.pmf.uns.ac.rs/radovanovic/publications/2016-kais-knn-weighting.pdf). \n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nOR\n\n```\nmach = machine(model, X, y, w)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `<:Finite` (`<:Multiclass` or `<:OrderedFactor` will do); check the scitype with `scitype(y)`\n  * `w` is the observation weights which can either be `nothing` (default) or an  `AbstractVector` whose element scitype is `Count` or `Continuous`. This is  different from `weights` kernel which is a model hyperparameter, see below.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `K::Int=5` : number of neighbors\n  * `algorithm::Symbol = :kdtree` : one of `(:kdtree, :brutetree, :balltree)`\n  * `metric::Metric = Euclidean()` : any `Metric` from    [Distances.jl](https://github.com/JuliaStats/Distances.jl) for the    distance between points. For `algorithm = :kdtree` only metrics which are    instances of `Distances.UnionMinkowskiMetric` are supported.\n  * `leafsize::Int = algorithm == 10` : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as `0`    for `algorithm = :brutetree`, since `brutetree` isn't actually a tree.\n  * `reorder::Bool = true` : if `true` then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to `true`    can significantly improve performance of the specified `algorithm`    (except `:brutetree`). This option is ignored and always taken as `false` for    `algorithm = :brutetree`.\n  * `weights::KNNKernel=Uniform()` : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    `list_kernels()`. User-defined weighting functions can be passed by wrapping the    function in a [`UserDefinedKernel`](@ref) kernel (do `?NearestNeighborModels.UserDefinedKernel` for more    info). If observation weights `w` are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have same scitype as `X` above. Predictions are probabilistic but uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `tree`: An instance of either `KDTree`, `BruteTree` or `BallTree` depending on the  value of the `algorithm` hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.\n\n# Examples\n\n```\nusing MLJ\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nX, y = @load_crabs; # a table and a vector from the crabs dataset\n# view possible kernels\nNearestNeighborModels.list_kernels()\n# KNNClassifier instantiation\nmodel = KNNClassifier(weights = NearestNeighborModels.Inverse())\nmach = machine(model, X, y) |> fit! # wrap model and required data in an MLJ machine and fit\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n\n```\n\nSee also [`MultitargetKNNClassifier`](@ref)\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/NearestNeighborModels.jl"
":package_name" = "NearestNeighborModels"
":name" = "KNNClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`true`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[NearestNeighborModels.MultitargetKNNClassifier]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Distances.Metric\", \"Int64\", \"Bool\", \"NearestNeighborModels.KNNKernel\", \"Type{<:Union{AbstractDict{<:AbstractString, <:AbstractVector}, AbstractDict{Symbol, <:AbstractVector}, NamedTuple{names, T} where {N, names, T<:NTuple{N, AbstractVector}}}}\")`"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Finite}}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Finite}}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "NearestNeighborModels.MultitargetKNNClassifier"
":hyperparameters" = "`(:K, :algorithm, :metric, :leafsize, :reorder, :weights, :output_type)`"
":is_pure_julia" = "`true`"
":human_name" = "multitarget K-nearest neighbor classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nMultitargetKNNClassifier\n```\n\nA model type for constructing a multitarget K-nearest neighbor classifier, based on [NearestNeighborModels.jl](https://github.com/JuliaAI/NearestNeighborModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultitargetKNNClassifier = @load MultitargetKNNClassifier pkg=NearestNeighborModels\n```\n\nDo `model = MultitargetKNNClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultitargetKNNClassifier(K=...)`.\n\nMulti-target K-Nearest Neighbors Classifier (MultitargetKNNClassifier) is a variation of  [`KNNClassifier`](@ref) that assumes the target variable is vector-valued with `Multiclass` or `OrderedFactor` components. (Target data must be presented as a table, however.)\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nOR\n\n```\nmach = machine(model, X, y, w)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * y`is the target, which can be any table of responses whose element scitype is either`<:Finite`(`<:Multiclass`or`<:OrderedFactor`will do); check the columns scitypes with`schema(y)`.  Each column of`y` is assumed to belong to a common categorical pool.\n  * `w` is the observation weights which can either be `nothing`(default) or an  `AbstractVector` whose element scitype is `Count` or `Continuous`. This is different  from `weights` kernel which is a model hyperparameter, see below.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `K::Int=5` : number of neighbors\n  * `algorithm::Symbol = :kdtree` : one of `(:kdtree, :brutetree, :balltree)`\n  * `metric::Metric = Euclidean()` : any `Metric` from    [Distances.jl](https://github.com/JuliaStats/Distances.jl) for the    distance between points. For `algorithm = :kdtree` only metrics which are    instances of `Distances.UnionMinkowskiMetric` are supported.\n  * `leafsize::Int = algorithm == 10` : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as `0`    for `algorithm = :brutetree`, since `brutetree` isn't actually a tree.\n  * `reorder::Bool = true` : if `true` then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to `true`    can significantly improve performance of the specified `algorithm`    (except `:brutetree`). This option is ignored and always taken as `false` for    `algorithm = :brutetree`.\n  * `weights::KNNKernel=Uniform()` : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    `list_kernels()`. User-defined weighting functions can be passed by wrapping the    function in a [`UserDefinedKernel`](@ref) kernel (do `?NearestNeighborModels.UserDefinedKernel` for more    info). If observation weights `w` are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.\n\n  * `output_type::Type{<:MultiUnivariateFinite}=DictTable` : One of    (`ColumnTable`, `DictTable`). The type of table type to use for predictions.   Setting to `ColumnTable` might improve performance for narrow tables while setting to    `DictTable` improves performance for wide tables.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have same scitype as `X` above. Predictions are either a `ColumnTable` or  `DictTable` of `UnivariateFiniteVector` columns depending on the value set for the  `output_type` parameter discussed above. The probabilistic predictions are uncalibrated.\n  * `predict_mode(mach, Xnew)`: Return the modes of each column of the table of probabilistic  predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `tree`: An instance of either `KDTree`, `BruteTree` or `BallTree` depending on the  value of the `algorithm` hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.\n\n# Examples\n\n```\nusing MLJ, StableRNGs\n\n# set rng for reproducibility\nrng = StableRNG(10)\n\n# Dataset generation\nn, p = 10, 3\nX = table(randn(rng, n, p)) # feature table\nfruit, color = categorical([\"apple\", \"orange\"]), categorical([\"blue\", \"green\"])\ny = [(fruit = rand(rng, fruit), color = rand(rng, color)) for _ in 1:n] # target_table\n# Each column in y has a common categorical pool as expected\nselectcols(y, :fruit) # categorical array\nselectcols(y, :color) # categorical array\n\n# Load MultitargetKNNClassifier\nMultitargetKNNClassifier = @load MultitargetKNNClassifier pkg=NearestNeighborModels\n\n# view possible kernels\nNearestNeighborModels.list_kernels()\n\n# MultitargetKNNClassifier instantiation\nmodel = MultitargetKNNClassifier(K=3, weights = NearestNeighborModels.Inverse())\n\n# wrap model and required data in an MLJ machine and fit\nmach = machine(model, X, y) |> fit!\n\n# predict\ny_hat = predict(mach, X)\nlabels = predict_mode(mach, X)\n\n```\n\nSee also [`KNNClassifier`](@ref)\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/NearestNeighborModels.jl"
":package_name" = "NearestNeighborModels"
":name" = "MultitargetKNNClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":predict_mode"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}}`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Finite}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`true`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[NearestNeighborModels.MultitargetKNNRegressor]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Distances.Metric\", \"Int64\", \"Bool\", \"NearestNeighborModels.KNNKernel\")`"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "NearestNeighborModels.MultitargetKNNRegressor"
":hyperparameters" = "`(:K, :algorithm, :metric, :leafsize, :reorder, :weights)`"
":is_pure_julia" = "`true`"
":human_name" = "multitarget K-nearest neighbor regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nMultitargetKNNRegressor\n```\n\nA model type for constructing a multitarget K-nearest neighbor regressor, based on [NearestNeighborModels.jl](https://github.com/JuliaAI/NearestNeighborModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultitargetKNNRegressor = @load MultitargetKNNRegressor pkg=NearestNeighborModels\n```\n\nDo `model = MultitargetKNNRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultitargetKNNRegressor(K=...)`.\n\nMulti-target K-Nearest Neighbors regressor (MultitargetKNNRegressor) is a variation of  [`KNNRegressor`](@ref) that assumes the target variable is vector-valued with `Continuous` components. (Target data must be presented as a table, however.)\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nOR\n\n```\nmach = machine(model, X, y, w)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any table of responses whose element scitype is  `Continuous`; check column scitypes with `schema(y)`.\n  * `w` is the observation weights which can either be `nothing`(default) or an  `AbstractVector` whoose element scitype is `Count` or `Continuous`. This is different  from `weights` kernel which is an hyperparameter to the model, see below.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `K::Int=5` : number of neighbors\n  * `algorithm::Symbol = :kdtree` : one of `(:kdtree, :brutetree, :balltree)`\n  * `metric::Metric = Euclidean()` : any `Metric` from    [Distances.jl](https://github.com/JuliaStats/Distances.jl) for the    distance between points. For `algorithm = :kdtree` only metrics which are    instances of `Distances.UnionMinkowskiMetric` are supported.\n  * `leafsize::Int = algorithm == 10` : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as `0`    for `algorithm = :brutetree`, since `brutetree` isn't actually a tree.\n  * `reorder::Bool = true` : if `true` then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to `true`    can significantly improve performance of the specified `algorithm`    (except `:brutetree`). This option is ignored and always taken as `false` for    `algorithm = :brutetree`.\n  * `weights::KNNKernel=Uniform()` : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    `list_kernels()`. User-defined weighting functions can be passed by wrapping the    function in a [`UserDefinedKernel`](@ref) kernel (do `?NearestNeighborModels.UserDefinedKernel` for more    info). If observation weights `w` are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `tree`: An instance of either `KDTree`, `BruteTree` or `BallTree` depending on the  value of the `algorithm` hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.\n\n# Examples\n\n```\nusing MLJ\n\n# Create Data\nX, y = make_regression(10, 5, n_targets=2)\n\n# load MultitargetKNNRegressor\nMultitargetKNNRegressor = @load MultitargetKNNRegressor pkg=NearestNeighborModels\n\n# view possible kernels\nNearestNeighborModels.list_kernels()\n\n# MutlitargetKNNRegressor instantiation\nmodel = MultitargetKNNRegressor(weights = NearestNeighborModels.Inverse())\n\n# Wrap model and required data in an MLJ machine and fit.\nmach = machine(model, X, y) |> fit! \n\n# Predict\ny_hat = predict(mach, X)\n\n```\n\nSee also [`KNNRegressor`](@ref)\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/NearestNeighborModels.jl"
":package_name" = "NearestNeighborModels"
":name" = "MultitargetKNNRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`true`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[NearestNeighborModels.KNNRegressor]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Symbol\", \"Distances.Metric\", \"Int64\", \"Bool\", \"NearestNeighborModels.KNNKernel\")`"
":package_uuid" = "6f286f6a-111f-5878-ab1e-185364afe411"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "NearestNeighborModels.KNNRegressor"
":hyperparameters" = "`(:K, :algorithm, :metric, :leafsize, :reorder, :weights)`"
":is_pure_julia" = "`true`"
":human_name" = "K-nearest neighbor regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nKNNRegressor\n```\n\nA model type for constructing a K-nearest neighbor regressor, based on [NearestNeighborModels.jl](https://github.com/JuliaAI/NearestNeighborModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKNNRegressor = @load KNNRegressor pkg=NearestNeighborModels\n```\n\nDo `model = KNNRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `KNNRegressor(K=...)`.\n\nKNNRegressor implements [K-Nearest Neighbors regressor](https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm)  which is non-parametric algorithm that predicts the response associated with a new point  by taking an weighted average of the response of the K-nearest points.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nOR\n\n```\nmach = machine(model, X, y, w)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`) whose columns are of scitype `Continuous`; check column scitypes with `schema(X)`.\n  * `y` is the target, which can be any table of responses whose element scitype is    `Continuous`; check the scitype with `scitype(y)`.\n  * `w` is the observation weights which can either be `nothing`(default) or an  `AbstractVector` whoose element scitype is `Count` or `Continuous`. This is different  from `weights` kernel which is an hyperparameter to the model, see below.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `K::Int=5` : number of neighbors\n  * `algorithm::Symbol = :kdtree` : one of `(:kdtree, :brutetree, :balltree)`\n  * `metric::Metric = Euclidean()` : any `Metric` from    [Distances.jl](https://github.com/JuliaStats/Distances.jl) for the    distance between points. For `algorithm = :kdtree` only metrics which are    instances of `Distances.UnionMinkowskiMetric` are supported.\n  * `leafsize::Int = algorithm == 10` : determines the number of points    at which to stop splitting the tree. This option is ignored and always taken as `0`    for `algorithm = :brutetree`, since `brutetree` isn't actually a tree.\n  * `reorder::Bool = true` : if `true` then points which are close in    distance are placed close in memory. In this case, a copy of the original data    will be made so that the original data is left unmodified. Setting this to `true`    can significantly improve performance of the specified `algorithm`    (except `:brutetree`). This option is ignored and always taken as `false` for    `algorithm = :brutetree`.\n  * `weights::KNNKernel=Uniform()` : kernel used in assigning weights to the    k-nearest neighbors for each observation. An instance of one of the types in    `list_kernels()`. User-defined weighting functions can be passed by wrapping the    function in a [`UserDefinedKernel`](@ref) kernel (do `?NearestNeighborModels.UserDefinedKernel` for more    info). If observation weights `w` are passed during machine construction then the    weight assigned to each neighbor vote is the product of the kernel generated    weight for that neighbor and the corresponding observation weight.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew`, which should have same scitype as `X` above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `tree`: An instance of either `KDTree`, `BruteTree` or `BallTree` depending on the  value of the `algorithm` hyperparameter (See hyper-parameters section above).  These are data structures that stores the training data with the view of making  quicker nearest neighbor searches on test data points.\n\n# Examples\n\n```\nusing MLJ\nKNNRegressor = @load KNNRegressor pkg=NearestNeighborModels\nX, y = @load_boston; # loads the crabs dataset from MLJBase\n# view possible kernels\nNearestNeighborModels.list_kernels()\nmodel = KNNRegressor(weights = NearestNeighborModels.Inverse()) #KNNRegressor instantiation\nmach = machine(model, X, y) |> fit! # wrap model and required data in an MLJ machine and fit\ny_hat = predict(mach, X)\n\n```\n\nSee also [`MultitargetKNNRegressor`](@ref)\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/NearestNeighborModels.jl"
":package_name" = "NearestNeighborModels"
":name" = "KNNRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`true`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[MLJXGBoostInterface.XGBoostCount]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"String\", \"Union{Bool, Int64}\", \"Float64\", \"Int64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Float64\", \"Float64\", \"Union{Nothing, String}\", \"Union{Bool, Int64}\", \"String\", \"String\", \"Int64\", \"Int64\", \"String\", \"String\", \"String\", \"Float64\", \"Union{Bool, Int64}\", \"Float64\", \"String\", \"Int64\", \"Float64\", \"Any\", \"Float64\", \"Int64\", \"Any\", \"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Bool\", \"Vector{String}\")`"
":package_uuid" = "009559a3-9522-5dbb-924b-0b6ed2b22bb9"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Count}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "unknown"
":prediction_type" = ":deterministic"
":load_path" = "MLJXGBoostInterface.XGBoostCount"
":hyperparameters" = "`(:test, :num_round, :booster, :disable_default_eval_metric, :eta, :num_parallel_tree, :gamma, :max_depth, :min_child_weight, :max_delta_step, :subsample, :colsample_bytree, :colsample_bylevel, :colsample_bynode, :lambda, :alpha, :tree_method, :sketch_eps, :scale_pos_weight, :updater, :refresh_leaf, :process_type, :grow_policy, :max_leaves, :max_bin, :predictor, :sample_type, :normalize_type, :rate_drop, :one_drop, :skip_drop, :feature_selector, :top_k, :tweedie_variance_power, :objective, :base_score, :early_stopping_rounds, :watchlist, :nthread, :importance_type, :seed, :validate_parameters, :eval_metric)`"
":is_pure_julia" = "`false`"
":human_name" = "eXtreme Gradient Boosting Count Regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nXGBoostCount\n```\n\nA model type for constructing a eXtreme Gradient Boosting Count Regressor, based on [XGBoost.jl](https://github.com/dmlc/XGBoost.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nXGBoostCount = @load XGBoostCount pkg=XGBoost\n```\n\nDo `model = XGBoostCount()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `XGBoostCount(test=...)`.\n\nUnivariate discrete regression using [xgboost](https://xgboost.readthedocs.io/en/stable/index.html).\n\n# Training data\n\nIn `MLJ` or `MLJBase`, bind an instance `model` to data with\n\n```julia\nm = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features, either an `AbstractMatrix` or Tables.jl-compatible table.\n  * `y`: is an `AbstractVector` continuous target.\n\nTrain using `fit!(m, rows=...)`.\n\n# Hyper-parameters\n\nSee https://xgboost.readthedocs.io/en/stable/parameter.html.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/dmlc/XGBoost.jl"
":package_name" = "XGBoost"
":name" = "XGBoostCount"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Count}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Count}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJXGBoostInterface.XGBoostRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"String\", \"Union{Bool, Int64}\", \"Float64\", \"Int64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Float64\", \"Float64\", \"Union{Nothing, String}\", \"Union{Bool, Int64}\", \"String\", \"String\", \"Int64\", \"Int64\", \"String\", \"String\", \"String\", \"Float64\", \"Union{Bool, Int64}\", \"Float64\", \"String\", \"Int64\", \"Float64\", \"Any\", \"Float64\", \"Int64\", \"Any\", \"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Bool\", \"Vector{String}\")`"
":package_uuid" = "009559a3-9522-5dbb-924b-0b6ed2b22bb9"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "unknown"
":prediction_type" = ":deterministic"
":load_path" = "MLJXGBoostInterface.XGBoostRegressor"
":hyperparameters" = "`(:test, :num_round, :booster, :disable_default_eval_metric, :eta, :num_parallel_tree, :gamma, :max_depth, :min_child_weight, :max_delta_step, :subsample, :colsample_bytree, :colsample_bylevel, :colsample_bynode, :lambda, :alpha, :tree_method, :sketch_eps, :scale_pos_weight, :updater, :refresh_leaf, :process_type, :grow_policy, :max_leaves, :max_bin, :predictor, :sample_type, :normalize_type, :rate_drop, :one_drop, :skip_drop, :feature_selector, :top_k, :tweedie_variance_power, :objective, :base_score, :early_stopping_rounds, :watchlist, :nthread, :importance_type, :seed, :validate_parameters, :eval_metric)`"
":is_pure_julia" = "`false`"
":human_name" = "eXtreme Gradient Boosting Regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nXGBoostRegressor\n```\n\nA model type for constructing a eXtreme Gradient Boosting Regressor, based on [XGBoost.jl](https://github.com/dmlc/XGBoost.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nXGBoostRegressor = @load XGBoostRegressor pkg=XGBoost\n```\n\nDo `model = XGBoostRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `XGBoostRegressor(test=...)`.\n\nUnivariate continuous regression using [xgboost](https://xgboost.readthedocs.io/en/stable/index.html).\n\n# Training data\n\nIn `MLJ` or `MLJBase`, bind an instance `model` to data with\n\n```julia\nm = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features whose columns have `Continuous` element scitype; check  column scitypes with `schema(X)`.\n  * `y`: is an `AbstractVector` target with `Continuous` elements; check the scitype with `scitype(y)`.\n\nTrain using `fit!(m, rows=...)`.\n\n# Hyper-parameters\n\nSee https://xgboost.readthedocs.io/en/stable/parameter.html.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/dmlc/XGBoost.jl"
":package_name" = "XGBoost"
":name" = "XGBoostRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`true`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJXGBoostInterface.XGBoostClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"String\", \"Union{Bool, Int64}\", \"Float64\", \"Int64\", \"Float64\", \"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"String\", \"Float64\", \"Float64\", \"Union{Nothing, String}\", \"Union{Bool, Int64}\", \"String\", \"String\", \"Int64\", \"Int64\", \"String\", \"String\", \"String\", \"Float64\", \"Union{Bool, Int64}\", \"Float64\", \"String\", \"Int64\", \"Float64\", \"Any\", \"Float64\", \"Int64\", \"Any\", \"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Bool\", \"Vector{String}\")`"
":package_uuid" = "009559a3-9522-5dbb-924b-0b6ed2b22bb9"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "unknown"
":prediction_type" = ":probabilistic"
":load_path" = "MLJXGBoostInterface.XGBoostClassifier"
":hyperparameters" = "`(:test, :num_round, :booster, :disable_default_eval_metric, :eta, :num_parallel_tree, :gamma, :max_depth, :min_child_weight, :max_delta_step, :subsample, :colsample_bytree, :colsample_bylevel, :colsample_bynode, :lambda, :alpha, :tree_method, :sketch_eps, :scale_pos_weight, :updater, :refresh_leaf, :process_type, :grow_policy, :max_leaves, :max_bin, :predictor, :sample_type, :normalize_type, :rate_drop, :one_drop, :skip_drop, :feature_selector, :top_k, :tweedie_variance_power, :objective, :base_score, :early_stopping_rounds, :watchlist, :nthread, :importance_type, :seed, :validate_parameters, :eval_metric)`"
":is_pure_julia" = "`false`"
":human_name" = "eXtreme Gradient Boosting Classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nXGBoostClassifier\n```\n\nA model type for constructing a eXtreme Gradient Boosting Classifier, based on [XGBoost.jl](https://github.com/dmlc/XGBoost.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nXGBoostClassifier = @load XGBoostClassifier pkg=XGBoost\n```\n\nDo `model = XGBoostClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `XGBoostClassifier(test=...)`.\n\nUnivariate classification using [xgboost](https://xgboost.readthedocs.io/en/stable/index.html).\n\n# Training data\n\nIn `MLJ` or `MLJBase`, bind an instance `model` to data with\n\n```julia\nm = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features, either an `AbstractMatrix` or Tables.jl-compatible table.\n  * `y`: is an `AbstractVector` `Finite` target.\n\nTrain using `fit!(m, rows=...)`.\n\n# Hyper-parameters\n\nSee https://xgboost.readthedocs.io/en/stable/parameter.html.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/dmlc/XGBoost.jl"
":package_name" = "XGBoost"
":name" = "XGBoostClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`true`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"
[MLJModelInterface]

[MLJScikitLearnInterface.ProbabilisticSGDClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"String\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Int64\", \"Union{Nothing, Float64}\", \"Bool\", \"Int64\", \"Float64\", \"Union{Nothing, Int64}\", \"Any\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.ProbabilisticSGDClassifier"
":hyperparameters" = "`(:loss, :penalty, :alpha, :l1_ratio, :fit_intercept, :max_iter, :tol, :shuffle, :verbose, :epsilon, :n_jobs, :random_state, :learning_rate, :eta0, :power_t, :early_stopping, :validation_fraction, :n_iter_no_change, :class_weight, :warm_start, :average)`"
":is_pure_julia" = "`false`"
":human_name" = "probabilistic sgd classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nProbabilisticSGDClassifier\n```\n\nA model type for constructing a probabilistic sgd classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nProbabilisticSGDClassifier = @load ProbabilisticSGDClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = ProbabilisticSGDClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`ProbabilisticSGDClassifier(loss=...)`.\n# Hyper-parameters\n\n- `loss = log_loss`\n\n- `penalty = l2`\n\n- `alpha = 0.0001`\n\n- `l1_ratio = 0.15`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.001`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `epsilon = 0.1`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `learning_rate = optimal`\n\n- `eta0 = 0.0`\n\n- `power_t = 0.5`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `class_weight = nothing`\n\n- `warm_start = false`\n\n- `average = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "ProbabilisticSGDClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.RidgeCVClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"AbstractArray{Float64}\", \"Bool\", \"Any\", \"Int64\", \"Any\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.RidgeCVClassifier"
":hyperparameters" = "`(:alphas, :fit_intercept, :scoring, :cv, :class_weight, :store_cv_values)`"
":is_pure_julia" = "`false`"
":human_name" = "ridge regression classifier with built-in cross-validation"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nRidgeCVClassifier\n```\n\nA model type for constructing a ridge regression classifier with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nRidgeCVClassifier = @load RidgeCVClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = RidgeCVClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RidgeCVClassifier(alphas=...)`.\n# Hyper-parameters\n\n- `alphas = [0.1, 1.0, 10.0]`\n\n- `fit_intercept = true`\n\n- `scoring = nothing`\n\n- `cv = 5`\n\n- `class_weight = nothing`\n\n- `store_cv_values = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "RidgeCVClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.LogisticClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"String\", \"Bool\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Any\", \"Any\", \"String\", \"Int64\", \"String\", \"Int64\", \"Bool\", \"Union{Nothing, Int64}\", \"Union{Nothing, Float64}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.LogisticClassifier"
":hyperparameters" = "`(:penalty, :dual, :tol, :C, :fit_intercept, :intercept_scaling, :class_weight, :random_state, :solver, :max_iter, :multi_class, :verbose, :warm_start, :n_jobs, :l1_ratio)`"
":is_pure_julia" = "`false`"
":human_name" = "logistic regression classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nLogisticClassifier\n```\n\nA model type for constructing a logistic regression classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLogisticClassifier = @load LogisticClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = LogisticClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LogisticClassifier(penalty=...)`.\n# Hyper-parameters\n\n- `penalty = l2`\n\n- `dual = false`\n\n- `tol = 0.0001`\n\n- `C = 1.0`\n\n- `fit_intercept = true`\n\n- `intercept_scaling = 1.0`\n\n- `class_weight = nothing`\n\n- `random_state = nothing`\n\n- `solver = lbfgs`\n\n- `max_iter = 100`\n\n- `multi_class = auto`\n\n- `verbose = 0`\n\n- `warm_start = false`\n\n- `n_jobs = nothing`\n\n- `l1_ratio = nothing`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "LogisticClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.RandomForestRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Union{Nothing, Float64, Int64, String}\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\", \"Bool\", \"Float64\", \"Union{Nothing, Float64, Int64}\", \"Union{Nothing, Dict, Vector}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Continuous}}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.RandomForestRegressor"
":hyperparameters" = "`(:n_estimators, :criterion, :max_depth, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_features, :max_leaf_nodes, :min_impurity_decrease, :bootstrap, :oob_score, :n_jobs, :random_state, :verbose, :warm_start, :ccp_alpha, :max_samples, :monotonic_cst)`"
":is_pure_julia" = "`false`"
":human_name" = "random forest regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nRandomForestRegressor\n```\n\nA model type for constructing a random forest regressor, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRandomForestRegressor = @load RandomForestRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = RandomForestRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `RandomForestRegressor(n_estimators=...)`.\n\nA random forest is a meta estimator that fits a number of  classifying decision trees on various sub-samples of the  dataset and uses averaging to improve the predictive accuracy  and control over-fitting. The sub-sample size is controlled  with the `max_samples` parameter if `bootstrap=True` (default),  otherwise the whole dataset is used to build each tree.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "RandomForestRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.ElasticNetCVRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Union{Float64, Vector{Float64}}\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Any\", \"Bool\", \"Union{Bool, Int64}\", \"Union{Nothing, Int64}\", \"Bool\", \"Any\", \"String\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.ElasticNetCVRegressor"
":hyperparameters" = "`(:l1_ratio, :eps, :n_alphas, :alphas, :fit_intercept, :precompute, :max_iter, :tol, :cv, :copy_X, :verbose, :n_jobs, :positive, :random_state, :selection)`"
":is_pure_julia" = "`false`"
":human_name" = "elastic net regression with built-in cross-validation"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nElasticNetCVRegressor\n```\n\nA model type for constructing a elastic net regression with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nElasticNetCVRegressor = @load ElasticNetCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = ElasticNetCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`ElasticNetCVRegressor(l1_ratio=...)`.\n# Hyper-parameters\n\n- `l1_ratio = 0.5`\n\n- `eps = 0.001`\n\n- `n_alphas = 100`\n\n- `alphas = nothing`\n\n- `fit_intercept = true`\n\n- `precompute = auto`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `cv = 5`\n\n- `copy_X = true`\n\n- `verbose = 0`\n\n- `n_jobs = nothing`\n\n- `positive = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "ElasticNetCVRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.PerceptronClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Union{Nothing, String}\", \"Float64\", \"Bool\", \"Int64\", \"Union{Nothing, Float64}\", \"Bool\", \"Int64\", \"Float64\", \"Union{Nothing, Int64}\", \"Any\", \"Bool\", \"Float64\", \"Int64\", \"Any\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.PerceptronClassifier"
":hyperparameters" = "`(:penalty, :alpha, :fit_intercept, :max_iter, :tol, :shuffle, :verbose, :eta0, :n_jobs, :random_state, :early_stopping, :validation_fraction, :n_iter_no_change, :class_weight, :warm_start)`"
":is_pure_julia" = "`false`"
":human_name" = "perceptron classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nPerceptronClassifier\n```\n\nA model type for constructing a perceptron classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nPerceptronClassifier = @load PerceptronClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = PerceptronClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`PerceptronClassifier(penalty=...)`.\n# Hyper-parameters\n\n- `penalty = nothing`\n\n- `alpha = 0.0001`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.001`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `eta0 = 1.0`\n\n- `n_jobs = nothing`\n\n- `random_state = 0`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `class_weight = nothing`\n\n- `warm_start = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "PerceptronClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.MultiTaskLassoRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Any\", \"String\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.MultiTaskLassoRegressor"
":hyperparameters" = "`(:alpha, :fit_intercept, :max_iter, :tol, :copy_X, :random_state, :selection)`"
":is_pure_julia" = "`false`"
":human_name" = "multi-target lasso regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nMultiTaskLassoRegressor\n```\n\nA model type for constructing a multi-target lasso regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nMultiTaskLassoRegressor = @load MultiTaskLassoRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = MultiTaskLassoRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MultiTaskLassoRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `copy_X = true`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "MultiTaskLassoRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.LinearRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\", \"Union{Nothing, Int64}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.LinearRegressor"
":hyperparameters" = "`(:fit_intercept, :copy_X, :n_jobs)`"
":is_pure_julia" = "`false`"
":human_name" = "ordinary least-squares regressor (OLS)"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nLinearRegressor\n```\n\nA model type for constructing a ordinary least-squares regressor (OLS), based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLinearRegressor = @load LinearRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LinearRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `n_jobs = nothing`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "LinearRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.HDBSCAN]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Nothing, Int64}\", \"Float64\", \"Union{Nothing, Int64}\", \"String\", \"Float64\", \"String\", \"Int64\", \"String\", \"Bool\", \"Union{Nothing, String}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "BSD"
":prediction_type" = ":unknown"
":load_path" = "MLJScikitLearnInterface.HDBSCAN"
":hyperparameters" = "`(:min_cluster_size, :min_samples, :cluster_selection_epsilon, :max_cluster_size, :metric, :alpha, :algorithm, :leaf_size, :cluster_selection_method, :allow_single_cluster, :store_centers)`"
":is_pure_julia" = "`false`"
":human_name" = "hdbscan"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nHDBSCAN\n```\n\nA model type for constructing a hdbscan, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nHDBSCAN = @load HDBSCAN pkg=MLJScikitLearnInterface\n```\n\nDo `model = HDBSCAN()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `HDBSCAN(min_cluster_size=...)`.\n\nHierarchical Density-Based Spatial Clustering of Applications with  Noise. Performs [`DBSCAN`](@ref) over varying epsilon values and  integrates the result to find a clustering that gives the best  stability over epsilon. This allows HDBSCAN to find clusters of  varying densities (unlike [`DBSCAN`](@ref)), and be more robust to  parameter selection. \n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "HDBSCAN"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.DBSCAN]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"String\", \"String\", \"Int64\", \"Union{Nothing, Float64}\", \"Union{Nothing, Int64}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "BSD"
":prediction_type" = ":unknown"
":load_path" = "MLJScikitLearnInterface.DBSCAN"
":hyperparameters" = "`(:eps, :min_samples, :metric, :algorithm, :leaf_size, :p, :n_jobs)`"
":is_pure_julia" = "`false`"
":human_name" = "dbscan"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nDBSCAN\n```\n\nA model type for constructing a dbscan, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDBSCAN = @load DBSCAN pkg=MLJScikitLearnInterface\n```\n\nDo `model = DBSCAN()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DBSCAN(eps=...)`.\n\nDensity-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "DBSCAN"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.RidgeRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Union{Float64, Vector{Float64}}\", \"Bool\", \"Bool\", \"Int64\", \"Float64\", \"String\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.RidgeRegressor"
":hyperparameters" = "`(:alpha, :fit_intercept, :copy_X, :max_iter, :tol, :solver, :random_state)`"
":is_pure_julia" = "`false`"
":human_name" = "ridge regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nRidgeRegressor\n```\n\nA model type for constructing a ridge regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nRidgeRegressor = @load RidgeRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = RidgeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RidgeRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `solver = auto`\n\n- `random_state = nothing`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "RidgeRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.LassoLarsICRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"String\", \"Bool\", \"Union{Bool, Int64}\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Bool\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.LassoLarsICRegressor"
":hyperparameters" = "`(:criterion, :fit_intercept, :verbose, :precompute, :max_iter, :eps, :copy_X, :positive)`"
":is_pure_julia" = "`false`"
":human_name" = "Lasso model with LARS using BIC or AIC for model selection"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nLassoLarsICRegressor\n```\n\nA model type for constructing a Lasso model with LARS using BIC or AIC for model selection, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLassoLarsICRegressor = @load LassoLarsICRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LassoLarsICRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoLarsICRegressor(criterion=...)`.\n# Hyper-parameters\n\n- `criterion = aic`\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `precompute = auto`\n\n- `max_iter = 500`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n- `positive = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "LassoLarsICRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.ARDRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Bool\", \"Bool\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.ARDRegressor"
":hyperparameters" = "`(:max_iter, :tol, :alpha_1, :alpha_2, :lambda_1, :lambda_2, :compute_score, :threshold_lambda, :fit_intercept, :copy_X, :verbose)`"
":is_pure_julia" = "`false`"
":human_name" = "Bayesian ARD regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nARDRegressor\n```\n\nA model type for constructing a Bayesian ARD regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nARDRegressor = @load ARDRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = ARDRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`ARDRegressor(max_iter=...)`.\n# Hyper-parameters\n\n- `max_iter = 300`\n\n- `tol = 0.001`\n\n- `alpha_1 = 1.0e-6`\n\n- `alpha_2 = 1.0e-6`\n\n- `lambda_1 = 1.0e-6`\n\n- `lambda_2 = 1.0e-6`\n\n- `compute_score = false`\n\n- `threshold_lambda = 10000.0`\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `verbose = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "ARDRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.SVMNuRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\", \"Union{Function, String}\", \"Int64\", \"Union{Float64, String}\", \"Float64\", \"Any\", \"Float64\", \"Int64\", \"Int64\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.SVMNuRegressor"
":hyperparameters" = "`(:nu, :C, :kernel, :degree, :gamma, :coef0, :shrinking, :tol, :cache_size, :max_iter)`"
":is_pure_julia" = "`false`"
":human_name" = "nu-support vector regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nSVMNuRegressor\n```\n\nA model type for constructing a nu-support vector regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSVMNuRegressor = @load SVMNuRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = SVMNuRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMNuRegressor(nu=...)`.\n# Hyper-parameters\n\n- `nu = 0.5`\n\n- `C = 1.0`\n\n- `kernel = rbf`\n\n- `degree = 3`\n\n- `gamma = scale`\n\n- `coef0 = 0.0`\n\n- `shrinking = true`\n\n- `tol = 0.001`\n\n- `cache_size = 200`\n\n- `max_iter = -1`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "SVMNuRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.RidgeClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Float64\", \"Any\", \"String\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.RidgeClassifier"
":hyperparameters" = "`(:alpha, :fit_intercept, :copy_X, :max_iter, :tol, :class_weight, :solver, :random_state)`"
":is_pure_julia" = "`false`"
":human_name" = "ridge regression classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nRidgeClassifier\n```\n\nA model type for constructing a ridge regression classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nRidgeClassifier = @load RidgeClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = RidgeClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RidgeClassifier(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `max_iter = nothing`\n\n- `tol = 0.001`\n\n- `class_weight = nothing`\n\n- `solver = auto`\n\n- `random_state = nothing`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "RidgeClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.SGDRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"String\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Union{Bool, Int64}\", \"Float64\", \"Any\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Bool\", \"Union{Bool, Int64}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.SGDRegressor"
":hyperparameters" = "`(:loss, :penalty, :alpha, :l1_ratio, :fit_intercept, :max_iter, :tol, :shuffle, :verbose, :epsilon, :random_state, :learning_rate, :eta0, :power_t, :early_stopping, :validation_fraction, :n_iter_no_change, :warm_start, :average)`"
":is_pure_julia" = "`false`"
":human_name" = "stochastic gradient descent-based regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nSGDRegressor\n```\n\nA model type for constructing a stochastic gradient descent-based regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSGDRegressor = @load SGDRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = SGDRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SGDRegressor(loss=...)`.\n# Hyper-parameters\n\n- `loss = squared_error`\n\n- `penalty = l2`\n\n- `alpha = 0.0001`\n\n- `l1_ratio = 0.15`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.001`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `epsilon = 0.1`\n\n- `random_state = nothing`\n\n- `learning_rate = invscaling`\n\n- `eta0 = 0.01`\n\n- `power_t = 0.25`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `warm_start = false`\n\n- `average = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "SGDRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.ComplementNBClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Union{Nothing, AbstractVector}\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.ComplementNBClassifier"
":hyperparameters" = "`(:alpha, :fit_prior, :class_prior, :norm)`"
":is_pure_julia" = "`false`"
":human_name" = "Complement naive Bayes classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nComplementNBClassifier\n```\n\nA model type for constructing a Complement naive Bayes classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nComplementNBClassifier = @load ComplementNBClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = ComplementNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ComplementNBClassifier(alpha=...)`.\n\nSimilar to [`MultinomialNBClassifier`](@ref) but with more robust assumptions. Suited for imbalanced datasets.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "ComplementNBClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.HuberRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Float64\", \"Bool\", \"Bool\", \"Float64\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.HuberRegressor"
":hyperparameters" = "`(:epsilon, :max_iter, :alpha, :warm_start, :fit_intercept, :tol)`"
":is_pure_julia" = "`false`"
":human_name" = "Huber regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nHuberRegressor\n```\n\nA model type for constructing a Huber regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nHuberRegressor = @load HuberRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = HuberRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`HuberRegressor(epsilon=...)`.\n# Hyper-parameters\n\n- `epsilon = 1.35`\n\n- `max_iter = 100`\n\n- `alpha = 0.0001`\n\n- `warm_start = false`\n\n- `fit_intercept = true`\n\n- `tol = 1.0e-5`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "HuberRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.SVMNuClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Union{Function, String}\", \"Int64\", \"Union{Float64, String}\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Int64\", \"String\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.SVMNuClassifier"
":hyperparameters" = "`(:nu, :kernel, :degree, :gamma, :coef0, :shrinking, :tol, :cache_size, :max_iter, :decision_function_shape, :random_state)`"
":is_pure_julia" = "`false`"
":human_name" = "nu-support vector classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nSVMNuClassifier\n```\n\nA model type for constructing a nu-support vector classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSVMNuClassifier = @load SVMNuClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = SVMNuClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMNuClassifier(nu=...)`.\n# Hyper-parameters\n\n- `nu = 0.5`\n\n- `kernel = rbf`\n\n- `degree = 3`\n\n- `gamma = scale`\n\n- `coef0 = 0.0`\n\n- `shrinking = true`\n\n- `tol = 0.001`\n\n- `cache_size = 200`\n\n- `max_iter = -1`\n\n- `decision_function_shape = ovr`\n\n- `random_state = nothing`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "SVMNuClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.GradientBoostingClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"String\", \"Float64\", \"Int64\", \"Float64\", \"String\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Int64\", \"Float64\", \"Any\", \"Any\", \"Union{Nothing, Float64, Int64, String}\", \"Int64\", \"Union{Nothing, Int64}\", \"Bool\", \"Float64\", \"Union{Nothing, Int64}\", \"Float64\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.GradientBoostingClassifier"
":hyperparameters" = "`(:loss, :learning_rate, :n_estimators, :subsample, :criterion, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_depth, :min_impurity_decrease, :init, :random_state, :max_features, :verbose, :max_leaf_nodes, :warm_start, :validation_fraction, :n_iter_no_change, :tol)`"
":is_pure_julia" = "`false`"
":human_name" = "gradient boosting classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nGradientBoostingClassifier\n```\n\nA model type for constructing a gradient boosting classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nGradientBoostingClassifier = @load GradientBoostingClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = GradientBoostingClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `GradientBoostingClassifier(loss=...)`.\n\nThis algorithm builds an additive model in a forward stage-wise fashion;  it allows for the optimization of arbitrary differentiable loss functions.  In each stage `n_classes_` regression trees are fit on the negative gradient  of the loss function, e.g. binary or multiclass log loss. Binary  classification is a special case where only a single regression tree is induced.\n\n[`HistGradientBoostingClassifier`](@ref) is a much faster variant of this  algorithm for intermediate datasets (`n_samples >= 10_000`).\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "GradientBoostingClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.GaussianProcessRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Any\", \"Union{Float64, AbstractArray}\", \"Any\", \"Int64\", \"Bool\", \"Bool\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.GaussianProcessRegressor"
":hyperparameters" = "`(:kernel, :alpha, :optimizer, :n_restarts_optimizer, :normalize_y, :copy_X_train, :random_state)`"
":is_pure_julia" = "`false`"
":human_name" = "Gaussian process regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nGaussianProcessRegressor\n```\n\nA model type for constructing a Gaussian process regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nGaussianProcessRegressor = @load GaussianProcessRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = GaussianProcessRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`GaussianProcessRegressor(kernel=...)`.\n# Hyper-parameters\n\n- `kernel = nothing`\n\n- `alpha = 1.0e-10`\n\n- `optimizer = fmin_l_bfgs_b`\n\n- `n_restarts_optimizer = 0`\n\n- `normalize_y = false`\n\n- `copy_X_train = true`\n\n- `random_state = nothing`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "GaussianProcessRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.SVMLinearRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\", \"Float64\", \"String\", \"Bool\", \"Float64\", \"Bool\", \"Any\", \"Int64\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.SVMLinearRegressor"
":hyperparameters" = "`(:epsilon, :tol, :C, :loss, :fit_intercept, :intercept_scaling, :dual, :random_state, :max_iter)`"
":is_pure_julia" = "`false`"
":human_name" = "linear support vector regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nSVMLinearRegressor\n```\n\nA model type for constructing a linear support vector regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSVMLinearRegressor = @load SVMLinearRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = SVMLinearRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMLinearRegressor(epsilon=...)`.\n# Hyper-parameters\n\n- `epsilon = 0.0`\n\n- `tol = 0.0001`\n\n- `C = 1.0`\n\n- `loss = epsilon_insensitive`\n\n- `fit_intercept = true`\n\n- `intercept_scaling = 1.0`\n\n- `dual = true`\n\n- `random_state = nothing`\n\n- `max_iter = 1000`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "SVMLinearRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.LarsRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Bool\", \"Union{Bool, Int64}\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Bool\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.LarsRegressor"
":hyperparameters" = "`(:fit_intercept, :verbose, :precompute, :n_nonzero_coefs, :eps, :copy_X, :fit_path)`"
":is_pure_julia" = "`false`"
":human_name" = "least angle regressor (LARS)"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nLarsRegressor\n```\n\nA model type for constructing a least angle regressor (LARS), based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLarsRegressor = @load LarsRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LarsRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LarsRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `precompute = auto`\n\n- `n_nonzero_coefs = 500`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n- `fit_path = true`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "LarsRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.MeanShift]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Union{Nothing, Float64}\", \"Union{Nothing, AbstractArray}\", \"Bool\", \"Int64\", \"Bool\", \"Union{Nothing, Int64}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "BSD"
":prediction_type" = ":unknown"
":load_path" = "MLJScikitLearnInterface.MeanShift"
":hyperparameters" = "`(:bandwidth, :seeds, :bin_seeding, :min_bin_freq, :cluster_all, :n_jobs)`"
":is_pure_julia" = "`false`"
":human_name" = "mean shift"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nMeanShift\n```\n\nA model type for constructing a mean shift, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMeanShift = @load MeanShift pkg=MLJScikitLearnInterface\n```\n\nDo `model = MeanShift()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MeanShift(bandwidth=...)`.\n\nMean shift clustering using a flat kernel. Mean shift clustering aims to discover \"blobs\" in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.\"\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "MeanShift"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.HistGradientBoostingClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"String\", \"Float64\", \"Int64\", \"Union{Nothing, Int64}\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Int64\", \"Union{Nothing, Vector}\", \"Union{Nothing, Dict, Vector}\", \"Any\", \"Bool\", \"Union{Bool, String}\", \"String\", \"Union{Nothing, Float64, Int64}\", \"Union{Nothing, Int64}\", \"Float64\", \"Any\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.HistGradientBoostingClassifier"
":hyperparameters" = "`(:loss, :learning_rate, :max_iter, :max_leaf_nodes, :max_depth, :min_samples_leaf, :l2_regularization, :max_bins, :categorical_features, :monotonic_cst, :interaction_cst, :warm_start, :early_stopping, :scoring, :validation_fraction, :n_iter_no_change, :tol, :random_state, :class_weight)`"
":is_pure_julia" = "`false`"
":human_name" = "hist gradient boosting classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nHistGradientBoostingClassifier\n```\n\nA model type for constructing a hist gradient boosting classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nHistGradientBoostingClassifier = @load HistGradientBoostingClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = HistGradientBoostingClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `HistGradientBoostingClassifier(loss=...)`.\n\nThis algorithm builds an additive model in a forward stage-wise fashion;  it allows for the optimization of arbitrary differentiable loss functions.  In each stage `n_classes_` regression trees are fit on the negative gradient  of the loss function, e.g. binary or multiclass log loss. Binary  classification is a special case where only a single regression tree is induced.\n\n[`HistGradientBoostingClassifier`](@ref) is a much faster variant of this  algorithm for intermediate datasets (`n_samples >= 10_000`).\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "HistGradientBoostingClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.AdaBoostRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Any\", \"Int64\", \"Float64\", \"String\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.AdaBoostRegressor"
":hyperparameters" = "`(:estimator, :n_estimators, :learning_rate, :loss, :random_state)`"
":is_pure_julia" = "`false`"
":human_name" = "AdaBoost ensemble regression"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nAdaBoostRegressor\n```\n\nA model type for constructing a AdaBoost ensemble regression, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nAdaBoostRegressor = @load AdaBoostRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = AdaBoostRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `AdaBoostRegressor(estimator=...)`.\n\nAn AdaBoost regressor is a meta-estimator that begins by fitting  a regressor on the original dataset and then fits additional  copies of the regressor on the same dataset but where the weights  of instances are adjusted according to the error of the current  prediction. As such, subsequent regressors focus more on difficult  cases.\n\nThis class implements the algorithm known as AdaBoost.R2.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "AdaBoostRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.AffinityPropagation]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Int64\", \"Bool\", \"Any\", \"String\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "BSD"
":prediction_type" = ":unknown"
":load_path" = "MLJScikitLearnInterface.AffinityPropagation"
":hyperparameters" = "`(:damping, :max_iter, :convergence_iter, :copy, :preference, :affinity, :verbose)`"
":is_pure_julia" = "`false`"
":human_name" = "Affinity Propagation Clustering of data"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nAffinityPropagation\n```\n\nA model type for constructing a Affinity Propagation Clustering of data, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nAffinityPropagation = @load AffinityPropagation pkg=MLJScikitLearnInterface\n```\n\nDo `model = AffinityPropagation()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`AffinityPropagation(damping=...)`.\n# Hyper-parameters\n\n- `damping = 0.5`\n\n- `max_iter = 200`\n\n- `convergence_iter = 15`\n\n- `copy = true`\n\n- `preference = nothing`\n\n- `affinity = euclidean`\n\n- `verbose = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "AffinityPropagation"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.MultiTaskLassoCVRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Any\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Any\", \"Union{Bool, Int64}\", \"Union{Nothing, Int64}\", \"Any\", \"String\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.MultiTaskLassoCVRegressor"
":hyperparameters" = "`(:eps, :n_alphas, :alphas, :fit_intercept, :max_iter, :tol, :copy_X, :cv, :verbose, :n_jobs, :random_state, :selection)`"
":is_pure_julia" = "`false`"
":human_name" = "multi-target lasso regressor with built-in cross-validation"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nMultiTaskLassoCVRegressor\n```\n\nA model type for constructing a multi-target lasso regressor with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nMultiTaskLassoCVRegressor = @load MultiTaskLassoCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = MultiTaskLassoCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MultiTaskLassoCVRegressor(eps=...)`.\n# Hyper-parameters\n\n- `eps = 0.001`\n\n- `n_alphas = 100`\n\n- `alphas = nothing`\n\n- `fit_intercept = true`\n\n- `max_iter = 300`\n\n- `tol = 0.0001`\n\n- `copy_X = true`\n\n- `cv = 5`\n\n- `verbose = false`\n\n- `n_jobs = 1`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "MultiTaskLassoCVRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.OrthogonalMatchingPursuitRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Union{Nothing, Int64}\", \"Union{Nothing, Float64}\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.OrthogonalMatchingPursuitRegressor"
":hyperparameters" = "`(:n_nonzero_coefs, :tol, :fit_intercept, :precompute)`"
":is_pure_julia" = "`false`"
":human_name" = "orthogonal matching pursuit regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nOrthogonalMatchingPursuitRegressor\n```\n\nA model type for constructing a orthogonal matching pursuit regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nOrthogonalMatchingPursuitRegressor = @load OrthogonalMatchingPursuitRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = OrthogonalMatchingPursuitRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`OrthogonalMatchingPursuitRegressor(n_nonzero_coefs=...)`.\n# Hyper-parameters\n\n- `n_nonzero_coefs = nothing`\n\n- `tol = nothing`\n\n- `fit_intercept = true`\n\n- `precompute = auto`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "OrthogonalMatchingPursuitRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.BernoulliNBClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Union{Nothing, Float64}\", \"Bool\", \"Union{Nothing, AbstractVector}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.BernoulliNBClassifier"
":hyperparameters" = "`(:alpha, :binarize, :fit_prior, :class_prior)`"
":is_pure_julia" = "`false`"
":human_name" = "Bernoulli naive Bayes classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nBernoulliNBClassifier\n```\n\nA model type for constructing a Bernoulli naive Bayes classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBernoulliNBClassifier = @load BernoulliNBClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = BernoulliNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `BernoulliNBClassifier(alpha=...)`.\n\nBinomial naive bayes classifier. It is suitable for classification with binary features; features will be binarized based on the `binarize` keyword (unless it's `nothing` in which case the features are assumed to be binary).\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "BernoulliNBClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.PassiveAggressiveClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Bool\", \"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Any\", \"Bool\", \"Any\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.PassiveAggressiveClassifier"
":hyperparameters" = "`(:C, :fit_intercept, :max_iter, :tol, :early_stopping, :validation_fraction, :n_iter_no_change, :shuffle, :verbose, :loss, :n_jobs, :random_state, :warm_start, :class_weight, :average)`"
":is_pure_julia" = "`false`"
":human_name" = "passive aggressive classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nPassiveAggressiveClassifier\n```\n\nA model type for constructing a passive aggressive classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nPassiveAggressiveClassifier = @load PassiveAggressiveClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = PassiveAggressiveClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`PassiveAggressiveClassifier(C=...)`.\n# Hyper-parameters\n\n- `C = 1.0`\n\n- `fit_intercept = true`\n\n- `max_iter = 100`\n\n- `tol = 0.001`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `loss = hinge`\n\n- `n_jobs = nothing`\n\n- `random_state = 0`\n\n- `warm_start = false`\n\n- `class_weight = nothing`\n\n- `average = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "PassiveAggressiveClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.RidgeCVRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Any\", \"Bool\", \"Any\", \"Any\", \"Union{Nothing, String}\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.RidgeCVRegressor"
":hyperparameters" = "`(:alphas, :fit_intercept, :scoring, :cv, :gcv_mode, :store_cv_values)`"
":is_pure_julia" = "`false`"
":human_name" = "ridge regressor with built-in cross-validation"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nRidgeCVRegressor\n```\n\nA model type for constructing a ridge regressor with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nRidgeCVRegressor = @load RidgeCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = RidgeCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RidgeCVRegressor(alphas=...)`.\n# Hyper-parameters\n\n- `alphas = (0.1, 1.0, 10.0)`\n\n- `fit_intercept = true`\n\n- `scoring = nothing`\n\n- `cv = 5`\n\n- `gcv_mode = nothing`\n\n- `store_cv_values = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "RidgeCVRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.SVMRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Union{Function, String}\", \"Int64\", \"Union{Float64, String}\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Any\", \"Int64\", \"Int64\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.SVMRegressor"
":hyperparameters" = "`(:kernel, :degree, :gamma, :coef0, :tol, :C, :epsilon, :shrinking, :cache_size, :max_iter)`"
":is_pure_julia" = "`false`"
":human_name" = "epsilon-support vector regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nSVMRegressor\n```\n\nA model type for constructing a epsilon-support vector regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSVMRegressor = @load SVMRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = SVMRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMRegressor(kernel=...)`.\n# Hyper-parameters\n\n- `kernel = rbf`\n\n- `degree = 3`\n\n- `gamma = scale`\n\n- `coef0 = 0.0`\n\n- `tol = 0.001`\n\n- `C = 1.0`\n\n- `epsilon = 0.1`\n\n- `shrinking = true`\n\n- `cache_size = 200`\n\n- `max_iter = -1`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "SVMRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.GaussianNBClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Union{Nothing, AbstractVector{Float64}}\", \"Float64\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.GaussianNBClassifier"
":hyperparameters" = "`(:priors, :var_smoothing)`"
":is_pure_julia" = "`false`"
":human_name" = "Gaussian naive Bayes classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nGaussianNBClassifier\n```\n\nA model type for constructing a Gaussian naive Bayes classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nGaussianNBClassifier = @load GaussianNBClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = GaussianNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`GaussianNBClassifier(priors=...)`.\n# Hyper-parameters\n\n- `priors = nothing`\n\n- `var_smoothing = 1.0e-9`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "GaussianNBClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.ExtraTreesClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Union{Nothing, Float64, Int64, String}\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\", \"Bool\", \"Any\", \"Float64\", \"Union{Nothing, Float64, Int64}\", \"Union{Nothing, Dict, Vector}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.ExtraTreesClassifier"
":hyperparameters" = "`(:n_estimators, :criterion, :max_depth, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_features, :max_leaf_nodes, :min_impurity_decrease, :bootstrap, :oob_score, :n_jobs, :random_state, :verbose, :warm_start, :class_weight, :ccp_alpha, :max_samples, :monotonic_cst)`"
":is_pure_julia" = "`false`"
":human_name" = "extra trees classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nExtraTreesClassifier\n```\n\nA model type for constructing a extra trees classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nExtraTreesClassifier = @load ExtraTreesClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = ExtraTreesClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ExtraTreesClassifier(n_estimators=...)`.\n\nExtra trees classifier, fits a number of randomized decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "ExtraTreesClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.KMeans]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Int64, String}\", \"Int64\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"String\", \"Union{String, AbstractArray}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "BSD"
":prediction_type" = ":unknown"
":load_path" = "MLJScikitLearnInterface.KMeans"
":hyperparameters" = "`(:n_clusters, :n_init, :max_iter, :tol, :verbose, :random_state, :copy_x, :algorithm, :init)`"
":is_pure_julia" = "`false`"
":human_name" = "k means"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nKMeans\n```\n\nA model type for constructing a k means, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nKMeans = @load KMeans pkg=MLJScikitLearnInterface\n```\n\nDo `model = KMeans()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `KMeans(n_clusters=...)`.\n\nK-Means algorithm: find K centroids corresponding to K clusters in the data.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "KMeans"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.MultiTaskElasticNetCVRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Union{Float64, Vector{Float64}}\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"Int64\", \"Float64\", \"Any\", \"Bool\", \"Union{Bool, Int64}\", \"Union{Nothing, Int64}\", \"Any\", \"String\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.MultiTaskElasticNetCVRegressor"
":hyperparameters" = "`(:l1_ratio, :eps, :n_alphas, :alphas, :fit_intercept, :max_iter, :tol, :cv, :copy_X, :verbose, :n_jobs, :random_state, :selection)`"
":is_pure_julia" = "`false`"
":human_name" = "multi-target elastic net regressor with built-in cross-validation"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nMultiTaskElasticNetCVRegressor\n```\n\nA model type for constructing a multi-target elastic net regressor with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nMultiTaskElasticNetCVRegressor = @load MultiTaskElasticNetCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = MultiTaskElasticNetCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MultiTaskElasticNetCVRegressor(l1_ratio=...)`.\n# Hyper-parameters\n\n- `l1_ratio = 0.5`\n\n- `eps = 0.001`\n\n- `n_alphas = 100`\n\n- `alphas = nothing`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `cv = 5`\n\n- `copy_X = true`\n\n- `verbose = 0`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "MultiTaskElasticNetCVRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.LassoLarsCVRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Bool\", \"Union{Bool, Int64}\", \"Int64\", \"Union{Bool, String, AbstractMatrix}\", \"Any\", \"Int64\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.LassoLarsCVRegressor"
":hyperparameters" = "`(:fit_intercept, :verbose, :max_iter, :precompute, :cv, :max_n_alphas, :n_jobs, :eps, :copy_X, :positive)`"
":is_pure_julia" = "`false`"
":human_name" = "Lasso model fit with least angle regression (LARS) with built-in cross-validation"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nLassoLarsCVRegressor\n```\n\nA model type for constructing a Lasso model fit with least angle regression (LARS) with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLassoLarsCVRegressor = @load LassoLarsCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LassoLarsCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoLarsCVRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `max_iter = 500`\n\n- `precompute = auto`\n\n- `cv = 5`\n\n- `max_n_alphas = 1000`\n\n- `n_jobs = nothing`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n- `positive = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "LassoLarsCVRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.OrthogonalMatchingPursuitCVRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Union{Nothing, Int64}\", \"Union{Bool, Int64}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.OrthogonalMatchingPursuitCVRegressor"
":hyperparameters" = "`(:copy, :fit_intercept, :max_iter, :cv, :n_jobs, :verbose)`"
":is_pure_julia" = "`false`"
":human_name" = "orthogonal ,atching pursuit (OMP) model with built-in cross-validation"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nOrthogonalMatchingPursuitCVRegressor\n```\n\nA model type for constructing a orthogonal ,atching pursuit (OMP) model with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nOrthogonalMatchingPursuitCVRegressor = @load OrthogonalMatchingPursuitCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = OrthogonalMatchingPursuitCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`OrthogonalMatchingPursuitCVRegressor(copy=...)`.\n# Hyper-parameters\n\n- `copy = true`\n\n- `fit_intercept = true`\n\n- `max_iter = nothing`\n\n- `cv = 5`\n\n- `n_jobs = 1`\n\n- `verbose = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "OrthogonalMatchingPursuitCVRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.AdaBoostClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Any\", \"Int64\", \"Float64\", \"String\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.AdaBoostClassifier"
":hyperparameters" = "`(:estimator, :n_estimators, :learning_rate, :algorithm, :random_state)`"
":is_pure_julia" = "`false`"
":human_name" = "ada boost classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nAdaBoostClassifier\n```\n\nA model type for constructing a ada boost classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nAdaBoostClassifier = @load AdaBoostClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = AdaBoostClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `AdaBoostClassifier(estimator=...)`.\n\nAn AdaBoost  classifier is a meta-estimator that begins by fitting a  classifier on the original dataset and then fits additional copies of  the classifier on the same dataset but where the weights of incorrectly  classified instances are adjusted such that subsequent classifiers  focus more on difficult cases.\n\nThis class implements the algorithm known as AdaBoost-SAMME.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "AdaBoostClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.PassiveAggressiveRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Bool\", \"Union{Bool, Int64}\", \"String\", \"Float64\", \"Any\", \"Bool\", \"Union{Bool, Int64}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.PassiveAggressiveRegressor"
":hyperparameters" = "`(:C, :fit_intercept, :max_iter, :tol, :early_stopping, :validation_fraction, :n_iter_no_change, :shuffle, :verbose, :loss, :epsilon, :random_state, :warm_start, :average)`"
":is_pure_julia" = "`false`"
":human_name" = "passive aggressive regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nPassiveAggressiveRegressor\n```\n\nA model type for constructing a passive aggressive regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nPassiveAggressiveRegressor = @load PassiveAggressiveRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = PassiveAggressiveRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`PassiveAggressiveRegressor(C=...)`.\n# Hyper-parameters\n\n- `C = 1.0`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `loss = epsilon_insensitive`\n\n- `epsilon = 0.1`\n\n- `random_state = nothing`\n\n- `warm_start = false`\n\n- `average = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "PassiveAggressiveRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.BayesianRidgeRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Float64\", \"Bool\", \"Bool\", \"Bool\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.BayesianRidgeRegressor"
":hyperparameters" = "`(:max_iter, :tol, :alpha_1, :alpha_2, :lambda_1, :lambda_2, :compute_score, :fit_intercept, :copy_X, :verbose)`"
":is_pure_julia" = "`false`"
":human_name" = "Bayesian ridge regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nBayesianRidgeRegressor\n```\n\nA model type for constructing a Bayesian ridge regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nBayesianRidgeRegressor = @load BayesianRidgeRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = BayesianRidgeRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`BayesianRidgeRegressor(max_iter=...)`.\n# Hyper-parameters\n\n- `max_iter = 300`\n\n- `tol = 0.001`\n\n- `alpha_1 = 1.0e-6`\n\n- `alpha_2 = 1.0e-6`\n\n- `lambda_1 = 1.0e-6`\n\n- `lambda_2 = 1.0e-6`\n\n- `compute_score = false`\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `verbose = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "BayesianRidgeRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.GaussianProcessClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Any\", \"Any\", \"Int64\", \"Bool\", \"Any\", \"Int64\", \"Bool\", \"String\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.GaussianProcessClassifier"
":hyperparameters" = "`(:kernel, :optimizer, :n_restarts_optimizer, :copy_X_train, :random_state, :max_iter_predict, :warm_start, :multi_class)`"
":is_pure_julia" = "`false`"
":human_name" = "Gaussian process classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nGaussianProcessClassifier\n```\n\nA model type for constructing a Gaussian process classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nGaussianProcessClassifier = @load GaussianProcessClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = GaussianProcessClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`GaussianProcessClassifier(kernel=...)`.\n# Hyper-parameters\n\n- `kernel = nothing`\n\n- `optimizer = fmin_l_bfgs_b`\n\n- `n_restarts_optimizer = 0`\n\n- `copy_X_train = true`\n\n- `random_state = nothing`\n\n- `max_iter_predict = 100`\n\n- `warm_start = false`\n\n- `multi_class = one_vs_rest`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "GaussianProcessClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.BaggingClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Any\", \"Int64\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Bool\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.BaggingClassifier"
":hyperparameters" = "`(:estimator, :n_estimators, :max_samples, :max_features, :bootstrap, :bootstrap_features, :oob_score, :warm_start, :n_jobs, :random_state, :verbose)`"
":is_pure_julia" = "`false`"
":human_name" = "bagging ensemble classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nBaggingClassifier\n```\n\nA model type for constructing a bagging ensemble classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBaggingClassifier = @load BaggingClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = BaggingClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `BaggingClassifier(estimator=...)`.\n\nA Bagging classifier is an ensemble meta-estimator that fits base  classifiers each on random subsets of the original dataset and then  aggregate their individual predictions (either by voting or by  averaging) to form a final prediction. Such a meta-estimator can  typically be used as a way to reduce the variance of a black-box  estimator (e.g., a decision tree), by introducing randomization into  its construction procedure and then making an ensemble out of it.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "BaggingClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.OPTICS]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Union{Float64, Int64}\", \"Float64\", \"String\", \"Int64\", \"String\", \"Union{Nothing, Float64}\", \"Float64\", \"Bool\", \"Union{Nothing, Float64, Int64}\", \"String\", \"Int64\", \"Union{Nothing, Int64}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "BSD"
":prediction_type" = ":unknown"
":load_path" = "MLJScikitLearnInterface.OPTICS"
":hyperparameters" = "`(:min_samples, :max_eps, :metric, :p, :cluster_method, :eps, :xi, :predecessor_correction, :min_cluster_size, :algorithm, :leaf_size, :n_jobs)`"
":is_pure_julia" = "`false`"
":human_name" = "optics"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nOPTICS\n```\n\nA model type for constructing a optics, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nOPTICS = @load OPTICS pkg=MLJScikitLearnInterface\n```\n\nDo `model = OPTICS()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `OPTICS(min_samples=...)`.\n\nOPTICS (Ordering Points To Identify the Clustering Structure), closely related to [`DBSCAN'](@ref), finds core sample of high density and expands clusters from them. Unlike DBSCAN, keeps cluster hierarchy for a variable neighborhood radius. Better suited for usage on large datasets than the current sklearn implementation of DBSCAN.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "OPTICS"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.RANSACRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Any\", \"Union{Float64, Int64}\", \"Union{Nothing, Float64}\", \"Any\", \"Any\", \"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Float64\", \"Union{Function, String}\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.RANSACRegressor"
":hyperparameters" = "`(:estimator, :min_samples, :residual_threshold, :is_data_valid, :is_model_valid, :max_trials, :max_skips, :stop_n_inliers, :stop_score, :stop_probability, :loss, :random_state)`"
":is_pure_julia" = "`false`"
":human_name" = "ransac regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nRANSACRegressor\n```\n\nA model type for constructing a ransac regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nRANSACRegressor = @load RANSACRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = RANSACRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`RANSACRegressor(estimator=...)`.\n# Hyper-parameters\n\n- `estimator = nothing`\n\n- `min_samples = 5`\n\n- `residual_threshold = nothing`\n\n- `is_data_valid = nothing`\n\n- `is_model_valid = nothing`\n\n- `max_trials = 100`\n\n- `max_skips = 9223372036854775807`\n\n- `stop_n_inliers = 9223372036854775807`\n\n- `stop_score = Inf`\n\n- `stop_probability = 0.99`\n\n- `loss = absolute_error`\n\n- `random_state = nothing`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "RANSACRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.KNeighborsRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Function, String}\", \"String\", \"Int64\", \"Int64\", \"Any\", \"Any\", \"Union{Nothing, Int64}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.KNeighborsRegressor"
":hyperparameters" = "`(:n_neighbors, :weights, :algorithm, :leaf_size, :p, :metric, :metric_params, :n_jobs)`"
":is_pure_julia" = "`false`"
":human_name" = "K-nearest neighbors regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nKNeighborsRegressor\n```\n\nA model type for constructing a K-nearest neighbors regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nKNeighborsRegressor = @load KNeighborsRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = KNeighborsRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`KNeighborsRegressor(n_neighbors=...)`.\n# Hyper-parameters\n\n- `n_neighbors = 5`\n\n- `weights = uniform`\n\n- `algorithm = auto`\n\n- `leaf_size = 30`\n\n- `p = 2`\n\n- `metric = minkowski`\n\n- `metric_params = nothing`\n\n- `n_jobs = nothing`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "KNeighborsRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.HistGradientBoostingRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"String\", \"Union{Nothing, Float64}\", \"Float64\", \"Int64\", \"Union{Nothing, Int64}\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Int64\", \"Union{Nothing, Vector}\", \"Union{Nothing, Dict, Vector}\", \"Any\", \"Bool\", \"Union{Bool, String}\", \"String\", \"Union{Nothing, Float64, Int64}\", \"Union{Nothing, Int64}\", \"Float64\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.HistGradientBoostingRegressor"
":hyperparameters" = "`(:loss, :quantile, :learning_rate, :max_iter, :max_leaf_nodes, :max_depth, :min_samples_leaf, :l2_regularization, :max_bins, :categorical_features, :monotonic_cst, :interaction_cst, :warm_start, :early_stopping, :scoring, :validation_fraction, :n_iter_no_change, :tol, :random_state)`"
":is_pure_julia" = "`false`"
":human_name" = "gradient boosting ensemble regression"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nHistGradientBoostingRegressor\n```\n\nA model type for constructing a gradient boosting ensemble regression, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nHistGradientBoostingRegressor = @load HistGradientBoostingRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = HistGradientBoostingRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `HistGradientBoostingRegressor(loss=...)`.\n\nThis estimator builds an additive model in a forward stage-wise fashion;  it allows for the optimization of arbitrary differentiable loss functions.  In each stage a regression tree is fit on the negative gradient of the  given loss function.\n\n[`HistGradientBoostingRegressor`](@ref) is a much faster variant of this  algorithm for intermediate datasets (`n_samples >= 10_000`).\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "HistGradientBoostingRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.MiniBatchKMeans]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Int64\", \"Int64\", \"Bool\", \"Any\", \"Float64\", \"Int64\", \"Union{Nothing, Int64}\", \"Union{Int64, String}\", \"Union{String, AbstractArray}\", \"Float64\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "BSD"
":prediction_type" = ":unknown"
":load_path" = "MLJScikitLearnInterface.MiniBatchKMeans"
":hyperparameters" = "`(:n_clusters, :max_iter, :batch_size, :verbose, :compute_labels, :random_state, :tol, :max_no_improvement, :init_size, :n_init, :init, :reassignment_ratio)`"
":is_pure_julia" = "`false`"
":human_name" = "Mini-Batch K-Means clustering."
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nMiniBatchKMeans\n```\n\nA model type for constructing a Mini-Batch K-Means clustering., based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nMiniBatchKMeans = @load MiniBatchKMeans pkg=MLJScikitLearnInterface\n```\n\nDo `model = MiniBatchKMeans()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MiniBatchKMeans(n_clusters=...)`.\n# Hyper-parameters\n\n- `n_clusters = 8`\n\n- `max_iter = 100`\n\n- `batch_size = 100`\n\n- `verbose = 0`\n\n- `compute_labels = true`\n\n- `random_state = nothing`\n\n- `tol = 0.0`\n\n- `max_no_improvement = 10`\n\n- `init_size = nothing`\n\n- `n_init = 3`\n\n- `init = k-means++`\n\n- `reassignment_ratio = 0.01`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "MiniBatchKMeans"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.LassoCVRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Any\", \"Bool\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Bool\", \"Any\", \"Union{Bool, Int64}\", \"Union{Nothing, Int64}\", \"Bool\", \"Any\", \"String\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.LassoCVRegressor"
":hyperparameters" = "`(:eps, :n_alphas, :alphas, :fit_intercept, :precompute, :max_iter, :tol, :copy_X, :cv, :verbose, :n_jobs, :positive, :random_state, :selection)`"
":is_pure_julia" = "`false`"
":human_name" = "lasso regressor with built-in cross-validation"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nLassoCVRegressor\n```\n\nA model type for constructing a lasso regressor with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLassoCVRegressor = @load LassoCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LassoCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoCVRegressor(eps=...)`.\n# Hyper-parameters\n\n- `eps = 0.001`\n\n- `n_alphas = 100`\n\n- `alphas = nothing`\n\n- `fit_intercept = true`\n\n- `precompute = auto`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `copy_X = true`\n\n- `cv = 5`\n\n- `verbose = false`\n\n- `n_jobs = nothing`\n\n- `positive = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "LassoCVRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.DummyRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"String\", \"Any\", \"Float64\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.DummyRegressor"
":hyperparameters" = "`(:strategy, :constant, :quantile)`"
":is_pure_julia" = "`false`"
":human_name" = "dummy regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nDummyRegressor\n```\n\nA model type for constructing a dummy regressor, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDummyRegressor = @load DummyRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = DummyRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DummyRegressor(strategy=...)`.\n\nDummyRegressor is a regressor that makes predictions using simple rules.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "DummyRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.BisectingKMeans]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Int64\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"String\", \"Union{String, AbstractArray}\", \"String\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "BSD"
":prediction_type" = ":unknown"
":load_path" = "MLJScikitLearnInterface.BisectingKMeans"
":hyperparameters" = "`(:n_clusters, :n_init, :max_iter, :tol, :verbose, :random_state, :copy_x, :algorithm, :init, :bisecting_strategy)`"
":is_pure_julia" = "`false`"
":human_name" = "bisecting k means"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nBisectingKMeans\n```\n\nA model type for constructing a bisecting k means, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBisectingKMeans = @load BisectingKMeans pkg=MLJScikitLearnInterface\n```\n\nDo `model = BisectingKMeans()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `BisectingKMeans(n_clusters=...)`.\n\nBisecting K-Means clustering.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "BisectingKMeans"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.LassoLarsRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Union{Bool, Int64}\", \"Union{Bool, String, AbstractMatrix}\", \"Int64\", \"Float64\", \"Bool\", \"Bool\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.LassoLarsRegressor"
":hyperparameters" = "`(:alpha, :fit_intercept, :verbose, :precompute, :max_iter, :eps, :copy_X, :fit_path, :positive)`"
":is_pure_julia" = "`false`"
":human_name" = "Lasso model fit with least angle regression (LARS)"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nLassoLarsRegressor\n```\n\nA model type for constructing a Lasso model fit with least angle regression (LARS), based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLassoLarsRegressor = @load LassoLarsRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LassoLarsRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoLarsRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `precompute = auto`\n\n- `max_iter = 500`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n- `fit_path = true`\n\n- `positive = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "LassoLarsRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.LarsCVRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Bool\", \"Union{Bool, Int64}\", \"Int64\", \"Union{Bool, String, AbstractMatrix}\", \"Any\", \"Int64\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.LarsCVRegressor"
":hyperparameters" = "`(:fit_intercept, :verbose, :max_iter, :precompute, :cv, :max_n_alphas, :n_jobs, :eps, :copy_X)`"
":is_pure_julia" = "`false`"
":human_name" = "least angle regressor with built-in cross-validation"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nLarsCVRegressor\n```\n\nA model type for constructing a least angle regressor with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLarsCVRegressor = @load LarsCVRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LarsCVRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LarsCVRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `verbose = false`\n\n- `max_iter = 500`\n\n- `precompute = auto`\n\n- `cv = 5`\n\n- `max_n_alphas = 1000`\n\n- `n_jobs = nothing`\n\n- `eps = 2.220446049250313e-16`\n\n- `copy_X = true`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "LarsCVRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.KNeighborsClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Function, String}\", \"String\", \"Int64\", \"Int64\", \"Any\", \"Any\", \"Union{Nothing, Int64}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.KNeighborsClassifier"
":hyperparameters" = "`(:n_neighbors, :weights, :algorithm, :leaf_size, :p, :metric, :metric_params, :n_jobs)`"
":is_pure_julia" = "`false`"
":human_name" = "K-nearest neighbors classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nKNeighborsClassifier\n```\n\nA model type for constructing a K-nearest neighbors classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nKNeighborsClassifier = @load KNeighborsClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = KNeighborsClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`KNeighborsClassifier(n_neighbors=...)`.\n# Hyper-parameters\n\n- `n_neighbors = 5`\n\n- `weights = uniform`\n\n- `algorithm = auto`\n\n- `leaf_size = 30`\n\n- `p = 2`\n\n- `metric = minkowski`\n\n- `metric_params = nothing`\n\n- `n_jobs = nothing`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "KNeighborsClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.SVMLinearClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"String\", \"String\", \"Bool\", \"Float64\", \"Float64\", \"String\", \"Bool\", \"Float64\", \"Any\", \"Int64\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.SVMLinearClassifier"
":hyperparameters" = "`(:penalty, :loss, :dual, :tol, :C, :multi_class, :fit_intercept, :intercept_scaling, :random_state, :max_iter)`"
":is_pure_julia" = "`false`"
":human_name" = "linear support vector classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nSVMLinearClassifier\n```\n\nA model type for constructing a linear support vector classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSVMLinearClassifier = @load SVMLinearClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = SVMLinearClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMLinearClassifier(penalty=...)`.\n# Hyper-parameters\n\n- `penalty = l2`\n\n- `loss = squared_hinge`\n\n- `dual = true`\n\n- `tol = 0.0001`\n\n- `C = 1.0`\n\n- `multi_class = ovr`\n\n- `fit_intercept = true`\n\n- `intercept_scaling = 1.0`\n\n- `random_state = nothing`\n\n- `max_iter = 1000`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "SVMLinearClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.FeatureAgglomeration]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Any\", \"Any\", \"Any\", \"Union{Bool, String}\", \"String\", \"Union{Nothing, Float64}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "BSD"
":prediction_type" = ":unknown"
":load_path" = "MLJScikitLearnInterface.FeatureAgglomeration"
":hyperparameters" = "`(:n_clusters, :memory, :connectivity, :metric, :compute_full_tree, :linkage, :distance_threshold)`"
":is_pure_julia" = "`false`"
":human_name" = "feature agglomeration"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nFeatureAgglomeration\n```\n\nA model type for constructing a feature agglomeration, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nFeatureAgglomeration = @load FeatureAgglomeration pkg=MLJScikitLearnInterface\n```\n\nDo `model = FeatureAgglomeration()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `FeatureAgglomeration(n_clusters=...)`.\n\nSimilar to [`AgglomerativeClustering`](@ref), but recursively merges features instead of samples.\"\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "FeatureAgglomeration"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.DummyClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"String\", \"Any\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.DummyClassifier"
":hyperparameters" = "`(:strategy, :constant, :random_state)`"
":is_pure_julia" = "`false`"
":human_name" = "dummy classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nDummyClassifier\n```\n\nA model type for constructing a dummy classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nDummyClassifier = @load DummyClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = DummyClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DummyClassifier(strategy=...)`.\n\nDummyClassifier is a classifier that makes predictions using simple rules.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "DummyClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.BaggingRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Any\", \"Int64\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Bool\", \"Bool\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.BaggingRegressor"
":hyperparameters" = "`(:estimator, :n_estimators, :max_samples, :max_features, :bootstrap, :bootstrap_features, :oob_score, :warm_start, :n_jobs, :random_state, :verbose)`"
":is_pure_julia" = "`false`"
":human_name" = "bagging ensemble regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nBaggingRegressor\n```\n\nA model type for constructing a bagging ensemble regressor, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBaggingRegressor = @load BaggingRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = BaggingRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `BaggingRegressor(estimator=...)`.\n\nA Bagging regressor is an ensemble meta-estimator that fits base  regressors each on random subsets of the original dataset and then  aggregate their individual predictions (either by voting or by  averaging) to form a final prediction. Such a meta-estimator can  typically be used as a way to reduce the variance of a black-box  estimator (e.g., a decision tree), by introducing randomization  into its construction procedure and then making an ensemble out  of it.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "BaggingRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.BayesianQDA]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Union{Nothing, AbstractVector}\", \"Float64\", \"Bool\", \"Float64\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.BayesianQDA"
":hyperparameters" = "`(:priors, :reg_param, :store_covariance, :tol)`"
":is_pure_julia" = "`false`"
":human_name" = "Bayesian quadratic discriminant analysis"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nBayesianQDA\n```\n\nA model type for constructing a Bayesian quadratic discriminant analysis, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nBayesianQDA = @load BayesianQDA pkg=MLJScikitLearnInterface\n```\n\nDo `model = BayesianQDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`BayesianQDA(priors=...)`.\n# Hyper-parameters\n\n- `priors = nothing`\n\n- `reg_param = 0.0`\n\n- `store_covariance = false`\n\n- `tol = 0.0001`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "BayesianQDA"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.BayesianLDA]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"String\", \"Union{Nothing, Float64, String}\", \"Union{Nothing, AbstractVector}\", \"Union{Nothing, Int64}\", \"Bool\", \"Float64\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.BayesianLDA"
":hyperparameters" = "`(:solver, :shrinkage, :priors, :n_components, :store_covariance, :tol, :covariance_estimator)`"
":is_pure_julia" = "`false`"
":human_name" = "Bayesian linear discriminant analysis"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nBayesianLDA\n```\n\nA model type for constructing a Bayesian linear discriminant analysis, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nBayesianLDA = @load BayesianLDA pkg=MLJScikitLearnInterface\n```\n\nDo `model = BayesianLDA()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`BayesianLDA(solver=...)`.\n# Hyper-parameters\n\n- `solver = svd`\n\n- `shrinkage = nothing`\n\n- `priors = nothing`\n\n- `n_components = nothing`\n\n- `store_covariance = false`\n\n- `tol = 0.0001`\n\n- `covariance_estimator = nothing`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "BayesianLDA"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.SGDClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"String\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Int64\", \"Union{Nothing, Float64}\", \"Bool\", \"Int64\", \"Float64\", \"Union{Nothing, Int64}\", \"Any\", \"String\", \"Float64\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Any\", \"Bool\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.SGDClassifier"
":hyperparameters" = "`(:loss, :penalty, :alpha, :l1_ratio, :fit_intercept, :max_iter, :tol, :shuffle, :verbose, :epsilon, :n_jobs, :random_state, :learning_rate, :eta0, :power_t, :early_stopping, :validation_fraction, :n_iter_no_change, :class_weight, :warm_start, :average)`"
":is_pure_julia" = "`false`"
":human_name" = "sgd classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nSGDClassifier\n```\n\nA model type for constructing a sgd classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSGDClassifier = @load SGDClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = SGDClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SGDClassifier(loss=...)`.\n# Hyper-parameters\n\n- `loss = hinge`\n\n- `penalty = l2`\n\n- `alpha = 0.0001`\n\n- `l1_ratio = 0.15`\n\n- `fit_intercept = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.001`\n\n- `shuffle = true`\n\n- `verbose = 0`\n\n- `epsilon = 0.1`\n\n- `n_jobs = nothing`\n\n- `random_state = nothing`\n\n- `learning_rate = optimal`\n\n- `eta0 = 0.0`\n\n- `power_t = 0.5`\n\n- `early_stopping = false`\n\n- `validation_fraction = 0.1`\n\n- `n_iter_no_change = 5`\n\n- `class_weight = nothing`\n\n- `warm_start = false`\n\n- `average = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "SGDClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.TheilSenRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\", \"Int64\", \"Union{Nothing, Int64}\", \"Int64\", \"Float64\", \"Any\", \"Union{Nothing, Int64}\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.TheilSenRegressor"
":hyperparameters" = "`(:fit_intercept, :copy_X, :max_subpopulation, :n_subsamples, :max_iter, :tol, :random_state, :n_jobs, :verbose)`"
":is_pure_julia" = "`false`"
":human_name" = "Theil-Sen regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nTheilSenRegressor\n```\n\nA model type for constructing a Theil-Sen regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nTheilSenRegressor = @load TheilSenRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = TheilSenRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`TheilSenRegressor(fit_intercept=...)`.\n# Hyper-parameters\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `max_subpopulation = 10000`\n\n- `n_subsamples = nothing`\n\n- `max_iter = 300`\n\n- `tol = 0.001`\n\n- `random_state = nothing`\n\n- `n_jobs = nothing`\n\n- `verbose = false`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "TheilSenRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.SpectralClustering]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Nothing, String}\", \"Any\", \"Int64\", \"Float64\", \"String\", \"Int64\", \"Float64\", \"String\", \"Union{Nothing, Int64}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "BSD"
":prediction_type" = ":unknown"
":load_path" = "MLJScikitLearnInterface.SpectralClustering"
":hyperparameters" = "`(:n_clusters, :eigen_solver, :random_state, :n_init, :gamma, :affinity, :n_neighbors, :eigen_tol, :assign_labels, :n_jobs)`"
":is_pure_julia" = "`false`"
":human_name" = "spectral clustering"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nSpectralClustering\n```\n\nA model type for constructing a spectral clustering, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nSpectralClustering = @load SpectralClustering pkg=MLJScikitLearnInterface\n```\n\nDo `model = SpectralClustering()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `SpectralClustering(n_clusters=...)`.\n\nApply clustering to a projection of the normalized Laplacian.  In practice spectral clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plane.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "SpectralClustering"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.Birch]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Int64\", \"Int64\", \"Bool\", \"Bool\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "BSD"
":prediction_type" = ":unknown"
":load_path" = "MLJScikitLearnInterface.Birch"
":hyperparameters" = "`(:threshold, :branching_factor, :n_clusters, :compute_labels, :copy)`"
":is_pure_julia" = "`false`"
":human_name" = "birch"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nBirch\n```\n\nA model type for constructing a birch, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nBirch = @load Birch pkg=MLJScikitLearnInterface\n```\n\nDo `model = Birch()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `Birch(threshold=...)`.\n\nMemory-efficient, online-learning algorithm provided as an alternative to MiniBatchKMeans. Note: noisy samples are given the label -1.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "Birch"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Multiclass}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.AgglomerativeClustering]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Any\", \"Any\", \"Union{Bool, String}\", \"String\", \"Union{Nothing, Float64}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "BSD"
":prediction_type" = ":unknown"
":load_path" = "MLJScikitLearnInterface.AgglomerativeClustering"
":hyperparameters" = "`(:n_clusters, :metric, :memory, :connectivity, :compute_full_tree, :linkage, :distance_threshold)`"
":is_pure_julia" = "`false`"
":human_name" = "agglomerative clustering"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nAgglomerativeClustering\n```\n\nA model type for constructing a agglomerative clustering, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nAgglomerativeClustering = @load AgglomerativeClustering pkg=MLJScikitLearnInterface\n```\n\nDo `model = AgglomerativeClustering()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `AgglomerativeClustering(n_clusters=...)`.\n\nRecursively merges the pair of clusters that minimally increases a given linkage distance. Note: there is no `predict` or `transform`. Instead, inspect the `fitted_params`.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "AgglomerativeClustering"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.ElasticNetRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Float64\", \"Bool\", \"Union{Bool, AbstractMatrix}\", \"Int64\", \"Bool\", \"Float64\", \"Bool\", \"Bool\", \"Any\", \"String\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.ElasticNetRegressor"
":hyperparameters" = "`(:alpha, :l1_ratio, :fit_intercept, :precompute, :max_iter, :copy_X, :tol, :warm_start, :positive, :random_state, :selection)`"
":is_pure_julia" = "`false`"
":human_name" = "elastic net regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nElasticNetRegressor\n```\n\nA model type for constructing a elastic net regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nElasticNetRegressor = @load ElasticNetRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = ElasticNetRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`ElasticNetRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `l1_ratio = 0.5`\n\n- `fit_intercept = true`\n\n- `precompute = false`\n\n- `max_iter = 1000`\n\n- `copy_X = true`\n\n- `tol = 0.0001`\n\n- `warm_start = false`\n\n- `positive = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "ElasticNetRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.RandomForestClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Union{Nothing, Float64, Int64, String}\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\", \"Bool\", \"Any\", \"Float64\", \"Union{Nothing, Float64, Int64}\", \"Union{Nothing, Dict, Vector}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Continuous}}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.RandomForestClassifier"
":hyperparameters" = "`(:n_estimators, :criterion, :max_depth, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_features, :max_leaf_nodes, :min_impurity_decrease, :bootstrap, :oob_score, :n_jobs, :random_state, :verbose, :warm_start, :class_weight, :ccp_alpha, :max_samples, :monotonic_cst)`"
":is_pure_julia" = "`false`"
":human_name" = "random forest classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nRandomForestClassifier\n```\n\nA model type for constructing a random forest classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nRandomForestClassifier = @load RandomForestClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = RandomForestClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `RandomForestClassifier(n_estimators=...)`.\n\nA random forest is a meta estimator that fits a number of  classifying decision trees on various sub-samples of the  dataset and uses averaging to improve the predictive accuracy  and control over-fitting. The sub-sample size is controlled  with the `max_samples` parameter if `bootstrap=True` (default),  otherwise the whole dataset is used to build each tree.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "RandomForestClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.LogisticCVClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Union{Int64, AbstractVector{Float64}}\", \"Bool\", \"Any\", \"Bool\", \"String\", \"Any\", \"String\", \"Float64\", \"Int64\", \"Any\", \"Union{Nothing, Int64}\", \"Int64\", \"Bool\", \"Float64\", \"String\", \"Any\", \"Union{Nothing, AbstractVector{Float64}}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.LogisticCVClassifier"
":hyperparameters" = "`(:Cs, :fit_intercept, :cv, :dual, :penalty, :scoring, :solver, :tol, :max_iter, :class_weight, :n_jobs, :verbose, :refit, :intercept_scaling, :multi_class, :random_state, :l1_ratios)`"
":is_pure_julia" = "`false`"
":human_name" = "logistic regression classifier with built-in cross-validation"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nLogisticCVClassifier\n```\n\nA model type for constructing a logistic regression classifier with built-in cross-validation, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLogisticCVClassifier = @load LogisticCVClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = LogisticCVClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LogisticCVClassifier(Cs=...)`.\n# Hyper-parameters\n\n- `Cs = 10`\n\n- `fit_intercept = true`\n\n- `cv = 5`\n\n- `dual = false`\n\n- `penalty = l2`\n\n- `scoring = nothing`\n\n- `solver = lbfgs`\n\n- `tol = 0.0001`\n\n- `max_iter = 100`\n\n- `class_weight = nothing`\n\n- `n_jobs = nothing`\n\n- `verbose = 0`\n\n- `refit = true`\n\n- `intercept_scaling = 1.0`\n\n- `multi_class = auto`\n\n- `random_state = nothing`\n\n- `l1_ratios = nothing`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "LogisticCVClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.MultiTaskElasticNetRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Union{Float64, Vector{Float64}}\", \"Bool\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Any\", \"String\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.MultiTaskElasticNetRegressor"
":hyperparameters" = "`(:alpha, :l1_ratio, :fit_intercept, :copy_X, :max_iter, :tol, :warm_start, :random_state, :selection)`"
":is_pure_julia" = "`false`"
":human_name" = "multi-target elastic net regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nMultiTaskElasticNetRegressor\n```\n\nA model type for constructing a multi-target elastic net regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nMultiTaskElasticNetRegressor = @load MultiTaskElasticNetRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = MultiTaskElasticNetRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`MultiTaskElasticNetRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `l1_ratio = 0.5`\n\n- `fit_intercept = true`\n\n- `copy_X = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `warm_start = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "MultiTaskElasticNetRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":target_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.ExtraTreesRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"String\", \"Union{Nothing, Int64}\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Union{Nothing, Float64, Int64, String}\", \"Union{Nothing, Int64}\", \"Float64\", \"Bool\", \"Bool\", \"Union{Nothing, Int64}\", \"Any\", \"Int64\", \"Bool\", \"Float64\", \"Union{Nothing, Float64, Int64}\", \"Union{Nothing, Dict, Vector}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.ExtraTreesRegressor"
":hyperparameters" = "`(:n_estimators, :criterion, :max_depth, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_features, :max_leaf_nodes, :min_impurity_decrease, :bootstrap, :oob_score, :n_jobs, :random_state, :verbose, :warm_start, :ccp_alpha, :max_samples, :monotonic_cst)`"
":is_pure_julia" = "`false`"
":human_name" = "extra trees regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nExtraTreesRegressor\n```\n\nA model type for constructing a extra trees regressor, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nExtraTreesRegressor = @load ExtraTreesRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = ExtraTreesRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ExtraTreesRegressor(n_estimators=...)`.\n\nExtra trees regressor, fits a number of randomized decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "ExtraTreesRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.LassoRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Union{Bool, AbstractMatrix}\", \"Bool\", \"Int64\", \"Float64\", \"Bool\", \"Bool\", \"Any\", \"String\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.LassoRegressor"
":hyperparameters" = "`(:alpha, :fit_intercept, :precompute, :copy_X, :max_iter, :tol, :warm_start, :positive, :random_state, :selection)`"
":is_pure_julia" = "`false`"
":human_name" = "lasso regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nLassoRegressor\n```\n\nA model type for constructing a lasso regressor, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nLassoRegressor = @load LassoRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = LassoRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`LassoRegressor(alpha=...)`.\n# Hyper-parameters\n\n- `alpha = 1.0`\n\n- `fit_intercept = true`\n\n- `precompute = false`\n\n- `copy_X = true`\n\n- `max_iter = 1000`\n\n- `tol = 0.0001`\n\n- `warm_start = false`\n\n- `positive = false`\n\n- `random_state = nothing`\n\n- `selection = cyclic`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "LassoRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.MultinomialNBClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Bool\", \"Union{Nothing, AbstractVector}\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "BSD"
":prediction_type" = ":probabilistic"
":load_path" = "MLJScikitLearnInterface.MultinomialNBClassifier"
":hyperparameters" = "`(:alpha, :fit_prior, :class_prior)`"
":is_pure_julia" = "`false`"
":human_name" = "multinomial naive Bayes classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nMultinomialNBClassifier\n```\n\nA model type for constructing a multinomial naive Bayes classifier, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMultinomialNBClassifier = @load MultinomialNBClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = MultinomialNBClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MultinomialNBClassifier(alpha=...)`.\n\nMultinomial naive bayes classifier. It is suitable for classification with discrete features (e.g. word counts for text classification).\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "MultinomialNBClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Count}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.GradientBoostingRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"String\", \"Float64\", \"Int64\", \"Float64\", \"String\", \"Union{Float64, Int64}\", \"Union{Float64, Int64}\", \"Float64\", \"Int64\", \"Float64\", \"Any\", \"Any\", \"Union{Nothing, Float64, Int64, String}\", \"Float64\", \"Int64\", \"Union{Nothing, Int64}\", \"Bool\", \"Float64\", \"Union{Nothing, Int64}\", \"Float64\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.GradientBoostingRegressor"
":hyperparameters" = "`(:loss, :learning_rate, :n_estimators, :subsample, :criterion, :min_samples_split, :min_samples_leaf, :min_weight_fraction_leaf, :max_depth, :min_impurity_decrease, :init, :random_state, :max_features, :alpha, :verbose, :max_leaf_nodes, :warm_start, :validation_fraction, :n_iter_no_change, :tol)`"
":is_pure_julia" = "`false`"
":human_name" = "gradient boosting ensemble regression"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nGradientBoostingRegressor\n```\n\nA model type for constructing a gradient boosting ensemble regression, based on [MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nGradientBoostingRegressor = @load GradientBoostingRegressor pkg=MLJScikitLearnInterface\n```\n\nDo `model = GradientBoostingRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `GradientBoostingRegressor(loss=...)`.\n\nThis estimator builds an additive model in a forward stage-wise fashion;  it allows for the optimization of arbitrary differentiable loss functions.  In each stage a regression tree is fit on the negative gradient of the  given loss function.\n\n[`HistGradientBoostingRegressor`](@ref) is a much faster variant of this  algorithm for intermediate datasets (`n_samples >= 10_000`).\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "GradientBoostingRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`true`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJScikitLearnInterface.SVMClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Float64\", \"Union{Function, String}\", \"Int64\", \"Union{Float64, String}\", \"Float64\", \"Bool\", \"Float64\", \"Int64\", \"Int64\", \"String\", \"Any\")`"
":package_uuid" = "3646fa90-6ef7-5e7e-9f22-8aca16db6324"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "BSD"
":prediction_type" = ":deterministic"
":load_path" = "MLJScikitLearnInterface.SVMClassifier"
":hyperparameters" = "`(:C, :kernel, :degree, :gamma, :coef0, :shrinking, :tol, :cache_size, :max_iter, :decision_function_shape, :random_state)`"
":is_pure_julia" = "`false`"
":human_name" = "C-support vector classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nSVMClassifier\n```\n\nA model type for constructing a C-support vector classifier, based on\n[MLJScikitLearnInterface.jl](https://github.com/JuliaAI/MLJScikitLearnInterface.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nSVMClassifier = @load SVMClassifier pkg=MLJScikitLearnInterface\n```\n\nDo `model = SVMClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in\n`SVMClassifier(C=...)`.\n# Hyper-parameters\n\n- `C = 1.0`\n\n- `kernel = rbf`\n\n- `degree = 3`\n\n- `gamma = scale`\n\n- `coef0 = 0.0`\n\n- `shrinking = true`\n\n- `tol = 0.001`\n\n- `cache_size = 200`\n\n- `max_iter = -1`\n\n- `decision_function_shape = ovr`\n\n- `random_state = nothing`\n\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJScikitLearnInterface.jl"
":package_name" = "MLJScikitLearnInterface"
":name" = "SVMClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[OutlierDetectionNeighbors.ABODDetector]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Integer\", \"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\", \"Bool\")`"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "OutlierDetectionNeighbors.ABODDetector"
":hyperparameters" = "`(:k, :metric, :algorithm, :static, :leafsize, :reorder, :parallel, :enhanced)`"
":is_pure_julia" = "`true`"
":human_name" = "abod detector"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nABODDetector(k = 5,\n             metric = Euclidean(),\n             algorithm = :kdtree,\n             static = :auto,\n             leafsize = 10,\n             reorder = true,\n             parallel = false,\n             enhanced = false)\n```\n\nDetermine outliers based on the angles to its nearest neighbors. This implements the `FastABOD` variant described in the paper, that is, it uses the variance of angles to its nearest neighbors, not to the whole dataset, see [1]. \n\n*Notice:* The scores are inverted, to conform to our notion that higher scores describe higher outlierness.\n\n## Parameters\n\n```\nk::Integer\n```\n\nNumber of neighbors (must be greater than 0).\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n```\nenhanced::Bool\n```\n\nWhen `enhanced=true`, it uses the enhanced ABOD (EABOD) adaptation proposed by [2].\n\n## Examples\n\n```julia\nusing OutlierDetection: ABODDetector, fit, transform\ndetector = ABODDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)\n```\n\n## References\n\n[1] Kriegel, Hans-Peter; S hubert, Matthias; Zimek, Arthur (2008): Angle-based outlier detection in high-dimensional data.\n\n[2] Li, Xiaojie; Lv, Jian Cheng; Cheng, Dongdong (2015): Angle-Based Outlier Detection Algorithm with More Stable Relationships.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":package_name" = "OutlierDetectionNeighbors"
":name" = "ABODDetector"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":constructor" = "`nothing`"

[OutlierDetectionNeighbors.DNNDetector]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\", \"Real\")`"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "OutlierDetectionNeighbors.DNNDetector"
":hyperparameters" = "`(:metric, :algorithm, :static, :leafsize, :reorder, :parallel, :d)`"
":is_pure_julia" = "`true`"
":human_name" = "dnn detector"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nDNNDetector(d = 0,\n            metric = Euclidean(),\n            algorithm = :kdtree,\n            leafsize = 10,\n            reorder = true,\n            parallel = false)\n```\n\nAnomaly score based on the number of neighbors in a hypersphere of radius `d`. Knorr et al. [1] directly converted the resulting outlier scores to labels, thus this implementation does not fully reflect the approach from the paper.\n\n## Parameters\n\n```\nd::Real\n```\n\nThe hypersphere radius used to calculate the global density of an instance.\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n## Examples\n\n```julia\nusing OutlierDetection: DNNDetector, fit, transform\ndetector = DNNDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)\n```\n\n## References\n\n[1] Knorr, Edwin M.; Ng, Raymond T. (1998): Algorithms for Mining Distance-Based Outliers in Large Datasets.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":package_name" = "OutlierDetectionNeighbors"
":name" = "DNNDetector"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":constructor" = "`nothing`"

[OutlierDetectionNeighbors.LOFDetector]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Integer\", \"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\")`"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "OutlierDetectionNeighbors.LOFDetector"
":hyperparameters" = "`(:k, :metric, :algorithm, :static, :leafsize, :reorder, :parallel)`"
":is_pure_julia" = "`true`"
":human_name" = "lof detector"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nLOFDetector(k = 5,\n            metric = Euclidean(),\n            algorithm = :kdtree,\n            leafsize = 10,\n            reorder = true,\n            parallel = false)\n```\n\nCalculate an anomaly score based on the density of an instance in comparison to its neighbors. This algorithm introduced the notion of local outliers and was developed by Breunig et al., see [1].\n\n## Parameters\n\n```\nk::Integer\n```\n\nNumber of neighbors (must be greater than 0).\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n## Examples\n\n```julia\nusing OutlierDetection: LOFDetector, fit, transform\ndetector = LOFDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)\n```\n\n## References\n\n[1] Breunig, Markus M.; Kriegel, Hans-Peter; Ng, Raymond T.; Sander, Jörg (2000): LOF: Identifying Density-Based Local Outliers.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":package_name" = "OutlierDetectionNeighbors"
":name" = "LOFDetector"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":constructor" = "`nothing`"

[OutlierDetectionNeighbors.KNNDetector]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Integer\", \"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\", \"Symbol\")`"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "OutlierDetectionNeighbors.KNNDetector"
":hyperparameters" = "`(:k, :metric, :algorithm, :static, :leafsize, :reorder, :parallel, :reduction)`"
":is_pure_julia" = "`true`"
":human_name" = "knn detector"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nKNNDetector(k=5,\n            metric=Euclidean,\n            algorithm=:kdtree,\n            leafsize=10,\n            reorder=true,\n            reduction=:maximum)\n```\n\nCalculate the anomaly score of an instance based on the distance to its k-nearest neighbors.\n\n## Parameters\n\n```\nk::Integer\n```\n\nNumber of neighbors (must be greater than 0).\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n```\nreduction::Symbol\n```\n\nOne of `(:maximum, :median, :mean)`. (`reduction=:maximum`) was proposed by [1]. Angiulli et al. [2] proposed sum to reduce the distances, but mean has been implemented for numerical stability.\n\n## Examples\n\n```julia\nusing OutlierDetection: KNNDetector, fit, transform\ndetector = KNNDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)\n```\n\n## References\n\n[1] Ramaswamy, Sridhar; Rastogi, Rajeev; Shim, Kyuseok (2000): Efficient Algorithms for Mining Outliers from Large Data Sets.\n\n[2] Angiulli, Fabrizio; Pizzuti, Clara (2002): Fast Outlier Detection in High Dimensional Spaces.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":package_name" = "OutlierDetectionNeighbors"
":name" = "KNNDetector"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":constructor" = "`nothing`"

[OutlierDetectionNeighbors.COFDetector]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Integer\", \"Distances.Metric\", \"Symbol\", \"Union{Bool, Symbol}\", \"Integer\", \"Bool\", \"Bool\")`"
":package_uuid" = "51249a0a-cb36-4849-8e04-30c7f8d311bb"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Union{Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}}, Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":abstract_type" = "`MLJModelInterface.UnsupervisedDetector`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "OutlierDetectionNeighbors.COFDetector"
":hyperparameters" = "`(:k, :metric, :algorithm, :static, :leafsize, :reorder, :parallel)`"
":is_pure_julia" = "`true`"
":human_name" = "cof detector"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nCOFDetector(k = 5,\n            metric = Euclidean(),\n            algorithm = :kdtree,\n            leafsize = 10,\n            reorder = true,\n            parallel = false)\n```\n\nLocal outlier density based on chaining distance between graphs of neighbors, as described in [1].\n\n## Parameters\n\n```\nk::Integer\n```\n\nNumber of neighbors (must be greater than 0).\n\n```\nmetric::Metric\n```\n\nThis is one of the Metric types defined in the Distances.jl package. It is possible to define your own metrics by creating new types that are subtypes of Metric.\n\n```\nalgorithm::Symbol\n```\n\nOne of `(:kdtree, :balltree)`. In a `kdtree`, points are recursively split into groups using hyper-planes. Therefore a KDTree only works with axis aligned metrics which are: Euclidean, Chebyshev, Minkowski and Cityblock. A *brutetree* linearly searches all points in a brute force fashion and works with any Metric. A *balltree* recursively splits points into groups bounded by hyper-spheres and works with any Metric.\n\n```\nstatic::Union{Bool, Symbol}\n```\n\nOne of `(true, false, :auto)`. Whether the input data for fitting and transform should be statically or dynamically allocated. If `true`, the data is statically allocated. If `false`, the data is dynamically allocated. If `:auto`, the data is dynamically allocated if the product of all dimensions except the last is greater than 100.\n\n```\nleafsize::Int\n```\n\nDetermines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n\n```\nreorder::Bool\n```\n\nWhile building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to true.\n\n```\nparallel::Bool\n```\n\nParallelize `score` and `predict` using all threads available. The number of threads can be set with the `JULIA_NUM_THREADS` environment variable. Note: `fit` is not parallel.\n\n## Examples\n\n```julia\nusing OutlierDetection: COFDetector, fit, transform\ndetector = COFDetector()\nX = rand(10, 100)\nmodel, result = fit(detector, X; verbosity=0)\ntest_scores = transform(detector, model, X)\n```\n\n## References\n\n[1] Tang, Jian; Chen, Zhixiang; Fu, Ada Wai-Chee; Cheung, David Wai-Lok (2002): Enhancing Effectiveness of Outlier Detections for Low Density Patterns.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/OutlierDetectionJL/OutlierDetectionNeighbors.jl"
":package_name" = "OutlierDetectionNeighbors"
":name" = "COFDetector"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":reformat", ":selectrows", ":fit", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`AbstractVector{<:Union{Missing, ScientificTypesBase.OrderedFactor{2}}}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":constructor" = "`nothing`"

[SIRUS.StableRulesClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Random.AbstractRNG\", \"Real\", \"Int64\", \"Int64\", \"Int64\", \"Int64\", \"Int64\", \"Float64\")`"
":package_uuid" = "9113e207-2504-4b06-8eee-d78e288bee65"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "SIRUS.StableForestClassifier"
":hyperparameters" = "`(:rng, :partial_sampling, :n_trees, :max_depth, :q, :min_data_in_leaf, :max_rules, :lambda)`"
":is_pure_julia" = "`true`"
":human_name" = "stable rules classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nStableRulesClassifier\n```\n\nA model type for constructing a stable rules classifier, based on [SIRUS.jl](https://github.com/rikhuijzer/SIRUS.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nStableRulesClassifier = @load StableRulesClassifier pkg=SIRUS\n```\n\nDo `model = StableRulesClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `StableRulesClassifier(rng=...)`.\n\n`StableRulesClassifier` implements the explainable rule-based model based on a random forest.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `rng::AbstractRNG=default_rng()`: Random number generator.   Using a `StableRNG` from `StableRNGs.jl` is advised.\n  * `partial_sampling::Float64=0.7`:   Ratio of samples to use in each subset of the data.   The default should be fine for most cases.\n  * `n_trees::Int=1000`:   The number of trees to use.   It is advisable to use at least thousand trees to for a better rule selection, and   in turn better predictive performance.\n  * `max_depth::Int=2`:   The depth of the tree.   A lower depth decreases model complexity and can therefore improve accuracy when the sample size is small (reduce overfitting).\n  * `q::Int=10`: Number of cutpoints to use per feature.   The default value should be fine for most situations.\n  * `min_data_in_leaf::Int=5`: Minimum number of data points per leaf.\n  * `max_rules::Int=10`:   This is the most important hyperparameter after `lambda`.   The more rules, the more accurate the model should be.   If this is not the case, tune `lambda` first.   However, more rules will also decrease model interpretability.   So, it is important to find a good balance here.   In most cases, 10 to 40 rules should provide reasonable accuracy while remaining interpretable.\n  * `lambda::Float64=1.0`:   The weights of the final rules are determined via a regularized regression over each rule as a binary feature.   This hyperparameter specifies the strength of the ridge (L2) regularizer.   SIRUS is very sensitive to the choice of this hyperparameter.   Ensure that you try the full range from 10^-4 to 10^4 (e.g., 0.001, 0.01, ..., 100).   When trying the range, one good check is to verify that an increase in `max_rules` increases performance.   If this is not the case, then try a different value for `lambda`.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `fitresult`: A `StableRules` object.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return a vector of predictions for each row of `Xnew`.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/rikhuijzer/SIRUS.jl"
":package_name" = "SIRUS"
":name" = "StableRulesClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[SIRUS.StableForestClassifier]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Random.AbstractRNG\", \"Real\", \"Int64\", \"Int64\", \"Int64\", \"Int64\")`"
":package_uuid" = "9113e207-2504-4b06-8eee-d78e288bee65"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}}}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "SIRUS.StableForestClassifier"
":hyperparameters" = "`(:rng, :partial_sampling, :n_trees, :max_depth, :q, :min_data_in_leaf)`"
":is_pure_julia" = "`true`"
":human_name" = "stable forest classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nStableForestClassifier\n```\n\nA model type for constructing a stable forest classifier, based on [SIRUS.jl](https://github.com/rikhuijzer/SIRUS.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nStableForestClassifier = @load StableForestClassifier pkg=SIRUS\n```\n\nDo `model = StableForestClassifier()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `StableForestClassifier(rng=...)`.\n\n`StableForestClassifier` implements the random forest classifier with a stabilized forest structure (Bénard et al., [2021](http://proceedings.mlr.press/v130/benard21a.html)). This stabilization increases stability when extracting rules. The impact on the predictive accuracy compared to standard random forests should be relatively small.\n\n!!! note\n    Just like normal random forests, this model is not easily explainable. If you are interested in an explainable model, use the `StableRulesClassifier` or `StableRulesRegressor`.\n\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `rng::AbstractRNG=default_rng()`: Random number generator.   Using a `StableRNG` from `StableRNGs.jl` is advised.\n  * `partial_sampling::Float64=0.7`:   Ratio of samples to use in each subset of the data.   The default should be fine for most cases.\n  * `n_trees::Int=1000`:   The number of trees to use.   It is advisable to use at least thousand trees to for a better rule selection, and   in turn better predictive performance.\n  * `max_depth::Int=2`:   The depth of the tree.   A lower depth decreases model complexity and can therefore improve accuracy when the sample size is small (reduce overfitting).\n  * `q::Int=10`: Number of cutpoints to use per feature.   The default value should be fine for most situations.\n  * `min_data_in_leaf::Int=5`: Minimum number of data points per leaf.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `fitresult`: A `StableForest` object.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return a vector of predictions for each row of `Xnew`.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/rikhuijzer/SIRUS.jl"
":package_name" = "SIRUS"
":name" = "StableForestClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{<:ScientificTypesBase.Finite}}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[SIRUS.StableRulesRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Random.AbstractRNG\", \"Real\", \"Int64\", \"Int64\", \"Int64\", \"Int64\", \"Int64\", \"Float64\")`"
":package_uuid" = "9113e207-2504-4b06-8eee-d78e288bee65"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "SIRUS.StableForestRegressor"
":hyperparameters" = "`(:rng, :partial_sampling, :n_trees, :max_depth, :q, :min_data_in_leaf, :max_rules, :lambda)`"
":is_pure_julia" = "`true`"
":human_name" = "stable rules regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nStableRulesRegressor\n```\n\nA model type for constructing a stable rules regressor, based on [SIRUS.jl](https://github.com/rikhuijzer/SIRUS.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nStableRulesRegressor = @load StableRulesRegressor pkg=SIRUS\n```\n\nDo `model = StableRulesRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `StableRulesRegressor(rng=...)`.\n\n`StableRulesRegressor` implements the explainable rule-based regression model based on a random forest.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `rng::AbstractRNG=default_rng()`: Random number generator.   Using a `StableRNG` from `StableRNGs.jl` is advised.\n  * `partial_sampling::Float64=0.7`:   Ratio of samples to use in each subset of the data.   The default should be fine for most cases.\n  * `n_trees::Int=1000`:   The number of trees to use.   It is advisable to use at least thousand trees to for a better rule selection, and   in turn better predictive performance.\n  * `max_depth::Int=2`:   The depth of the tree.   A lower depth decreases model complexity and can therefore improve accuracy when the sample size is small (reduce overfitting).\n  * `q::Int=10`: Number of cutpoints to use per feature.   The default value should be fine for most situations.\n  * `min_data_in_leaf::Int=5`: Minimum number of data points per leaf.\n  * `max_rules::Int=10`:   This is the most important hyperparameter after `lambda`.   The more rules, the more accurate the model should be.   If this is not the case, tune `lambda` first.   However, more rules will also decrease model interpretability.   So, it is important to find a good balance here.   In most cases, 10 to 40 rules should provide reasonable accuracy while remaining interpretable.\n  * `lambda::Float64=1.0`:   The weights of the final rules are determined via a regularized regression over each rule as a binary feature.   This hyperparameter specifies the strength of the ridge (L2) regularizer.   SIRUS is very sensitive to the choice of this hyperparameter.   Ensure that you try the full range from 10^-4 to 10^4 (e.g., 0.001, 0.01, ..., 100).   When trying the range, one good check is to verify that an increase in `max_rules` increases performance.   If this is not the case, then try a different value for `lambda`.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `fitresult`: A `StableRules` object.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return a vector of predictions for each row of `Xnew`.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/rikhuijzer/SIRUS.jl"
":package_name" = "SIRUS"
":name" = "StableRulesRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[SIRUS.StableForestRegressor]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Random.AbstractRNG\", \"Real\", \"Int64\", \"Int64\", \"Int64\", \"Int64\")`"
":package_uuid" = "9113e207-2504-4b06-8eee-d78e288bee65"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}}}, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "SIRUS.StableForestRegressor"
":hyperparameters" = "`(:rng, :partial_sampling, :n_trees, :max_depth, :q, :min_data_in_leaf)`"
":is_pure_julia" = "`true`"
":human_name" = "stable forest regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nStableForestRegressor\n```\n\nA model type for constructing a stable forest regressor, based on [SIRUS.jl](https://github.com/rikhuijzer/SIRUS.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nStableForestRegressor = @load StableForestRegressor pkg=SIRUS\n```\n\nDo `model = StableForestRegressor()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `StableForestRegressor(rng=...)`.\n\n`StableForestRegressor` implements the random forest regressor with a stabilized forest structure (Bénard et al., [2021](http://proceedings.mlr.press/v130/benard21a.html)).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have one of the following element scitypes: `Continuous`, `Count`, or `<:OrderedFactor`; check column scitypes with `schema(X)`\n  * `y`: the target, which can be any `AbstractVector` whose element scitype is `<:OrderedFactor` or `<:Multiclass`; check the scitype with `scitype(y)`\n\nTrain the machine with `fit!(mach, rows=...)`.\n\n# Hyperparameters\n\n  * `rng::AbstractRNG=default_rng()`: Random number generator.   Using a `StableRNG` from `StableRNGs.jl` is advised.\n  * `partial_sampling::Float64=0.7`:   Ratio of samples to use in each subset of the data.   The default should be fine for most cases.\n  * `n_trees::Int=1000`:   The number of trees to use.   It is advisable to use at least thousand trees to for a better rule selection, and   in turn better predictive performance.\n  * `max_depth::Int=2`:   The depth of the tree.   A lower depth decreases model complexity and can therefore improve accuracy when the sample size is small (reduce overfitting).\n  * `q::Int=10`: Number of cutpoints to use per feature.   The default value should be fine for most situations.\n  * `min_data_in_leaf::Int=5`: Minimum number of data points per leaf.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `fitresult`: A `StableForest` object.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return a vector of predictions for each row of `Xnew`.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/rikhuijzer/SIRUS.jl"
":package_name" = "SIRUS"
":name" = "StableForestRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table{<:Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJIteration.IteratedModel]
":constructor" = "`IteratedModel`"
":hyperparameter_types" = "`(\"MLJModelInterface.Probabilistic\", \"Any\", \"Any\", \"Any\", \"Union{Nothing, AbstractVector{<:Real}}\", \"Union{Nothing, Dict{Any, <:Real}}\", \"Any\", \"Bool\", \"Bool\", \"Union{Nothing, Expr, Symbol}\", \"Bool\")`"
":package_uuid" = "614be32b-d00c-4edb-bd02-1eb411ab5e55"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Unknown, ScientificTypesBase.Unknown}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "MLJIteration.IteratedModel"
":hyperparameters" = "`(:model, :controls, :resampling, :measure, :weights, :class_weights, :operation, :retrain, :check_measure, :iteration_parameter, :cache)`"
":is_pure_julia" = "`false`"
":human_name" = "probabilistic iterated model"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nIteratedModel(model;\n    controls=MLJIteration.DEFAULT_CONTROLS,\n    resampling=Holdout(),\n    measure=nothing,\n    retrain=false,\n    advanced_options...,\n)\n```\n\nWrap the specified supervised `model` in the specified iteration `controls`. Here `model` should support iteration, which is true if (`iteration_parameter(model)` is different from `nothing`.\n\nAvailable controls: Step(), Info(), Warn(), Error(), Callback(), WithLossDo(), WithTrainingLossesDo(), WithNumberDo(), Data(), Disjunction(), GL(), InvalidValue(), Never(), NotANumber(), NumberLimit(), NumberSinceBest(), PQ(), Patience(), Threshold(), TimeLimit(), Warmup(), WithIterationsDo(), WithEvaluationDo(), WithFittedParamsDo(), WithReportDo(), WithMachineDo(), WithModelDo(), CycleLearningRate() and Save().\n\n!!! important\n    To make out-of-sample losses available to the controls, the wrapped `model` is only trained on part of the data, as iteration proceeds. The user may want to force retraining on all data after controlled iteration has finished by specifying `retrain=true`. See also \"Training\", and the `retrain` option, under \"Extended help\" below.\n\n\n# Extended help\n\n# Options\n\n  * `controls=Any[IterationControl.Step(1), EarlyStopping.Patience(5), EarlyStopping.GL(2.0), EarlyStopping.TimeLimit(Dates.Millisecond(108000)), EarlyStopping.InvalidValue()]`: Controls are summarized at [https://JuliaAI.github.io/MLJ.jl/dev/getting_started/](https://JuliaAI.github.io/MLJ.jl/dev/controlling_iterative_models/) but query individual doc-strings for details and advanced options. For creating your own controls, refer to the documentation just cited.\n  * `resampling=Holdout(fraction_train=0.7)`: The default resampling holds back 30% of data for computing an out-of-sample estimate of performance (the \"loss\") for loss-based controls such as `WithLossDo`. Specify `resampling=nothing` if all data is to be used for controlled iteration, with each out-of-sample loss replaced by the most recent training loss, assuming this is made available by the model (`supports_training_losses(model) == true`). If the model does not report a training loss, you can use `resampling=InSample()` instead. Otherwise, `resampling` must have type `Holdout` or be a vector with one element of the form `(train_indices, test_indices)`.\n  * `measure=nothing`: StatisticalMeasures.jl compatible measure for estimating model performance (the \"loss\", but the orientation is immaterial - i.e., this could be a score). Inferred by default. Ignored if `resampling=nothing`.\n  * `retrain=false`: If `retrain=true` or `resampling=nothing`, `iterated_model` behaves exactly like the original `model` but with the iteration parameter automatically selected (\"learned\"). That is, the model is retrained on *all* available data, using the same number of iterations, once controlled iteration has stopped. This is typically desired if wrapping the iterated model further, or when inserting in a pipeline or other composite model. If `retrain=false` (default) and `resampling isa Holdout`, then `iterated_model` behaves like the original model trained on a subset of the provided data.\n  * `weights=nothing`: per-observation weights to be passed to `measure` where supported; if unspecified, these are understood to be uniform.\n  * `class_weights=nothing`: class-weights to be passed to `measure` where supported; if unspecified, these are understood to be uniform.\n  * `operation=nothing`: Operation, such as `predict` or `predict_mode`, for computing target values, or proxy target values, for consumption by `measure`; automatically inferred by default.\n  * `check_measure=true`: Specify `false` to override checks on `measure` for compatibility with the training data.\n  * `iteration_parameter=nothing`: A symbol, such as `:epochs`, naming the iteration parameter of `model`; inferred by default. Note that the actual value of the iteration parameter in the supplied `model` is ignored; only the value of an internal clone is mutated during training the wrapped model.\n  * `cache=true`: Whether or not model-specific representations of data are cached in between iteration parameter increments; specify `cache=false` to prioritize memory over speed.\n\n# Training\n\nTraining an instance `iterated_model` of `IteratedModel` on some `data` (by binding to a machine and calling `fit!`, for example) performs the following actions:\n\n  * Assuming `resampling !== nothing`, the `data` is split into *train* and *test* sets, according to the specified `resampling` strategy.\n  * A clone of the wrapped model, `model` is bound to the train data in an internal machine, `train_mach`. If `resampling === nothing`, all data is used instead. This machine is the object to which controls are applied. For example, `Callback(fitted_params |> print)` will print the value of `fitted_params(train_mach)`.\n  * The iteration parameter of the clone is set to `0`.\n  * The specified `controls` are repeatedly applied to `train_mach` in sequence, until one of the controls triggers a stop. Loss-based controls (eg, `Patience()`, `GL()`, `Threshold(0.001)`) use an out-of-sample loss, obtained by applying `measure` to predictions and the test target values. (Specifically, these predictions are those returned by `operation(train_mach)`.)  If `resampling === nothing` then the most recent training loss is used instead. Some controls require *both* out-of-sample and training losses (eg, `PQ()`).\n  * Once a stop has been triggered, a clone of `model` is bound to all `data` in a machine called `mach_production` below, unless `retrain == false` (true by default) or `resampling === nothing`, in which case `mach_production` coincides with `train_mach`.\n\n# Prediction\n\nCalling `predict(mach, Xnew)` in the example above returns `predict(mach_production, Xnew)`. Similar similar statements hold for `predict_mean`, `predict_mode`, `predict_median`.\n\n# Controls that mutate parameters\n\nA control is permitted to mutate the fields (hyper-parameters) of `train_mach.model` (the clone of `model`). For example, to mutate a learning rate one might use the control\n\n```\nCallback(mach -> mach.model.eta = 1.05*mach.model.eta)\n```\n\nHowever, unless `model` supports warm restarts with respect to changes in that parameter, this will trigger retraining of `train_mach` from scratch, with a different training outcome, which is not recommended.\n\n# Warm restarts\n\nIn the following example, the second `fit!` call will not restart training of the internal `train_mach`, assuming `model` supports warm restarts:\n\n```julia\niterated_model = IteratedModel(\n    model,\n    controls = [Step(1), NumberLimit(100)],\n)\nmach = machine(iterated_model, X, y)\nfit!(mach) # train for 100 iterations\niterated_model.controls = [Step(1), NumberLimit(50)],\nfit!(mach) # train for an *extra* 50 iterations\n```\n\nMore generally, if `iterated_model` is mutated and `fit!(mach)` is called again, then a warm restart is attempted if the only parameters to change are `model` or `controls` or both.\n\nSpecifically, `train_mach.model` is mutated to match the current value of `iterated_model.model` and the iteration parameter of the latter is updated to the last value used in the preceding `fit!(mach)` call. Then repeated application of the (updated) controls begin anew.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJIteration.jl"
":package_name" = "MLJIteration"
":name" = "IteratedModel"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = []
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`true`"

[MLJTSVDInterface.TSVDTransformer]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Int64\", \"Int64\", \"Union{Int64, Random.AbstractRNG}\")`"
":package_uuid" = "9449cd9e-2762-5aa3-a617-5413e99d722e"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "MLJTSVDInterface.TSVDTransformer"
":hyperparameters" = "`(:nvals, :maxiter, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "truncated SVD transformer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = "Truncated SVD dimensionality reduction"
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaLinearAlgebra/TSVD.jl"
":package_name" = "TSVD"
":name" = "TSVDTransformer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`Union{ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":is_wrapper" = "`false`"

[PartitionedLS.PartLS]
":constructor" = "`nothing`"
":hyperparameter_types" = "`(\"Union{Type{PartitionedLS.Alt}, Type{PartitionedLS.BnB}, Type{PartitionedLS.Opt}}\", \"Matrix{Int64}\", \"AbstractFloat\", \"AbstractFloat\", \"Int64\", \"Union{Nothing, Int64, Random.AbstractRNG}\")`"
":package_uuid" = "19f41c5e-8610-11e9-2f2a-0d67e7c5027f"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "PartitionedLS.PartLS"
":hyperparameters" = "`(:Optimizer, :P, :η, :ϵ, :T, :rng)`"
":is_pure_julia" = "`true`"
":human_name" = "part ls"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nPartLS\n```\n\nA model type for fitting a partitioned least squares model to data. Both an MLJ and native interface are provided.\n\n# MLJ Interface\n\nFrom MLJ, the type can be imported using\n\n```\nPartLS = @load PartLS pkg=PartitionedLS\n```\n\nConstruct an instance with default hyper-parameters using the syntax `model = PartLS()`. Provide keyword arguments to override hyper-parameter defaults, as in `model = PartLS(P=...)`.\n\n## Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nwhere\n\n  * `X`: any matrix or table with `Continuous` element scitype.       Check column scitypes of a table `X` with `schema(X)`.\n  * `y`: any vector with `Continuous` element scitype. Check scitype with `scitype(y)`.\n\nTrain the machine using `fit!(mach)`.\n\n## Hyper-parameters\n\n  * `Optimizer`: the optimization algorithm to use. It can be `Opt`, `Alt` or `BnB` (names exported by `PartitionedLS.jl`).\n  * `P`: the partition matrix. It is a binary matrix where each row corresponds to a partition and each column corresponds to a feature. The element `P_{k, i} = 1` if feature `i` belongs to partition `k`.\n  * `η`: the regularization parameter. It controls the strength of the regularization.\n  * `ϵ`: the tolerance parameter. It is used to determine when the Alt optimization algorithm has converged. Only used by the `Alt` algorithm.\n  * `T`: the maximum number of iterations. It is used to determine when to stop the Alt optimization algorithm has converged. Only used by the `Alt` algorithm.\n  * `rng`: the random number generator to use.\n\n      * If `nothing`, the global random number generator `rand` is used.\n      * If an integer, the global number generator `rand` is used after seeding it with the given integer.\n      * If an object of type `AbstractRNG`, the given random number generator is used.\n\n## Operations\n\n  * `predict(mach, Xnew)`: return the predictions of the model on new data `Xnew`\n\n## Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `α`: the values of the α variables. For each partition `k`, it holds the values of the α variables are such that $\\sum_{i \\in P_k} \\alpha_{k} = 1$.\n  * `β`: the values of the β variables. For each partition `k`, `β_k` is the coefficient that multiplies the features in the k-th partition.\n  * `t`: the intercept term of the model.\n  * `P`: the partition matrix. It is a binary matrix where each row corresponds to a partition and each column corresponds to a feature. The element `P_{k, i} = 1` if feature `i` belongs to partition `k`.\n\n## Examples\n\n```julia\nPartLS = @load PartLS pkg=PartitionedLS\n\nX = [[1. 2. 3.];\n     [3. 3. 4.];\n     [8. 1. 3.];\n     [5. 3. 1.]]\n\ny = [1.;\n     1.;\n     2.;\n     3.]\n\nP = [[1 0];\n     [1 0];\n     [0 1]]\n\n\nmodel = PartLS(P=P)\nmach = machine(model, X, y) |> fit!\n\n# predictions on the training set:\npredict(mach, X)\n\n```\n\n# Native Interface\n\n```\nusing PartitionedLS\n\nX = [[1. 2. 3.];\n     [3. 3. 4.];\n     [8. 1. 3.];\n     [5. 3. 1.]]\n\ny = [1.;\n     1.;\n     2.;\n     3.]\n\nP = [[1 0];\n     [1 0];\n     [0 1]]\n\n\n# fit using the optimal algorithm\nresult = fit(Opt, X, y, P, η = 0.0)\ny_hat = predict(result.model, X)\n```\n\nFor other `fit` keyword options, refer to the \"Hyper-parameters\" section for the MLJ interface.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/ml-unito/PartitionedLS.jl.git"
":package_name" = "PartitionedLS"
":name" = "PartLS"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}, AbstractMatrix{ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`false`"

[MLJBase.Pipeline]
":constructor" = "`Pipeline`"
":hyperparameter_types" = "`(\"NamedTuple\", \"Bool\")`"
":package_uuid" = "unknown"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":reporting_operations" = "`(:predict, :predict_mean, :predict_mode, :predict_median, :predict_joint, :transform, :inverse_transform)`"
":fit_data_scitype" = "`Tuple{}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Static`"
":package_license" = "unknown"
":prediction_type" = ":unknown"
":load_path" = "MLJBase.Pipeline"
":hyperparameters" = "`(:named_components, :cache)`"
":is_pure_julia" = "`false`"
":human_name" = "static pipeline"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nPipeline(component1, component2, ... , componentk; options...)\nPipeline(name1=component1, name2=component2, ..., namek=componentk; options...)\ncomponent1 |> component2 |> ... |> componentk\n```\n\nCreate an instance of a composite model type which sequentially composes the specified components in order. This means `component1` receives inputs, whose output is passed to `component2`, and so forth. A \"component\" is either a `Model` instance, a model type (converted immediately to its default instance) or any callable object. Here the \"output\" of a model is what `predict` returns if it is `Supervised`, or what `transform` returns if it is `Unsupervised`.\n\nNames for the component fields are automatically generated unless explicitly specified, as in\n\n```julia\nPipeline(encoder=ContinuousEncoder(drop_last=false),\n         stand=Standardizer())\n```\n\nThe `Pipeline` constructor accepts keyword `options` discussed further below.\n\nOrdinary functions (and other callables) may be inserted in the pipeline as shown in the following example:\n\n```\nPipeline(X->coerce(X, :age=>Continuous), OneHotEncoder, ConstantClassifier)\n```\n\n### Syntactic sugar\n\nThe `|>` operator is overloaded to construct pipelines out of models, callables, and existing pipelines:\n\n```julia\nLinearRegressor = @load LinearRegressor pkg=MLJLinearModels add=true\nPCA = @load PCA pkg=MultivariateStats add=true\n\npipe1 = MLJBase.table |> ContinuousEncoder |> Standardizer\npipe2 = PCA |> LinearRegressor\npipe1 |> pipe2\n```\n\nAt most one of the components may be a supervised model, but this model can appear in any position. A pipeline with a `Supervised` component is itself `Supervised` and implements the `predict` operation.  It is otherwise `Unsupervised` (possibly `Static`) and implements `transform`.\n\n### Special operations\n\nIf all the `components` are invertible unsupervised models (ie, implement `inverse_transform`) then `inverse_transform` is implemented for the pipeline. If there are no supervised models, then `predict` is nevertheless implemented, assuming the last component is a model that implements it (some clustering models). Similarly, calling `transform` on a supervised pipeline calls `transform` on the supervised component.\n\n### Transformers that need a target in training\n\nSome transformers that have type `Unsupervised` (so that the output of `transform` is propagated in pipelines) may require a target variable for training. An example are so-called target encoders (which transform categorical input features, based on some target observations). Provided they appear before any `Supervised` component in the pipelines, such models are supported. Of course a target must be provided whenever training such a pipeline, whether or not it contains a `Supervised` component.\n\n### Optional key-word arguments\n\n  * `prediction_type`  - prediction type of the pipeline; possible values: `:deterministic`, `:probabilistic`, `:interval` (default=`:deterministic` if not inferable)\n  * `operation` - operation applied to the supervised component model, when present; possible values: `predict`, `predict_mean`, `predict_median`, `predict_mode` (default=`predict`)\n  * `cache` - whether the internal machines created for component models should cache model-specific representations of data (see [`machine`](@ref)) (default=`true`)\n\n!!! warning\n    Set `cache=false` to guarantee data anonymization.\n\n\nTo build more complicated non-branching pipelines, refer to the MLJ manual sections on composing models.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "unknown"
":package_name" = "MLJBase"
":name" = "Pipeline"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = []
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`true`"

[MLJBase.Resampler]
":constructor" = "`MLJBase.Resampler`"
":hyperparameter_types" = "`(\"Any\", \"Any\", \"Any\", \"Union{Nothing, AbstractVector{<:Real}}\", \"Union{Nothing, AbstractDict{<:Any, <:Real}}\", \"Any\", \"ComputationalResources.AbstractResource\", \"Bool\", \"Int64\", \"Bool\", \"Bool\", \"Any\", \"Bool\")`"
":package_uuid" = "unknown"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`ScientificTypesBase.Unknown`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`Any`"
":package_license" = "unknown"
":prediction_type" = ":unknown"
":load_path" = "MLJBase.Resampler"
":hyperparameters" = "`(:model, :resampling, :measure, :weights, :class_weights, :operation, :acceleration, :check_measure, :repeats, :cache, :per_observation, :logger, :compact)`"
":is_pure_julia" = "`true`"
":human_name" = "resampler"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nresampler = Resampler(\n    model=ConstantRegressor(),\n    resampling=CV(),\n    measure=nothing,\n    weights=nothing,\n    class_weights=nothing\n    operation=predict,\n    repeats = 1,\n    acceleration=default_resource(),\n    check_measure=true,\n    per_observation=true,\n    logger=default_logger(),\n    compact=false,\n)\n```\n\n*Private method.* Use at own risk.\n\nResampling model wrapper, used internally by the `fit` method of `TunedModel` instances and `IteratedModel` instances. See [`evaluate!`](@ref) for meaning of the options. Not intended for use by general user, who will ordinarily use [`evaluate!`](@ref) directly.\n\nGiven a machine `mach = machine(resampler, args...)` one obtains a performance evaluation of the specified `model`, performed according to the prescribed `resampling` strategy and other parameters, using data `args...`, by calling `fit!(mach)` followed by `evaluate(mach)`.\n\nOn subsequent calls to `fit!(mach)` new train/test pairs of row indices are only regenerated if `resampling`, `repeats` or `cache` fields of `resampler` have changed. The evolution of an RNG field of `resampler` does *not* constitute a change (`==` for `MLJType` objects is not sensitive to such changes; see [`is_same_except`](@ref)).\n\nIf there is single train/test pair, then warm-restart behavior of the wrapped model `resampler.model` will extend to warm-restart behaviour of the wrapper `resampler`, with respect to mutations of the wrapped model.\n\nThe sample `weights` are passed to the specified performance measures that support weights for evaluation. These weights are not to be confused with any weights bound to a `Resampler` instance in a machine, used for training the wrapped `model` when supported.\n\nThe sample `class_weights` are passed to the specified performance measures that support per-class weights for evaluation. These weights are not to be confused with any weights bound to a `Resampler` instance in a machine, used for training the wrapped `model` when supported.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "unknown"
":package_name" = "MLJBase"
":name" = "Resampler"
":target_in_fit" = "`false`"
":supports_class_weights" = "`missing`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":evaluate", ":fit", ":fitted_params", ":update"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`missing`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`true`"

[MLJBase.Stack]
":constructor" = "`MLJBase.Stack`"
":hyperparameter_types" = "`(\"Vector{MLJModelInterface.Supervised}\", \"MLJModelInterface.Probabilistic\", \"Any\", \"Union{Nothing, AbstractVector}\", \"Bool\", \"ComputationalResources.AbstractResource\")`"
":package_uuid" = "a7f614a8-145f-11e9-1d2a-a57a1082229d"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`(:predict, :predict_mean, :predict_mode, :predict_median, :predict_joint, :transform, :inverse_transform)`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Unknown, ScientificTypesBase.Unknown}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "MLJBase.Stack"
":hyperparameters" = "`(:models, :metalearner, :resampling, :measures, :cache, :acceleration)`"
":is_pure_julia" = "`false`"
":human_name" = "probabilistic stack"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nUnion{Types...}\n```\n\nA `Union` type is an abstract type which includes all instances of any of its argument types. This means that `T <: Union{T,S}` and `S <: Union{T,S}`.\n\nLike other abstract types, it cannot be instantiated, even if all of its arguments are non abstract.\n\n# Examples\n\n```jldoctest\njulia> IntOrString = Union{Int,AbstractString}\nUnion{Int64, AbstractString}\n\njulia> 1 isa IntOrString # instance of Int is included in the union\ntrue\n\njulia> \"Hello!\" isa IntOrString # String is also included\ntrue\n\njulia> 1.0 isa IntOrString # Float64 is not included because it is neither Int nor AbstractString\nfalse\n```\n\n# Extended Help\n\nUnlike most other parametric types, unions are covariant in their parameters. For example, `Union{Real, String}` is a subtype of `Union{Number, AbstractString}`.\n\nThe empty union [`Union{}`](@ref) is the bottom type of Julia.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJBase.jl"
":package_name" = "MLJBase"
":name" = "Stack"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = []
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`true`"

[MLJBase.TransformedTargetModel]
":constructor" = "`TransformedTargetModel`"
":hyperparameter_types" = "`(\"MLJModelInterface.Probabilistic\", \"Any\", \"Any\", \"Any\")`"
":package_uuid" = "a7f614a8-145f-11e9-1d2a-a57a1082229d"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`(:predict, :predict_mean, :predict_mode, :predict_median, :predict_joint, :transform, :inverse_transform)`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Unknown, ScientificTypesBase.Unknown}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "MLJBase.TransformedTargetModel"
":hyperparameters" = "`(:model, :transformer, :inverse, :cache)`"
":is_pure_julia" = "`false`"
":human_name" = "transformed target model probabilistic"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nTransformedTargetModel(model; transformer=nothing, inverse=nothing, cache=true)\n```\n\nWrap the supervised or semi-supervised `model` in a transformation of the target variable.\n\nHere `transformer` one of the following:\n\n  * The `Unsupervised` model that is to transform the training target. By default (`inverse=nothing`) the parameters learned by this transformer are also used to inverse-transform the predictions of `model`, which means `transformer` must implement the `inverse_transform` method. If this is not the case, specify `inverse=identity` to suppress inversion.\n  * A callable object for transforming the target, such as `y -> log.(y)`. In this case a callable `inverse`, such as `z -> exp.(z)`, should be specified.\n\nSpecify `cache=false` to prioritize memory over speed, or to guarantee data anonymity.\n\nSpecify `inverse=identity` if `model` is a probabilistic predictor, as inverse-transforming sample spaces is not supported. Alternatively, replace `model` with a deterministic model, such as `Pipeline(model, y -> mode.(y))`.\n\n### Examples\n\nA model that normalizes the target before applying ridge regression, with predictions returned on the original scale:\n\n```julia\n@load RidgeRegressor pkg=MLJLinearModels\nmodel = RidgeRegressor()\ntmodel = TransformedTargetModel(model, transformer=Standardizer())\n```\n\nA model that applies a static `log` transformation to the data, again returning predictions to the original scale:\n\n```julia\ntmodel2 = TransformedTargetModel(model, transformer=y->log.(y), inverse=z->exp.(y))\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJBase.jl"
":package_name" = "MLJBase"
":name" = "TransformedTargetModel"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = []
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`true`"

[MLJTuning.TunedModel]
":constructor" = "`TunedModel`"
":hyperparameter_types" = "`(\"Union{MLJModelInterface.Probabilistic, MLJModelInterface.ProbabilisticSupervisedDetector, MLJModelInterface.ProbabilisticUnsupervisedDetector}\", \"Any\", \"Any\", \"Any\", \"Union{Nothing, AbstractVector{<:Real}}\", \"Union{Nothing, AbstractDict}\", \"Any\", \"Any\", \"Any\", \"Bool\", \"Int64\", \"Union{Nothing, Int64}\", \"ComputationalResources.AbstractResource\", \"ComputationalResources.AbstractResource\", \"Bool\", \"Bool\", \"Bool\", \"Any\")`"
":package_uuid" = "03970b2e-30c4-11ea-3135-d1576263f10f"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Unknown, ScientificTypesBase.Unknown}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "MLJTuning.TunedModel"
":hyperparameters" = "`(:model, :tuning, :resampling, :measure, :weights, :class_weights, :operation, :range, :selection_heuristic, :train_best, :repeats, :n, :acceleration, :acceleration_resampling, :check_measure, :cache, :compact_history, :logger)`"
":is_pure_julia" = "`false`"
":human_name" = "probabilistic tuned model"
":is_supervised" = "`true`"
":iteration_parameter" = ":n"
":docstring" = """```\ntuned_model = TunedModel(; model=<model to be mutated>,\n                         tuning=RandomSearch(),\n                         resampling=Holdout(),\n                         range=nothing,\n                         measure=nothing,\n                         n=default_n(tuning, range),\n                         operation=nothing,\n                         other_options...)\n```\n\nConstruct a model wrapper for hyper-parameter optimization of a supervised learner, specifying the `tuning` strategy and `model` whose hyper-parameters are to be mutated.\n\n```\ntuned_model = TunedModel(; models=<models to be compared>,\n                         resampling=Holdout(),\n                         measure=nothing,\n                         n=length(models),\n                         operation=nothing,\n                         other_options...)\n```\n\nConstruct a wrapper for multiple `models`, for selection of an optimal one (equivalent to specifying `tuning=Explicit()` and `range=models` above). Elements of the iterator `models` need not have a common type, but they must all be `Deterministic` or all be `Probabilistic` *and this is not checked* but inferred from the first element generated.\n\nSee below for a complete list of options.\n\n### Training\n\nCalling `fit!(mach)` on a machine `mach=machine(tuned_model, X, y)` or `mach=machine(tuned_model, X, y, w)` will:\n\n  * Instigate a search, over clones of `model`, with the hyperparameter mutations specified by `range`, for a model optimizing the specified `measure`, using performance evaluations carried out using the specified `tuning` strategy and `resampling` strategy. In the case `models` is explictly listed, the search is instead over the models generated by the iterator `models`.\n  * Fit an internal machine, based on the optimal model `fitted_params(mach).best_model`, wrapping the optimal `model` object in *all* the provided data `X`, `y`(, `w`). Calling `predict(mach, Xnew)` then returns predictions on `Xnew` of this internal machine. The final train can be supressed by setting `train_best=false`.\n\n### Search space\n\nThe `range` objects supported depend on the `tuning` strategy specified. Query the `strategy` docstring for details. To optimize over an explicit list `v` of models of the same type, use `strategy=Explicit()` and specify `model=v[1]` and `range=v`.\n\nThe number of models searched is specified by `n`. If unspecified, then `MLJTuning.default_n(tuning, range)` is used. When `n` is increased and `fit!(mach)` called again, the old search history is re-instated and the search continues where it left off.\n\n### Measures (metrics)\n\nIf more than one `measure` is specified, then only the first is optimized (unless `strategy` is multi-objective) but the performance against every measure specified will be computed and reported in `report(mach).best_performance` and other relevant attributes of the generated report. Options exist to pass per-observation weights or class weights to measures; see below.\n\n*Important.* If a custom measure, `my_measure` is used, and the measure is a score, rather than a loss, be sure to check that `MLJ.orientation(my_measure) == :score` to ensure maximization of the measure, rather than minimization. Override an incorrect value with `MLJ.orientation(::typeof(my_measure)) = :score`.\n\n### Accessing the fitted parameters and other training (tuning) outcomes\n\nA Plots.jl plot of performance estimates is returned by `plot(mach)` or `heatmap(mach)`.\n\nOnce a tuning machine `mach` has bee trained as above, then `fitted_params(mach)` has these keys/values:\n\n|                  key |                                   value |\n| --------------------:| ---------------------------------------:|\n|         `best_model` |                  optimal model instance |\n| `best_fitted_params` | learned parameters of the optimal model |\n\nThe named tuple `report(mach)` includes these keys/values:\n\n|                  key |                                                              value |\n| --------------------:| ------------------------------------------------------------------:|\n|         `best_model` |                                             optimal model instance |\n| `best_history_entry` | corresponding entry in the history, including performance estimate |\n|        `best_report` |          report generated by fitting the optimal model to all data |\n|            `history` |                tuning strategy-specific history of all evaluations |\n\nplus other key/value pairs specific to the `tuning` strategy.\n\nEach element of `history` is a property-accessible object with these properties:\n\n|           key |                                                             value |\n| -------------:| -----------------------------------------------------------------:|\n|     `measure` |                                      vector of measures (metrics) |\n| `measurement` |                           vector of measurements, one per measure |\n|    `per_fold` |           vector of vectors of unaggregated per-fold measurements |\n|  `evaluation` | full `PerformanceEvaluation`/`CompactPerformaceEvaluation` object |\n\n### Complete list of key-word options\n\n  * `model`: `Supervised` model prototype that is cloned and mutated to generate models for evaluation\n  * `models`: Alternatively, an iterator of MLJ models to be explicitly evaluated. These may have varying types.\n  * `tuning=RandomSearch()`: tuning strategy to be applied (eg, `Grid()`). See the [Tuning Models](https://alan-turing-institute.github.io/MLJ.jl/dev/tuning_models/#Tuning-Models) section of the MLJ manual for a complete list of options.\n  * `resampling=Holdout()`: resampling strategy (eg, `Holdout()`, `CV()`), `StratifiedCV()`) to be applied in performance evaluations\n  * `measure`: measure or measures to be applied in performance evaluations; only the first used in optimization (unless the strategy is multi-objective) but all reported to the history\n  * `weights`: per-observation weights to be passed the measure(s) in performance evaluations, where supported. Check support with `supports_weights(measure)`.\n  * `class_weights`: class weights to be passed the measure(s) in performance evaluations, where supported. Check support with `supports_class_weights(measure)`.\n  * `repeats=1`: for generating train/test sets multiple times in resampling (\"Monte Carlo\" resampling); see [`evaluate!`](@ref) for details\n  * `operation`/`operations` - One of `predict`, `predict_mean`, `predict_mode`, `predict_median`, or `predict_joint`, or a vector of these of the same length as `measure`/`measures`. Automatically inferred if left unspecified.\n  * `range`: range object; tuning strategy documentation describes supported types\n  * `selection_heuristic`: the rule determining how the best model is decided. According to the default heuristic, `NaiveSelection()`, `measure` (or the first element of `measure`) is evaluated for each resample and these per-fold measurements are aggregrated. The model with the lowest (resp. highest) aggregate is chosen if the measure is a `:loss` (resp. a `:score`).\n  * `n`: number of iterations (ie, models to be evaluated); set by tuning strategy if left unspecified\n  * `train_best=true`: whether to train the optimal model\n  * `acceleration=default_resource()`: mode of parallelization for tuning strategies that support this\n  * `acceleration_resampling=CPU1()`: mode of parallelization for resampling\n  * `check_measure=true`: whether to check `measure` is compatible with the specified `model` and `operation`)\n  * `cache=true`: whether to cache model-specific representations of user-suplied data; set to `false` to conserve memory. Speed gains likely limited to the case `resampling isa Holdout`.\n  * `compact_history=true`: whether to write `CompactPerformanceEvaluation`](@ref) or regular [`PerformanceEvaluation`](@ref) objects to the history (accessed via the `:evaluation` key); the compact form excludes some fields to conserve memory.\n  * `logger=default_logger()`: a logger for externally reporting model performance evaluations, such as an `MLJFlow.Logger` instance. On startup, `default_logger()=nothing`; use `default_logger(logger)` to set a global logger.\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/alan-turing-institute/MLJTuning.jl"
":package_name" = "MLJTuning"
":name" = "TunedModel"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = []
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`true`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":is_wrapper" = "`true`"

[MLJModels.ConstantClassifier]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`()`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`()`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Union{Tuple{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Finite}}, Tuple{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Finite}, AbstractVector{<:Union{ScientificTypesBase.Continuous, ScientificTypesBase.Count}}}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "MLJModels.ConstantClassifier"
":hyperparameters" = "`()`"
":is_pure_julia" = "`true`"
":human_name" = "constant classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nConstantClassifier\n```\n\nThis \"dummy\" probabilistic predictor always returns the same distribution, irrespective of the provided input pattern. The distribution `d` returned is the `UnivariateFinite` distribution based on frequency of classes observed in the training target data. So, `pdf(d, level)` is the number of times the training target takes on the value `level`. Use `predict_mode` instead of `predict` to obtain the training target mode instead. For more on the `UnivariateFinite` type, see the CategoricalDistributions.jl package.\n\nAlmost any reasonable model is expected to outperform `ConstantClassifier`, which is used almost exclusively for testing and establishing performance baselines.\n\nIn MLJ (or MLJModels) do `model = ConstantClassifier()` to construct an instance.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`)\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Finite`; check the scitype with `schema(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\nNone.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew` (which for this model are ignored). Predictions are probabilistic.\n  * `predict_mode(mach, Xnew)`: Return the mode of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `target_distribution`: The distribution fit to the supplied target data.\n\n# Examples\n\n```julia\nusing MLJ\n\nclf = ConstantClassifier()\n\nX, y = @load_crabs # a table and a categorical vector\nmach = machine(clf, X, y) |> fit!\n\nfitted_params(mach)\n\nXnew = (;FL = [8.1, 24.8, 7.2],\n        RW = [5.1, 25.7, 6.4],\n        CL = [15.9, 46.7, 14.3],\n        CW = [18.7, 59.7, 12.2],\n        BD = [6.2, 23.6, 8.4],)\n\n# probabilistic predictions:\nyhat = predict(mach, Xnew)\nyhat[1]\n\n# raw probabilities:\npdf.(yhat, \"B\")\n\n# probability matrix:\nL = levels(y)\npdf(yhat, L)\n\n# point predictions:\npredict_mode(mach, Xnew)\n```\n\nSee also [`ConstantRegressor`](@ref)\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "ConstantClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{_s25} where _s25<:ScientificTypesBase.Finite}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`true`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[MLJModels.Standardizer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Union{Function, AbstractVector{Symbol}}\", \"Bool\", \"Bool\", \"Bool\")`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}}`"
":output_scitype" = "`Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "MLJModels.Standardizer"
":hyperparameters" = "`(:features, :ignore, :ordered_factor, :count)`"
":is_pure_julia" = "`true`"
":human_name" = "standardizer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nStandardizer\n```\n\nA model type for constructing a standardizer, based on [MLJModels.jl](https://github.com/JuliaAI/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nStandardizer = @load Standardizer pkg=MLJModels\n```\n\nDo `model = Standardizer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `Standardizer(features=...)`.\n\nUse this model to standardize (whiten) a `Continuous` vector, or relevant columns of a table. The rescalings applied by this transformer to new data are always those learned during the training phase, which are generally different from what would actually standardize the new data.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any Tables.jl compatible table or any abstract vector with `Continuous` element scitype (any abstract float vector). Only features in a table with `Continuous` scitype can be standardized; check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `features`: one of the following, with the behavior indicated below:\n\n      * `[]` (empty, the default): standardize all features (columns) having `Continuous` element scitype\n      * non-empty vector of feature names (symbols): standardize only the `Continuous` features in the vector (if `ignore=false`) or `Continuous` features *not* named in the vector (`ignore=true`).\n      * function or other callable: standardize a feature if the callable returns `true` on its name. For example, `Standardizer(features = name -> name in [:x1, :x3], ignore = true, count=true)` has the same effect as `Standardizer(features = [:x1, :x3], ignore = true, count=true)`, namely to standardize all `Continuous` and `Count` features, with the exception of `:x1` and `:x3`.\n\n    Note this behavior is further modified if the `ordered_factor` or `count` flags are set to `true`; see below\n  * `ignore=false`: whether to ignore or standardize specified `features`, as explained above\n  * `ordered_factor=false`: if `true`, standardize any `OrderedFactor` feature wherever a `Continuous` feature would be standardized, as described above\n  * `count=false`: if `true`, standardize any `Count` feature wherever a `Continuous` feature would be standardized, as described above\n\n# Operations\n\n  * `transform(mach, Xnew)`: return `Xnew` with relevant features standardized according to the rescalings learned during fitting of `mach`.\n  * `inverse_transform(mach, Z)`: apply the inverse transformation to `Z`, so that `inverse_transform(mach, transform(mach, Xnew))` is approximately the same as `Xnew`; unavailable if `ordered_factor` or `count` flags were set to `true`.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features_fit` - the names of features that will be standardized\n  * `means` - the corresponding untransformed mean values\n  * `stds` - the corresponding untransformed standard deviations\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `features_fit`: the names of features that will be standardized\n\n# Examples\n\n```\nusing MLJ\n\nX = (ordinal1 = [1, 2, 3],\n     ordinal2 = coerce([:x, :y, :x], OrderedFactor),\n     ordinal3 = [10.0, 20.0, 30.0],\n     ordinal4 = [-20.0, -30.0, -40.0],\n     nominal = coerce([\"Your father\", \"he\", \"is\"], Multiclass));\n\njulia> schema(X)\n┌──────────┬──────────────────┐\n│ names    │ scitypes         │\n├──────────┼──────────────────┤\n│ ordinal1 │ Count            │\n│ ordinal2 │ OrderedFactor{2} │\n│ ordinal3 │ Continuous       │\n│ ordinal4 │ Continuous       │\n│ nominal  │ Multiclass{3}    │\n└──────────┴──────────────────┘\n\nstand1 = Standardizer();\n\njulia> transform(fit!(machine(stand1, X)), X)\n(ordinal1 = [1, 2, 3],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [-1.0, 0.0, 1.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n\nstand2 = Standardizer(features=[:ordinal3, ], ignore=true, count=true);\n\njulia> transform(fit!(machine(stand2, X)), X)\n(ordinal1 = [-1.0, 0.0, 1.0],\n ordinal2 = CategoricalValue{Symbol,UInt32}[:x, :y, :x],\n ordinal3 = [10.0, 20.0, 30.0],\n ordinal4 = [1.0, 0.0, -1.0],\n nominal = CategoricalValue{String,UInt32}[\"Your father\", \"he\", \"is\"],)\n```\n\nSee also [`OneHotEncoder`](@ref), [`ContinuousEncoder`](@ref).\n"""
":inverse_transform_scitype" = "`Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "Standardizer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":inverse_transform", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":transform_scitype" = "`Union{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Continuous}}`"
":constructor" = "`nothing`"

[MLJModels.DeterministicConstantClassifier]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`()`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`()`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table, AbstractVector{<:ScientificTypesBase.Finite}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "MLJModels.DeterministicConstantClassifier"
":hyperparameters" = "`()`"
":is_pure_julia" = "`true`"
":human_name" = "deterministic constant classifier"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nDeterministicConstantClassifier\n```\n\nA model type for constructing a deterministic constant classifier, based on\n[MLJModels.jl](https://github.com/JuliaAI/MLJModels.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nDeterministicConstantClassifier = @load DeterministicConstantClassifier pkg=MLJModels\n```\n\nDo `model = DeterministicConstantClassifier()` to construct an instance with default hyper-parameters. """
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "DeterministicConstantClassifier"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":target_scitype" = "`AbstractVector{<:ScientificTypesBase.Finite}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[MLJModels.UnivariateTimeTypeToContinuous]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Union{Nothing, Dates.TimeType}\", \"Dates.Period\")`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.ScientificTimeType}}`"
":output_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "MLJModels.UnivariateTimeTypeToContinuous"
":hyperparameters" = "`(:zero_time, :step)`"
":is_pure_julia" = "`true`"
":human_name" = "single variable transformer that creates continuous representations of temporally typed data"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nUnivariateTimeTypeToContinuous\n```\n\nA model type for constructing a single variable transformer that creates continuous representations of temporally typed data, based on [MLJModels.jl](https://github.com/JuliaAI/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nUnivariateTimeTypeToContinuous = @load UnivariateTimeTypeToContinuous pkg=MLJModels\n```\n\nDo `model = UnivariateTimeTypeToContinuous()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `UnivariateTimeTypeToContinuous(zero_time=...)`.\n\nUse this model to convert vectors with a `TimeType` element type to vectors of `Float64` type (`Continuous` element scitype).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, x)\n```\n\nwhere\n\n  * `x`: any abstract vector whose element type is a subtype of `Dates.TimeType`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `zero_time`: the time that is to correspond to 0.0 under transformations, with the type coinciding with the training data element type. If unspecified, the earliest time encountered in training is used.\n  * `step::Period=Hour(24)`: time interval to correspond to one unit under transformation\n\n# Operations\n\n  * `transform(mach, xnew)`: apply the encoding inferred when `mach` was fit\n\n# Fitted parameters\n\n`fitted_params(mach).fitresult` is the tuple `(zero_time, step)` actually used in transformations, which may differ from the user-specified hyper-parameters.\n\n# Example\n\n```\nusing MLJ\nusing Dates\n\nx = [Date(2001, 1, 1) + Day(i) for i in 0:4]\n\nencoder = UnivariateTimeTypeToContinuous(zero_time=Date(2000, 1, 1),\n                                         step=Week(1))\n\nmach = machine(encoder, x)\nfit!(mach)\njulia> transform(mach, x)\n5-element Vector{Float64}:\n 52.285714285714285\n 52.42857142857143\n 52.57142857142857\n 52.714285714285715\n 52.857142\n```\n"""
":inverse_transform_scitype" = "`AbstractVector{<:ScientificTypesBase.ScientificTimeType}`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "UnivariateTimeTypeToContinuous"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":fit", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`AbstractVector{<:ScientificTypesBase.ScientificTimeType}`"
":transform_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":constructor" = "`nothing`"

[MLJModels.OneHotEncoder]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Vector{Symbol}\", \"Bool\", \"Bool\", \"Bool\")`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":output_scitype" = "`ScientificTypesBase.Table`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "MLJModels.OneHotEncoder"
":hyperparameters" = "`(:features, :drop_last, :ordered_factor, :ignore)`"
":is_pure_julia" = "`true`"
":human_name" = "one-hot encoder"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nOneHotEncoder\n```\n\nA model type for constructing a one-hot encoder, based on [MLJModels.jl](https://github.com/JuliaAI/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nOneHotEncoder = @load OneHotEncoder pkg=MLJModels\n```\n\nDo `model = OneHotEncoder()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `OneHotEncoder(features=...)`.\n\nUse this model to one-hot encode the `Multiclass` and `OrderedFactor` features (columns) of some table, leaving other columns unchanged.\n\nNew data to be transformed may lack features present in the fit data, but no *new* features can be present.\n\n**Warning:** This transformer assumes that `levels(col)` for any `Multiclass` or `OrderedFactor` column, `col`, is the same for training data and new data to be transformed.\n\nTo ensure *all* features are transformed into `Continuous` features, or dropped, use [`ContinuousEncoder`](@ref) instead.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype `Multiclass` or `OrderedFactor` can be encoded. Check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `features`: a vector of symbols (column names). If empty (default) then all `Multiclass` and `OrderedFactor` features are encoded. Otherwise, encoding is further restricted to the specified features (`ignore=false`) or the unspecified features (`ignore=true`). This default behavior can be modified by the `ordered_factor` flag.\n  * `ordered_factor=false`: when `true`, `OrderedFactor` features are universally excluded\n  * `drop_last=true`: whether to drop the column corresponding to the final class of encoded features. For example, a three-class feature is spawned into three new features if `drop_last=false`, but just two features otherwise.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `all_features`: names of all features encountered in training\n  * `fitted_levels_given_feature`: dictionary of the levels associated with each feature encoded, keyed on the feature name\n  * `ref_name_pairs_given_feature`: dictionary of pairs `r => ftr` (such as `0x00000001 => :grad__A`) where `r` is a CategoricalArrays.jl reference integer representing a level, and `ftr` the corresponding new feature name; the dictionary is keyed on the names of features that are encoded\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * `features_to_be_encoded`: names of input features to be encoded\n  * `new_features`: names of all output features\n\n# Example\n\n```\nusing MLJ\n\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n└───────────┴──────────────────┘\n\nhot = OneHotEncoder(drop_last=true)\nmach = fit!(machine(hot, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade__A     │ Continuous │\n│ grade__B     │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Count      │\n└──────────────┴────────────┘\n```\n\nSee also [`ContinuousEncoder`](@ref).\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "OneHotEncoder"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":fitted_params", ":transform", ":OneHotEncoder"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":constructor" = "`nothing`"

[MLJModels.ContinuousEncoder]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Bool\", \"Bool\")`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":output_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "MLJModels.ContinuousEncoder"
":hyperparameters" = "`(:drop_last, :one_hot_ordered_factors)`"
":is_pure_julia" = "`true`"
":human_name" = "continuous encoder"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nContinuousEncoder\n```\n\nA model type for constructing a continuous encoder, based on [MLJModels.jl](https://github.com/JuliaAI/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nContinuousEncoder = @load ContinuousEncoder pkg=MLJModels\n```\n\nDo `model = ContinuousEncoder()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ContinuousEncoder(drop_last=...)`.\n\nUse this model to arrange all features (columns) of a table to have `Continuous` element scitype, by applying the following protocol to each feature `ftr`:\n\n  * If `ftr` is already `Continuous` retain it.\n  * If `ftr` is `Multiclass`, one-hot encode it.\n  * If `ftr` is `OrderedFactor`, replace it with `coerce(ftr, Continuous)` (vector of floating point integers), unless `ordered_factors=false` is specified, in which case one-hot encode it.\n  * If `ftr` is `Count`, replace it with `coerce(ftr, Continuous)`.\n  * If `ftr` has some other element scitype, or was not observed in fitting the encoder, drop it from the table.\n\n**Warning:** This transformer assumes that `levels(col)` for any `Multiclass` or `OrderedFactor` column, `col`, is the same for training data and new data to be transformed.\n\nTo selectively one-hot-encode categorical features (without dropping columns) use [`OneHotEncoder`](@ref) instead.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any Tables.jl compatible table. Columns can be of mixed type but only those with element scitype `Multiclass` or `OrderedFactor` can be encoded. Check column scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `drop_last=true`: whether to drop the column corresponding to the final class of one-hot encoded features. For example, a three-class feature is spawned into three new features if `drop_last=false`, but two just features otherwise.\n  * `one_hot_ordered_factors=false`: whether to one-hot any feature with `OrderedFactor` element scitype, or to instead coerce it directly to a (single) `Continuous` feature using the order\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features_to_keep`: names of features that will not be dropped from the table\n  * `one_hot_encoder`: the `OneHotEncoder` model instance for handling the one-hot encoding\n  * `one_hot_encoder_fitresult`: the fitted parameters of the `OneHotEncoder` model\n\n# Report\n\n  * `features_to_keep`: names of input features that will not be dropped from the table\n  * `new_features`: names of all output features\n\n# Example\n\n```julia\nX = (name=categorical([\"Danesh\", \"Lee\", \"Mary\", \"John\"]),\n     grade=categorical([\"A\", \"B\", \"A\", \"C\"], ordered=true),\n     height=[1.85, 1.67, 1.5, 1.67],\n     n_devices=[3, 2, 4, 3],\n     comments=[\"the force\", \"be\", \"with you\", \"too\"])\n\njulia> schema(X)\n┌───────────┬──────────────────┐\n│ names     │ scitypes         │\n├───────────┼──────────────────┤\n│ name      │ Multiclass{4}    │\n│ grade     │ OrderedFactor{3} │\n│ height    │ Continuous       │\n│ n_devices │ Count            │\n│ comments  │ Textual          │\n└───────────┴──────────────────┘\n\nencoder = ContinuousEncoder(drop_last=true)\nmach = fit!(machine(encoder, X))\nW = transform(mach, X)\n\njulia> schema(W)\n┌──────────────┬────────────┐\n│ names        │ scitypes   │\n├──────────────┼────────────┤\n│ name__Danesh │ Continuous │\n│ name__John   │ Continuous │\n│ name__Lee    │ Continuous │\n│ grade        │ Continuous │\n│ height       │ Continuous │\n│ n_devices    │ Continuous │\n└──────────────┴────────────┘\n\njulia> setdiff(schema(X).names, report(mach).features_to_keep) # dropped features\n1-element Vector{Symbol}:\n :comments\n\n```\n\nSee also [`OneHotEncoder`](@ref)\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "ContinuousEncoder"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":fitted_params", ":transform", ":ContinuousEncoder"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table`"
":transform_scitype" = "`ScientificTypesBase.Table{<:AbstractVector{<:ScientificTypesBase.Continuous}}`"
":constructor" = "`nothing`"

[MLJModels.UnivariateBoxCoxTransformer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Bool\")`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "MLJModels.UnivariateBoxCoxTransformer"
":hyperparameters" = "`(:n, :shift)`"
":is_pure_julia" = "`true`"
":human_name" = "single variable Box-Cox transformer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nUnivariateBoxCoxTransformer\n```\n\nA model type for constructing a single variable Box-Cox transformer, based on [MLJModels.jl](https://github.com/JuliaAI/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nUnivariateBoxCoxTransformer = @load UnivariateBoxCoxTransformer pkg=MLJModels\n```\n\nDo `model = UnivariateBoxCoxTransformer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `UnivariateBoxCoxTransformer(n=...)`.\n\nBox-Cox transformations attempt to make data look more normally distributed. This can improve performance and assist in the interpretation of models which suppose that data is generated by a normal distribution.\n\nA Box-Cox transformation (with shift) is of the form\n\n```\nx -> ((x + c)^λ - 1)/λ\n```\n\nfor some constant `c` and real `λ`, unless `λ = 0`, in which case the above is replaced with\n\n```\nx -> log(x + c)\n```\n\nGiven user-specified hyper-parameters `n::Integer` and `shift::Bool`, the present implementation learns the parameters `c` and `λ` from the training data as follows: If `shift=true` and zeros are encountered in the data, then `c` is set to `0.2` times the data mean.  If there are no zeros, then no shift is applied. Finally, `n` different values of `λ` between `-0.4` and `3` are considered, with `λ` fixed to the value maximizing normality of the transformed data.\n\n*Reference:* [Wikipedia entry for power  transform](https://en.wikipedia.org/wiki/Power_transform).\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, x)\n```\n\nwhere\n\n  * `x`: any abstract vector with element scitype `Continuous`; check the scitype with `scitype(x)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `n=171`: number of values of the exponent `λ` to try\n  * `shift=false`: whether to include a preliminary constant translation in transformations, in the presence of zeros\n\n# Operations\n\n  * `transform(mach, xnew)`: apply the Box-Cox transformation learned when fitting `mach`\n  * `inverse_transform(mach, z)`: reconstruct the vector `z` whose transformation learned by `mach` is `z`\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `λ`: the learned Box-Cox exponent\n  * `c`: the learned shift\n\n# Examples\n\n```\nusing MLJ\nusing UnicodePlots\nusing Random\nRandom.seed!(123)\n\ntransf = UnivariateBoxCoxTransformer()\n\nx = randn(1000).^2\n\nmach = machine(transf, x)\nfit!(mach)\n\nz = transform(mach, x)\n\njulia> histogram(x)\n                ┌                                        ┐\n   [ 0.0,  2.0) ┤███████████████████████████████████  848\n   [ 2.0,  4.0) ┤████▌ 109\n   [ 4.0,  6.0) ┤█▍ 33\n   [ 6.0,  8.0) ┤▍ 7\n   [ 8.0, 10.0) ┤▏ 2\n   [10.0, 12.0) ┤  0\n   [12.0, 14.0) ┤▏ 1\n                └                                        ┘\n                                 Frequency\n\njulia> histogram(z)\n                ┌                                        ┐\n   [-5.0, -4.0) ┤█▎ 8\n   [-4.0, -3.0) ┤████████▊ 64\n   [-3.0, -2.0) ┤█████████████████████▊ 159\n   [-2.0, -1.0) ┤█████████████████████████████▊ 216\n   [-1.0,  0.0) ┤███████████████████████████████████  254\n   [ 0.0,  1.0) ┤█████████████████████████▊ 188\n   [ 1.0,  2.0) ┤████████████▍ 90\n   [ 2.0,  3.0) ┤██▊ 20\n   [ 3.0,  4.0) ┤▎ 1\n                └                                        ┘\n                                 Frequency\n\n```\n"""
":inverse_transform_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "UnivariateBoxCoxTransformer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":fitted_params", ":inverse_transform", ":transform", ":UnivariateBoxCoxTransformer"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":transform_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":constructor" = "`nothing`"

[MLJModels.InteractionTransformer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\", \"Union{Nothing, Vector{Symbol}}\")`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`(nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{}`"
":output_scitype" = "`ScientificTypesBase.Table`"
":abstract_type" = "`MLJModelInterface.Static`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "MLJModels.InteractionTransformer"
":hyperparameters" = "`(:order, :features)`"
":is_pure_julia" = "`true`"
":human_name" = "interaction transformer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nInteractionTransformer\n```\n\nA model type for constructing a interaction transformer, based on [MLJModels.jl](https://github.com/JuliaAI/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nInteractionTransformer = @load InteractionTransformer pkg=MLJModels\n```\n\nDo `model = InteractionTransformer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `InteractionTransformer(order=...)`.\n\nGenerates all polynomial interaction terms up to the given order for the subset of chosen columns.  Any column that contains elements with scitype `<:Infinite` is a valid basis to generate interactions.  If `features` is not specified, all such columns with scitype `<:Infinite` in the table are used as a basis.\n\nIn MLJ or MLJBase, you can transform features `X` with the single call\n\n```\ntransform(machine(model), X)\n```\n\nSee also the example below.\n\n# Hyper-parameters\n\n  * `order`: Maximum order of interactions to be generated.\n  * `features`: Restricts interations generation to those columns\n\n# Operations\n\n  * `transform(machine(model), X)`: Generates polynomial interaction terms out of table `X` using the hyper-parameters specified in `model`.\n\n# Example\n\n```\nusing MLJ\n\nX = (\n    A = [1, 2, 3],\n    B = [4, 5, 6],\n    C = [7, 8, 9],\n    D = [\"x₁\", \"x₂\", \"x₃\"]\n)\nit = InteractionTransformer(order=3)\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],\n A_C = [7, 16, 27],\n B_C = [28, 40, 54],\n A_B_C = [28, 80, 162],)\n\nit = InteractionTransformer(order=2, features=[:A, :B])\nmach = machine(it)\n\njulia> transform(mach, X)\n(A = [1, 2, 3],\n B = [4, 5, 6],\n C = [7, 8, 9],\n D = [\"x₁\", \"x₂\", \"x₃\"],\n A_B = [4, 10, 18],)\n\n```\n"""
":inverse_transform_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "InteractionTransformer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":clean!", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":constructor" = "`nothing`"

[MLJModels.ConstantRegressor]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Type{D} where D<:Distributions.Sampleable\",)`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`(nothing,)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Probabilistic`"
":package_license" = "MIT"
":prediction_type" = ":probabilistic"
":load_path" = "MLJModels.ConstantRegressor"
":hyperparameters" = "`(:distribution_type,)`"
":is_pure_julia" = "`true`"
":human_name" = "constant regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nConstantRegressor\n```\n\nThis \"dummy\" probabilistic predictor always returns the same distribution, irrespective of the provided input pattern. The distribution returned is the one of the type specified that best fits the training target data. Use `predict_mean` or `predict_median` to predict the mean or median values instead. If not specified, a normal distribution is fit.\n\nAlmost any reasonable model is expected to outperform `ConstantRegressor` which is used almost exclusively for testing and establishing performance baselines.\n\nIn MLJ (or MLJModels) do `model = ConstantRegressor()` or `model = ConstantRegressor(distribution=...)` to construct a model instance.\n\n# Training data\n\nIn MLJ (or MLJBase) bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`)\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Continuous`; check the scitype with `schema(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `distribution_type=Distributions.Normal`: The distribution to be fit to the target data. Must be a subtype of `Distributions.ContinuousUnivariateDistribution`.\n\n# Operations\n\n  * `predict(mach, Xnew)`: Return predictions of the target given features `Xnew` (which for this model are ignored). Predictions are probabilistic.\n  * `predict_mean(mach, Xnew)`: Return instead the means of the probabilistic predictions returned above.\n  * `predict_median(mach, Xnew)`: Return instead the medians of the probabilistic predictions returned above.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `target_distribution`: The distribution fit to the supplied target data.\n\n# Examples\n\n```julia\nusing MLJ\n\nX, y = make_regression(10, 2) # synthetic data: a table and vector\nregressor = ConstantRegressor()\nmach = machine(regressor, X, y) |> fit!\n\nfitted_params(mach)\n\nXnew, _ = make_regression(3, 2)\npredict(mach, Xnew)\npredict_mean(mach, Xnew)\n\n```\n\nSee also [`ConstantClassifier`](@ref)\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "ConstantRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fitted_params", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Density{ScientificTypesBase.Continuous}}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[MLJModels.UnivariateDiscretizer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Int64\",)`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`(nothing,)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.Continuous}}`"
":output_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "MLJModels.UnivariateDiscretizer"
":hyperparameters" = "`(:n_classes,)`"
":is_pure_julia" = "`true`"
":human_name" = "single variable discretizer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nUnivariateDiscretizer\n```\n\nA model type for constructing a single variable discretizer, based on [MLJModels.jl](https://github.com/JuliaAI/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nUnivariateDiscretizer = @load UnivariateDiscretizer pkg=MLJModels\n```\n\nDo `model = UnivariateDiscretizer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `UnivariateDiscretizer(n_classes=...)`.\n\nDiscretization converts a `Continuous` vector into an `OrderedFactor` vector. In particular, the output is a `CategoricalVector` (whose reference type is optimized).\n\nThe transformation is chosen so that the vector on which the transformer is fit has, in transformed form, an approximately uniform distribution of values. Specifically, if `n_classes` is the level of discretization, then `2*n_classes - 1` ordered quantiles are computed, the odd quantiles being used for transforming (discretization) and the even quantiles for inverse transforming.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, x)\n```\n\nwhere\n\n  * `x`: any abstract vector with `Continuous` element scitype; check scitype with `scitype(x)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `n_classes`: number of discrete classes in the output\n\n# Operations\n\n  * `transform(mach, xnew)`: discretize `xnew` according to the discretization learned when fitting `mach`\n  * `inverse_transform(mach, z)`: attempt to reconstruct from `z` a vector that transforms to give `z`\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach).fitesult` include:\n\n  * `odd_quantiles`: quantiles used for transforming (length is `n_classes - 1`)\n  * `even_quantiles`: quantiles used for inverse transforming (length is `n_classes`)\n\n# Example\n\n```\nusing MLJ\nusing Random\nRandom.seed!(123)\n\ndiscretizer = UnivariateDiscretizer(n_classes=100)\nmach = machine(discretizer, randn(1000))\nfit!(mach)\n\njulia> x = rand(5)\n5-element Vector{Float64}:\n 0.8585244609846809\n 0.37541692370451396\n 0.6767070590395461\n 0.9208844241267105\n 0.7064611415680901\n\njulia> z = transform(mach, x)\n5-element CategoricalArrays.CategoricalArray{UInt8,1,UInt8}:\n 0x52\n 0x42\n 0x4d\n 0x54\n 0x4e\n\nx_approx = inverse_transform(mach, z)\njulia> x - x_approx\n5-element Vector{Float64}:\n 0.008224506144777322\n 0.012731354778359405\n 0.0056265330571125816\n 0.005738175684445124\n 0.006835652575801987\n```\n"""
":inverse_transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "UnivariateDiscretizer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":fitted_params", ":inverse_transform", ":transform", ":UnivariateDiscretizer"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`AbstractVector{<:ScientificTypesBase.Continuous}`"
":transform_scitype" = "`AbstractVector{<:ScientificTypesBase.OrderedFactor}`"
":constructor" = "`nothing`"

[MLJModels.BinaryThresholdPredictor]
":is_wrapper" = "`true`"
":hyperparameter_types" = "`(\"MLJModelInterface.Probabilistic\", \"Float64\")`"
":package_uuid" = ""
":hyperparameter_ranges" = "`(nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Unknown, ScientificTypesBase.Unknown}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "unknown"
":prediction_type" = ":deterministic"
":load_path" = "MLJModels.BinaryThresholdPredictor"
":hyperparameters" = "`(:model, :threshold)`"
":is_pure_julia" = "`false`"
":human_name" = "binary threshold predictor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nBinaryThresholdPredictor(model; threshold=0.5)\n```\n\nWrap the `Probabilistic` model, `model`, assumed to support binary classification, as a `Deterministic` model, by applying the specified `threshold` to the positive class probability. In addition to conventional supervised classifiers, it can also be applied to outlier detection models that predict normalized scores - in the form of appropriate `UnivariateFinite` distributions - that is, models that subtype `AbstractProbabilisticUnsupervisedDetector` or `AbstractProbabilisticSupervisedDetector`.\n\nBy convention the positive class is the second class returned by `levels(y)`, where `y` is the target.\n\nIf `threshold=0.5` then calling `predict` on the wrapped model is equivalent to calling `predict_mode` on the atomic model.\n\n# Example\n\nBelow is an application to the well-known Pima Indian diabetes dataset, including optimization of the `threshold` parameter, with a high balanced accuracy the objective. The target class distribution is 500 positives to 268 negatives.\n\nLoading the data:\n\n```julia\nusing MLJ, Random\nrng = Xoshiro(123)\n\ndiabetes = OpenML.load(43582)\noutcome, X = unpack(diabetes, ==(:Outcome), rng=rng);\ny = coerce(Int.(outcome), OrderedFactor);\n```\n\nChoosing a probabilistic classifier:\n\n```julia\nEvoTreesClassifier = @load EvoTreesClassifier\nprob_predictor = EvoTreesClassifier()\n```\n\nWrapping in `TunedModel` to get a deterministic classifier with `threshold` as a new hyperparameter:\n\n```julia\npoint_predictor = BinaryThresholdPredictor(prob_predictor, threshold=0.6)\nXnew, _ = make_moons(3, rng=rng)\nmach = machine(point_predictor, X, y) |> fit!\npredict(mach, X)[1:3] # [0, 0, 0]\n```\n\nEstimating performance:\n\n```julia\nbalanced = BalancedAccuracy(adjusted=true)\ne = evaluate!(mach, resampling=CV(nfolds=6), measures=[balanced, accuracy])\ne.measurement[1] # 0.405 ± 0.089\n```\n\nWrapping in tuning strategy to learn `threshold` that maximizes balanced accuracy:\n\n```julia\nr = range(point_predictor, :threshold, lower=0.1, upper=0.9)\ntuned_point_predictor = TunedModel(\n    point_predictor,\n    tuning=RandomSearch(rng=rng),\n    resampling=CV(nfolds=6),\n    range = r,\n    measure=balanced,\n    n=30,\n)\nmach2 = machine(tuned_point_predictor, X, y) |> fit!\noptimized_point_predictor = report(mach2).best_model\noptimized_point_predictor.threshold # 0.260\npredict(mach2, X)[1:3] # [1, 1, 0]\n```\n\nEstimating the performance of the auto-thresholding model (nested resampling here):\n\n```julia\ne = evaluate!(mach2, resampling=CV(nfolds=6), measure=[balanced, accuracy])\ne.measurement[1] # 0.477 ± 0.110\n```\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "BinaryThresholdPredictor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = []
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`MLJModels.BinaryThresholdPredictor`"

[MLJModels.FillImputer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Vector{Symbol}\", \"Function\", \"Function\", \"Function\")`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":output_scitype" = "`ScientificTypesBase.Table`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "MLJModels.FillImputer"
":hyperparameters" = "`(:features, :continuous_fill, :count_fill, :finite_fill)`"
":is_pure_julia" = "`true`"
":human_name" = "fill imputer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nFillImputer\n```\n\nA model type for constructing a fill imputer, based on [MLJModels.jl](https://github.com/JuliaAI/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nFillImputer = @load FillImputer pkg=MLJModels\n```\n\nDo `model = FillImputer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `FillImputer(features=...)`.\n\nUse this model to impute `missing` values in tabular data. A fixed \"filler\" value is learned from the training data, one for each column of the table.\n\nFor imputing missing values in a vector, use [`UnivariateFillImputer`](@ref) instead.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nwhere\n\n  * `X`: any table of input features (eg, a `DataFrame`) whose columns each have element scitypes `Union{Missing, T}`, where `T` is a subtype of `Continuous`, `Multiclass`, `OrderedFactor` or `Count`. Check scitypes with `schema(X)`.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `features`: a vector of names of features (symbols) for which imputation is to be attempted; default is empty, which is interpreted as \"impute all\".\n  * `continuous_fill`: function or other callable to determine value to be imputed in the case of `Continuous` (abstract float) data; default is to apply `median` after skipping `missing` values\n  * `count_fill`: function or other callable to determine value to be imputed in the case of `Count` (integer) data; default is to apply rounded `median` after skipping `missing` values\n  * `finite_fill`: function or other callable to determine value to be imputed in the case of `Multiclass` or `OrderedFactor` data (categorical vectors); default is to apply `mode` after skipping `missing` values\n\n# Operations\n\n  * `transform(mach, Xnew)`: return `Xnew` with missing values imputed with the fill values learned when fitting `mach`\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `features_seen_in_fit`: the names of features (columns) encountered during training\n  * `univariate_transformer`: the univariate model applied to determine   the fillers (it's fields contain the functions defining the filler computations)\n  * `filler_given_feature`: dictionary of filler values, keyed on feature (column) names\n\n# Examples\n\n```\nusing MLJ\nimputer = FillImputer()\n\nX = (a = [1.0, 2.0, missing, 3.0, missing],\n     b = coerce([\"y\", \"n\", \"y\", missing, \"y\"], Multiclass),\n     c = [1, 1, 2, missing, 3])\n\nschema(X)\njulia> schema(X)\n┌───────┬───────────────────────────────┐\n│ names │ scitypes                      │\n├───────┼───────────────────────────────┤\n│ a     │ Union{Missing, Continuous}    │\n│ b     │ Union{Missing, Multiclass{2}} │\n│ c     │ Union{Missing, Count}         │\n└───────┴───────────────────────────────┘\n\nmach = machine(imputer, X)\nfit!(mach)\n\njulia> fitted_params(mach).filler_given_feature\n(filler = 2.0,)\n\njulia> fitted_params(mach).filler_given_feature\nDict{Symbol, Any} with 3 entries:\n  :a => 2.0\n  :b => \"y\"\n  :c => 2\n\njulia> transform(mach, X)\n(a = [1.0, 2.0, 2.0, 3.0, 2.0],\n b = CategoricalValue{String, UInt32}[\"y\", \"n\", \"y\", \"y\", \"y\"],\n c = [1, 1, 2, 2, 3],)\n```\n\nSee also [`UnivariateFillImputer`](@ref).\n"""
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "FillImputer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":fitted_params", ":transform", ":FillImputer"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":constructor" = "`nothing`"

[MLJModels.DeterministicConstantRegressor]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`()`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`()`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table, AbstractVector{ScientificTypesBase.Continuous}}`"
":output_scitype" = "`ScientificTypesBase.Unknown`"
":abstract_type" = "`MLJModelInterface.Deterministic`"
":package_license" = "MIT"
":prediction_type" = ":deterministic"
":load_path" = "MLJModels.DeterministicConstantRegressor"
":hyperparameters" = "`()`"
":is_pure_julia" = "`true`"
":human_name" = "deterministic constant regressor"
":is_supervised" = "`true`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nDeterministicConstantRegressor\n```\n\nA model type for constructing a deterministic constant regressor, based on\n[MLJModels.jl](https://github.com/JuliaAI/MLJModels.jl), and implementing the MLJ\nmodel interface.\n\nFrom MLJ, the type can be imported using\n```\nDeterministicConstantRegressor = @load DeterministicConstantRegressor pkg=MLJModels\n```\n\nDo `model = DeterministicConstantRegressor()` to construct an instance with default hyper-parameters. """
":inverse_transform_scitype" = "`ScientificTypesBase.Unknown`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "DeterministicConstantRegressor"
":target_in_fit" = "`true`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":predict"]
":deep_properties" = "`()`"
":predict_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":target_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`ScientificTypesBase.Table`"
":transform_scitype" = "`ScientificTypesBase.Unknown`"
":constructor" = "`nothing`"

[MLJModels.UnivariateStandardizer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`()`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`()`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{AbstractVector{<:ScientificTypesBase.Infinite}}`"
":output_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "MLJModels.UnivariateStandardizer"
":hyperparameters" = "`()`"
":is_pure_julia" = "`true`"
":human_name" = "single variable discretizer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nUnivariateStandardizer()\n```\n\nTransformer type for standardizing (whitening) single variable data.\n\nThis model may be deprecated in the future. Consider using [`Standardizer`](@ref), which handles both tabular *and* univariate data.\n"""
":inverse_transform_scitype" = "`AbstractVector{<:ScientificTypesBase.Infinite}`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "UnivariateStandardizer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":fitted_params", ":inverse_transform", ":transform"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`AbstractVector{<:ScientificTypesBase.Infinite}`"
":transform_scitype" = "`AbstractVector{ScientificTypesBase.Continuous}`"
":constructor" = "`nothing`"

[MLJModels.UnivariateFillImputer]
":is_wrapper" = "`false`"
":hyperparameter_types" = "`(\"Function\", \"Function\", \"Function\")`"
":package_uuid" = "d491faf4-2d78-11e9-2867-c94bc002c0b7"
":hyperparameter_ranges" = "`(nothing, nothing, nothing)`"
":reporting_operations" = "`()`"
":fit_data_scitype" = "`Tuple{Union{AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Count}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}}`"
":output_scitype" = "`Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":package_license" = "MIT"
":prediction_type" = ":unknown"
":load_path" = "MLJModels.UnivariateFillImputer"
":hyperparameters" = "`(:continuous_fill, :count_fill, :finite_fill)`"
":is_pure_julia" = "`true`"
":human_name" = "single variable fill imputer"
":is_supervised" = "`false`"
":iteration_parameter" = "`nothing`"
":docstring" = """```\nUnivariateFillImputer\n```\n\nA model type for constructing a single variable fill imputer, based on [MLJModels.jl](https://github.com/JuliaAI/MLJModels.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nUnivariateFillImputer = @load UnivariateFillImputer pkg=MLJModels\n```\n\nDo `model = UnivariateFillImputer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `UnivariateFillImputer(continuous_fill=...)`.\n\nUse this model to imputing `missing` values in a vector with a fixed value learned from the non-missing values of training vector.\n\nFor imputing missing values in tabular data, use [`FillImputer`](@ref) instead.\n\n# Training data\n\nIn MLJ or MLJBase, bind an instance `model` to data with\n\n```\nmach = machine(model, x)\n```\n\nwhere\n\n  * `x`: any abstract vector with element scitype `Union{Missing, T}` where `T` is a subtype of `Continuous`, `Multiclass`, `OrderedFactor` or `Count`; check scitype using `scitype(x)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * `continuous_fill`: function or other callable to determine value to be imputed in the case of `Continuous` (abstract float) data; default is to apply `median` after skipping `missing` values\n  * `count_fill`: function or other callable to determine value to be imputed in the case of `Count` (integer) data; default is to apply rounded `median` after skipping `missing` values\n  * `finite_fill`: function or other callable to determine value to be imputed in the case of `Multiclass` or `OrderedFactor` data (categorical vectors); default is to apply `mode` after skipping `missing` values\n\n# Operations\n\n  * `transform(mach, xnew)`: return `xnew` with missing values imputed with the fill values learned when fitting `mach`\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `filler`: the fill value to be imputed in all new data\n\n# Examples\n\n```\nusing MLJ\nimputer = UnivariateFillImputer()\n\nx_continuous = [1.0, 2.0, missing, 3.0]\nx_multiclass = coerce([\"y\", \"n\", \"y\", missing, \"y\"], Multiclass)\nx_count = [1, 1, 1, 2, missing, 3, 3]\n\nmach = machine(imputer, x_continuous)\nfit!(mach)\n\njulia> fitted_params(mach)\n(filler = 2.0,)\n\njulia> transform(mach, [missing, missing, 101.0])\n3-element Vector{Float64}:\n 2.0\n 2.0\n 101.0\n\nmach2 = machine(imputer, x_multiclass) |> fit!\n\njulia> transform(mach2, x_multiclass)\n5-element CategoricalArray{String,1,UInt32}:\n \"y\"\n \"n\"\n \"y\"\n \"y\"\n \"y\"\n\nmach3 = machine(imputer, x_count) |> fit!\n\njulia> transform(mach3, [missing, missing, 5])\n3-element Vector{Int64}:\n 2\n 2\n 5\n```\n\nFor imputing tabular data, use [`FillImputer`](@ref).\n"""
":inverse_transform_scitype" = "`Union{AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Count}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}`"
":package_url" = "https://github.com/JuliaAI/MLJModels.jl"
":package_name" = "MLJModels"
":name" = "UnivariateFillImputer"
":target_in_fit" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":implemented_methods" = [":fit", ":fitted_params", ":transform", ":UnivariateFillImputer"]
":deep_properties" = "`()`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":supports_training_losses" = "`false`"
":supports_weights" = "`false`"
":reports_feature_importances" = "`false`"
":input_scitype" = "`Union{AbstractVector{<:Union{Missing, ScientificTypesBase.Continuous}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Count}}, AbstractVector{<:Union{Missing, ScientificTypesBase.Finite}}}`"
":transform_scitype" = "`Union{AbstractVector{<:ScientificTypesBase.Continuous}, AbstractVector{<:ScientificTypesBase.Count}, AbstractVector{<:ScientificTypesBase.Finite}}`"
":constructor" = "`nothing`"

[MLJTransforms.TargetEncoder]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table, ScientificTypesBase.Unknown}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":target_in_fit" = "`true`"
":is_pure_julia" = "`true`"
":package_name" = "MLJTransforms"
":package_license" = "unknown"
":load_path" = "MLJTransforms.TargetEncoder"
":package_uuid" = "23777cdb-d90c-4eb0-a694-7c2b83d5c1d6"
":package_url" = "https://github.com/JuliaAI/MLJTransforms.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = """```\nTargetEncoder\n```\n\nA model type for constructing a target encoder, based on [MLJTransforms.jl](https://github.com/JuliaAI/MLJTransforms.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nTargetEncoder = @load TargetEncoder pkg=MLJTransforms\n```\n\nDo `model = TargetEncoder()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `TargetEncoder(features=...)`.\n\n`TargetEncoder` implements target encoding as defined in [1] to encode categorical variables      into continuous ones using statistics from the target variable.\n\n# Training data\n\nIn MLJ (or MLJBase) bind an instance `model` to data with\n\n```\nmach = machine(model, X, y)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`). Features to be transformed must  have element scitype `Multiclass` or `OrderedFactor`. Use `schema(X)` to   check scitypes.\n\n  * `y` is the target, which can be any `AbstractVector` whose element scitype is `Continuous` or `Count` for regression problems and  `Multiclass` or `OrderedFactor` for classification problems; check the scitype with `schema(y)`\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * features=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of `ignore`, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\n  * ignore=true: Whether to exclude or include the features given in `features`\n  * ordered_factor=false: Whether to encode `OrderedFactor` or ignore them\n  * `λ`: Shrinkage hyperparameter used to mix between posterior and prior statistics as described in [1]\n  * `m`: An integer hyperparameter to compute shrinkage as described in [1]. If `m=:auto` then m will be computed using\n\nempirical Bayes estimation as described in [1]\n\n# Operations\n\n  * `transform(mach, Xnew)`: Apply target encoding to selected `Multiclass` or `OrderedFactor features of`Xnew`specified by hyper-parameters, and   return the new table.   Features that are neither`Multiclass`nor`OrderedFactor`  are always left unchanged.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `task`: Whether the task is `Classification` or `Regression`\n  * `y_statistic_given_feat_level`: A dictionary with the necessary statistics to encode each categorical feature. It maps each    level in each categorical feature to a statistic computed over the target.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * encoded_features: The subset of the categorical features of `X` that were encoded\n\n# Examples\n\n```julia\nusing MLJ\n\n# Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]  \nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]  \nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n# Define the target variable \ny = [\"c1\", \"c2\", \"c3\", \"c1\", \"c2\",]\n\n# Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\ny = coerce(y, Multiclass)\n\nencoder = TargetEncoder(ordered_factor = false, lambda = 1.0, m = 0,)\nmach = fit!(machine(encoder, X, y))\nXnew = transform(mach, X)\n\njulia > schema(Xnew)\n┌───────┬──────────────────┬─────────────────────────────────┐\n│ names │ scitypes         │ types                           │\n├───────┼──────────────────┼─────────────────────────────────┤\n│ A_1   │ Continuous       │ Float64                         │\n│ A_2   │ Continuous       │ Float64                         │\n│ A_3   │ Continuous       │ Float64                         │\n│ B     │ Continuous       │ Float64                         │\n│ C_1   │ Continuous       │ Float64                         │\n│ C_2   │ Continuous       │ Float64                         │\n│ C_3   │ Continuous       │ Float64                         │\n│ D_1   │ Continuous       │ Float64                         │\n│ D_2   │ Continuous       │ Float64                         │\n│ D_3   │ Continuous       │ Float64                         │\n│ E     │ OrderedFactor{5} │ CategoricalValue{Int64, UInt32} │\n└───────┴──────────────────┴─────────────────────────────────┘\n```\n\n# Reference\n\n[1] Micci-Barreca, Daniele.      “A preprocessing scheme for high-cardinality categorical attributes in classification and prediction problems”      SIGKDD Explor. Newsl. 3, 1 (July 2001), 27–32.\n\nSee also [`OneHotEncoder`](@ref)\n"""
":name" = "TargetEncoder"
":human_name" = "target encoder"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":clean!", ":fit", ":fitted_params", ":transform"]
":hyperparameters" = "`(:features, :ignore, :ordered_factor, :lambda, :m)`"
":hyperparameter_types" = "`(\"Any\", \"Bool\", \"Bool\", \"Real\", \"Real\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"
":constructor" = "`nothing`"

[MLJTransforms.MissingnessEncoder]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":target_in_fit" = "`false`"
":is_pure_julia" = "`true`"
":package_name" = "MLJTransforms"
":package_license" = "unknown"
":load_path" = "MLJTransforms.MissingnessEncoder"
":package_uuid" = "23777cdb-d90c-4eb0-a694-7c2b83d5c1d6"
":package_url" = "https://github.com/JuliaAI/MLJTransforms.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = """```\nMissingnessEncoder\n```\n\nA model type for constructing a missingness encoder, based on [MLJTransforms.jl](https://github.com/JuliaAI/MLJTransforms.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nMissingnessEncoder = @load MissingnessEncoder pkg=MLJTransforms\n```\n\nDo `model = MissingnessEncoder()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `MissingnessEncoder(features=...)`.\n\n`MissingnessEncoder` maps any missing level of a categorical feature into a new level (e.g., \"Missing\").  By this, missingness will be treated as a new level by any subsequent model. This assumes that the categorical features have raw types that are in `Char`, `AbstractString`, and `Number`.\n\n# Training data\n\nIn MLJ (or MLJBase) bind an instance unsupervised `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`). Features to be transformed must  have element scitype `Multiclass` or `OrderedFactor`. Use `schema(X)` to   check scitypes.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * features=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of `ignore`, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\n  * ignore=true: Whether to exclude or include the features given in `features`\n  * ordered_factor=false: Whether to encode `OrderedFactor` or ignore them\n  * `label_for_missing::Dict{<:Type, <:Any}()= Dict( AbstractString => \"missing\", Char => 'm', )`: A\n\ndictionary where the possible values for keys are the types in `Char`, `AbstractString`, and `Number` and where each value signifies the new level to map into given a column raw super type. By default, if the raw type of the column subtypes `AbstractString` then missing values will be replaced with `\"missing\"` and if the raw type subtypes `Char` then the new value is `'m'` and if the raw type subtypes `Number` then the new value is the lowest value in the column - 1.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Apply cardinality reduction to selected `Multiclass` or `OrderedFactor` features of `Xnew` specified by hyper-parameters, and   return the new table.   Features that are neither `Multiclass` nor `OrderedFactor`  are always left unchanged.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `label_for_missing_given_feature`: A dictionary that for each column, maps `missing` into some value according to `label_for_missing`\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * encoded_features: The subset of the categorical features of `X` that were encoded\n\n# Examples\n\n```julia\nimport StatsBase.proportionmap\nusing MLJ\n\n# Define a table with missing values\nXm = (\n    A = categorical([\"Ben\", \"John\", missing, missing, \"Mary\", \"John\", missing]),\n    B = [1.85, 1.67, missing, missing, 1.5, 1.67, missing],\n    C= categorical([7, 5, missing, missing, 10, 0, missing]),\n    D = [23, 23, 44, 66, 14, 23, 11],\n    E = categorical([missing, 'g', 'r', missing, 'r', 'g', 'p'])\n)\n\nencoder = MissingnessEncoder()\nmach = fit!(machine(encoder, Xm))\nXnew = transform(mach, Xm)\n\njulia> Xnew\n(A = [\"Ben\", \"John\", \"missing\", \"missing\", \"Mary\", \"John\", \"missing\"],\n B = Union{Missing, Float64}[1.85, 1.67, missing, missing, 1.5, 1.67, missing],\n C = [7, 5, -1, -1, 10, 0, -1],\n D = [23, 23, 44, 66, 14, 23, 11],\n E = ['m', 'g', 'r', 'm', 'r', 'g', 'p'],)\n\n```\n\nSee also [`CardinalityReducer`](@ref)\n"""
":name" = "MissingnessEncoder"
":human_name" = "missingness encoder"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":transform"]
":hyperparameters" = "`(:features, :ignore, :ordered_factor, :label_for_missing)`"
":hyperparameter_types" = "`(\"Any\", \"Bool\", \"Bool\", \"Dict{T} where T<:Type\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"
":constructor" = "`nothing`"

[MLJTransforms.ContrastEncoder]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":target_in_fit" = "`false`"
":is_pure_julia" = "`true`"
":package_name" = "MLJTransforms"
":package_license" = "unknown"
":load_path" = "MLJTransforms.ContrastEncoder"
":package_uuid" = "23777cdb-d90c-4eb0-a694-7c2b83d5c1d6"
":package_url" = "https://github.com/JuliaAI/MLJTransforms.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = """```\nContrastEncoder\n```\n\nA model type for constructing a contrast encoder, based on [MLJTransforms.jl](https://github.com/JuliaAI/MLJTransforms.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nContrastEncoder = @load ContrastEncoder pkg=MLJTransforms\n```\n\nDo `model = ContrastEncoder()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `ContrastEncoder(features=...)`.\n\n`ContrastEncoder` implements the following contrast encoding methods for  categorical features: dummy, sum, backward/forward difference, and Helmert coding.  More generally, users can specify a custom contrast or hypothesis matrix, and each feature  can be encoded using a different method.\n\n# Training data\n\nIn MLJ (or MLJBase) bind an instance unsupervised `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`). Features to be transformed must  have element scitype `Multiclass` or `OrderedFactor`. Use `schema(X)` to   check scitypes.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * features=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of `ignore`, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\n  * `mode=:dummy`: The type of encoding to use. Can be one of `:contrast`, `:dummy`, `:sum`, `:backward_diff`, `:forward_diff`, `:helmert` or `:hypothesis`.\n\nIf `ignore=false` (features to be encoded are listed explictly in `features`), then this can be a vector of the same length as `features` to specify a different contrast encoding scheme for each feature\n\n  * `buildmatrix=nothing`: A function or other callable with signature `buildmatrix(colname, k)`,\n\nwhere `colname` is the name of the feature levels and `k` is it's length, and which returns contrast or  hypothesis matrix with row/column ordering consistent with the ordering of `levels(col)`. Only relevant if `mode` is `:contrast` or `:hypothesis`.\n\n  * ignore=true: Whether to exclude or include the features given in `features`\n  * ordered_factor=false: Whether to encode `OrderedFactor` or ignore them\n\n# Operations\n\n  * `transform(mach, Xnew)`: Apply contrast encoding to selected `Multiclass` or `OrderedFactor features of`Xnew`specified by hyper-parameters, and   return the new table. Features that are neither`Multiclass`nor`OrderedFactor`  are always left unchanged.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `vector_given_value_given_feature`: A dictionary that maps each level for each column in a subset of the categorical features of X into its frequency.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * encoded_features: The subset of the categorical features of `X` that were encoded\n\n# Examples\n\n```julia\nusing MLJ\n\n# Define categorical dataset\nX = (\n    name   = categorical([\"Ben\", \"John\", \"Mary\", \"John\"]),\n    height = [1.85, 1.67, 1.5, 1.67],\n    favnum = categorical([7, 5, 10, 1]),\n    age    = [23, 23, 14, 23],\n)\n\n# Check scitype coercions:\nschema(X)\n\nencoder =  ContrastEncoder(\n    features = [:name, :favnum],\n    ignore = false, \n    mode = [:dummy, :helmert],\n)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (name_John = [1.0, 0.0, 0.0, 0.0],\n    name_Mary = [0.0, 1.0, 0.0, 1.0],\n    height = [1.85, 1.67, 1.5, 1.67],\n    favnum_5 = [0.0, 1.0, 0.0, -1.0],\n    favnum_7 = [2.0, -1.0, 0.0, -1.0],\n    favnum_10 = [-1.0, -1.0, 3.0, -1.0],\n    age = [23, 23, 14, 23],)\n```\n\nSee also [`OneHotEncoder`](@ref)\n"""
":name" = "ContrastEncoder"
":human_name" = "contrast encoder"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":transform"]
":hyperparameters" = "`(:features, :ignore, :mode, :buildmatrix, :ordered_factor)`"
":hyperparameter_types" = "`(\"Any\", \"Bool\", \"Union{Symbol, AbstractVector{Symbol}}\", \"Any\", \"Bool\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"
":constructor" = "`nothing`"

[MLJTransforms.FrequencyEncoder]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":target_in_fit" = "`false`"
":is_pure_julia" = "`true`"
":package_name" = "MLJTransforms"
":package_license" = "unknown"
":load_path" = "MLJTransforms.FrequencyEncoder"
":package_uuid" = "23777cdb-d90c-4eb0-a694-7c2b83d5c1d6"
":package_url" = "https://github.com/JuliaAI/MLJTransforms.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = """```\nFrequencyEncoder\n```\n\nA model type for constructing a frequency encoder, based on [MLJTransforms.jl](https://github.com/JuliaAI/MLJTransforms.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nFrequencyEncoder = @load FrequencyEncoder pkg=MLJTransforms\n```\n\nDo `model = FrequencyEncoder()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `FrequencyEncoder(features=...)`.\n\n`FrequencyEncoder` implements frequency encoding which replaces the categorical values in the specified     categorical features with their (normalized or raw) frequencies of occurrence in the dataset. \n\n# Training data\n\nIn MLJ (or MLJBase) bind an instance unsupervised `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`). Features to be transformed must  have element scitype `Multiclass` or `OrderedFactor`. Use `schema(X)` to   check scitypes.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * features=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of `ignore`, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\n  * ignore=true: Whether to exclude or include the features given in `features`\n  * ordered_factor=false: Whether to encode `OrderedFactor` or ignore them\n  * `normalize=false`: Whether to use normalized frequencies that sum to 1 over category values or to use raw counts.\n  * `output_type=Float32`: The type of the output values. The default is `Float32`, but you can set it to `Float64` or any other type that can hold the frequency values.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Apply frequency encoding to selected `Multiclass` or `OrderedFactor features of`Xnew`specified by hyper-parameters, and   return the new table.   Features that are neither`Multiclass`nor`OrderedFactor`  are always left unchanged.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `statistic_given_feat_val`: A dictionary that maps each level for each column in a subset of the categorical features of X into its frequency.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * encoded_features: The subset of the categorical features of `X` that were encoded\n\n# Examples\n\n```julia\nusing MLJ\n\n# Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]  \nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]  \nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n# Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\n\n# Check scitype coercions:\nschema(X)\n\nencoder = FrequencyEncoder(ordered_factor = false, normalize=true)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (A = [2, 1, 2, 2, 2],\n    B = [1.0, 2.0, 3.0, 4.0, 5.0],\n    C = [4, 4, 4, 1, 4],\n    D = [3, 2, 3, 2, 3],\n    E = CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 2, 3, 4, 5],)\n```\n\nSee also [`TargetEncoder`](@ref)\n"""
":name" = "FrequencyEncoder"
":human_name" = "frequency encoder"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":transform"]
":hyperparameters" = "`(:features, :ignore, :ordered_factor, :normalize, :output_type)`"
":hyperparameter_types" = "`(\"Any\", \"Bool\", \"Bool\", \"Bool\", \"Type\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"
":constructor" = "`nothing`"

[MLJTransforms.CardinalityReducer]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":target_in_fit" = "`false`"
":is_pure_julia" = "`true`"
":package_name" = "MLJTransforms"
":package_license" = "unknown"
":load_path" = "MLJTransforms.CardinalityReducer"
":package_uuid" = "23777cdb-d90c-4eb0-a694-7c2b83d5c1d6"
":package_url" = "https://github.com/JuliaAI/MLJTransforms.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = """```\nCardinalityReducer\n```\n\nA model type for constructing a cardinality reducer, based on [MLJTransforms.jl](https://github.com/JuliaAI/MLJTransforms.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nCardinalityReducer = @load CardinalityReducer pkg=MLJTransforms\n```\n\nDo `model = CardinalityReducer()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `CardinalityReducer(features=...)`.\n\n`CardinalityReducer` maps any level of a categorical feature that occurs with frequency < `min_frequency` into a new level (e.g., \"Other\"). This is useful when some categorical features have high cardinality and many levels are infrequent. This assumes that the categorical features have raw types that are in `Union{AbstractString, Char, Number}`.\n\n# Training data\n\nIn MLJ (or MLJBase) bind an instance unsupervised `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`). Features to be transformed must  have element scitype `Multiclass` or `OrderedFactor`. Use `schema(X)` to   check scitypes.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * features=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of `ignore`, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\n  * ignore=true: Whether to exclude or include the features given in `features`\n  * ordered_factor=false: Whether to encode `OrderedFactor` or ignore them\n  * `min_frequency::Real=3`: Any level of a categorical feature that occurs with frequency < `min_frequency` will be mapped to a new level. Could be\n\nan integer or a float which decides whether raw counts or normalized frequencies are used.\n\n  * `label_for_infrequent::Dict{<:Type, <:Any}()= Dict( AbstractString => \"Other\", Char => 'O', )`: A\n\ndictionary where the possible values for keys are the types in `Char`, `AbstractString`, and `Number` and each value signifies the new level to map into given a column raw super type. By default, if the raw type of the column subtypes `AbstractString` then the new value is `\"Other\"` and if the raw type subtypes `Char` then the new value is `'O'` and if the raw type subtypes `Number` then the new value is the lowest value in the column - 1.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Apply cardinality reduction to selected `Multiclass` or `OrderedFactor` features of `Xnew` specified by hyper-parameters, and   return the new table.   Features that are neither `Multiclass` nor `OrderedFactor`  are always left unchanged.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `new_cat_given_col_val`: A dictionary that maps each level in a   categorical feature to a new level (either itself or the new level specified in `label_for_infrequent`)\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * encoded_features: The subset of the categorical features of `X` that were encoded\n\n# Examples\n\n```julia\nimport StatsBase.proportionmap\nusing MLJ\n\n# Define categorical features\nA = [ [\"a\" for i in 1:100]..., \"b\", \"b\", \"b\", \"c\", \"d\"]\nB = [ [0 for i in 1:100]..., 1, 2, 3, 4, 4]\n\n# Combine into a named tuple\nX = (A = A, B = B)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Multiclass\n)\n\nencoder = CardinalityReducer(ordered_factor = false, min_frequency=3)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia> proportionmap(Xnew.A)\nDict{CategoricalArrays.CategoricalValue{String, UInt32}, Float64} with 3 entries:\n  \"Other\" => 0.0190476\n  \"b\"     => 0.0285714\n  \"a\"     => 0.952381\n\njulia> proportionmap(Xnew.B)\nDict{CategoricalArrays.CategoricalValue{Int64, UInt32}, Float64} with 2 entries:\n  0  => 0.952381\n  -1 => 0.047619\n```\n\nSee also [`FrequencyEncoder`](@ref)\n"""
":name" = "CardinalityReducer"
":human_name" = "cardinality reducer"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":transform"]
":hyperparameters" = "`(:features, :ignore, :ordered_factor, :min_frequency, :label_for_infrequent)`"
":hyperparameter_types" = "`(\"Any\", \"Bool\", \"Bool\", \"Real\", \"Dict{T} where T<:Type\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"
":constructor" = "`nothing`"

[MLJTransforms.OrdinalEncoder]
":input_scitype" = "`ScientificTypesBase.Table`"
":output_scitype" = "`ScientificTypesBase.Table`"
":target_scitype" = "`ScientificTypesBase.Unknown`"
":fit_data_scitype" = "`Tuple{ScientificTypesBase.Table}`"
":predict_scitype" = "`ScientificTypesBase.Unknown`"
":transform_scitype" = "`ScientificTypesBase.Table`"
":inverse_transform_scitype" = "`ScientificTypesBase.Table`"
":target_in_fit" = "`false`"
":is_pure_julia" = "`true`"
":package_name" = "MLJTransforms"
":package_license" = "unknown"
":load_path" = "MLJTransforms.OrdinalEncoder"
":package_uuid" = "23777cdb-d90c-4eb0-a694-7c2b83d5c1d6"
":package_url" = "https://github.com/JuliaAI/MLJTransforms.jl"
":is_wrapper" = "`false`"
":supports_weights" = "`false`"
":supports_class_weights" = "`false`"
":supports_online" = "`false`"
":docstring" = """```\nOrdinalEncoder\n```\n\nA model type for constructing a ordinal encoder, based on [MLJTransforms.jl](https://github.com/JuliaAI/MLJTransforms.jl), and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\n```\nOrdinalEncoder = @load OrdinalEncoder pkg=MLJTransforms\n```\n\nDo `model = OrdinalEncoder()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `OrdinalEncoder(features=...)`.\n\n`OrdinalEncoder` implements ordinal encoding which replaces the categorical values in the specified     categorical features with integers (ordered arbitrarily). This will create an implicit ordering between     categories which may not be a proper modelling assumption.\n\n# Training data\n\nIn MLJ (or MLJBase) bind an instance unsupervised `model` to data with\n\n```\nmach = machine(model, X)\n```\n\nHere:\n\n  * `X` is any table of input features (eg, a `DataFrame`). Features to be transformed must  have element scitype `Multiclass` or `OrderedFactor`. Use `schema(X)` to   check scitypes.\n\nTrain the machine using `fit!(mach, rows=...)`.\n\n# Hyper-parameters\n\n  * features=[]: A list of names of categorical features given as symbols to exclude or include from encoding, according to the value of `ignore`, or a single symbol (which is treated as a vector with one symbol), or a callable that returns true for features to be included/excluded\n  * ignore=true: Whether to exclude or include the features given in `features`\n  * ordered_factor=false: Whether to encode `OrderedFactor` or ignore them\n  * `output_type`: The numerical concrete type of the encoded features. Default is `Float32`.\n\n# Operations\n\n  * `transform(mach, Xnew)`: Apply ordinal encoding to selected `Multiclass` or `OrderedFactor features of`Xnew`specified by hyper-parameters, and   return the new table.   Features that are neither`Multiclass`nor`OrderedFactor`  are always left unchanged.\n\n# Fitted parameters\n\nThe fields of `fitted_params(mach)` are:\n\n  * `index_given_feat_level`: A dictionary that maps each level for each column in a subset of the categorical features of X into an integer.\n\n# Report\n\nThe fields of `report(mach)` are:\n\n  * encoded_features: The subset of the categorical features of `X` that were encoded\n\n# Examples\n\n```julia\nusing MLJ\n\n# Define categorical features\nA = [\"g\", \"b\", \"g\", \"r\", \"r\",]  \nB = [1.0, 2.0, 3.0, 4.0, 5.0,]\nC = [\"f\", \"f\", \"f\", \"m\", \"f\",]  \nD = [true, false, true, false, true,]\nE = [1, 2, 3, 4, 5,]\n\n# Combine into a named tuple\nX = (A = A, B = B, C = C, D = D, E = E)\n\n# Coerce A, C, D to multiclass and B to continuous and E to ordinal\nX = coerce(X,\n:A => Multiclass,\n:B => Continuous,\n:C => Multiclass,\n:D => Multiclass,\n:E => OrderedFactor,\n)\n\n# Check scitype coercion:\nschema(X)\n\nencoder = OrdinalEncoder(ordered_factor = false)\nmach = fit!(machine(encoder, X))\nXnew = transform(mach, X)\n\njulia > Xnew\n    (A = [2, 1, 2, 3, 3],\n    B = [1.0, 2.0, 3.0, 4.0, 5.0],\n    C = [1, 1, 1, 2, 1],\n    D = [2, 1, 2, 1, 2],\n    E = CategoricalArrays.CategoricalValue{Int64, UInt32}[1, 2, 3, 4, 5],)\n```\n\nSee also [`TargetEncoder`](@ref)\n"""
":name" = "OrdinalEncoder"
":human_name" = "ordinal encoder"
":is_supervised" = "`false`"
":prediction_type" = ":unknown"
":abstract_type" = "`MLJModelInterface.Unsupervised`"
":implemented_methods" = [":fit", ":fitted_params", ":transform"]
":hyperparameters" = "`(:features, :ignore, :ordered_factor, :output_type)`"
":hyperparameter_types" = "`(\"Any\", \"Bool\", \"Bool\", \"Type\")`"
":hyperparameter_ranges" = "`(nothing, nothing, nothing, nothing)`"
":iteration_parameter" = "`nothing`"
":supports_training_losses" = "`false`"
":reports_feature_importances" = "`false`"
":deep_properties" = "`()`"
":reporting_operations" = "`()`"
":constructor" = "`nothing`"

[MLJModelRegistry]
